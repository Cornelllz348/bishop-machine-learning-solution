#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{"/config/config.tex"}
\end_preamble
\use_default_options true
\master ../lyxmain.lyx
\begin_modules
theorems-ams-chap-bytype
enumitem
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.12
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 2
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3.2cm
\topmargin 3cm
\rightmargin 3.2cm
\bottommargin 3cm
\secnumdepth -1
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Solutions for exercises to chapter 1
\end_layout

\begin_layout Section
Problem 1.1 - Closed form solution to polynomial regression
\end_layout

\begin_layout Standard
We use a slightly better notation to write this problem.
 Let 
\begin_inset Formula $X$
\end_inset

 be the matrix of the form 
\begin_inset Formula 
\[
X=\begin{bmatrix}x_{1}^{0} & x_{1}^{1} & \cdots & x_{1}^{M}\\
x_{2}^{0} & x_{2}^{1} & \cdots & x_{2}^{M}\\
\vdots & \vdots & \ddots & \vdots\\
x_{N}^{0} & x_{N}^{1} & \cdots & x_{N}^{M}
\end{bmatrix},\ \ t=\begin{bmatrix}t_{1}\\
t_{2}\\
\vdots\\
t_{n}
\end{bmatrix}
\]

\end_inset

The the problem can be rewritten in the following form: 
\begin_inset Formula 
\begin{align*}
E(w) & =\frac{1}{2}\left(\left(Xw-t\right)^{T}\left(Xw-t\right)\right).
\end{align*}

\end_inset

Now we differentiate w.r.t 
\begin_inset Formula $w$
\end_inset

, note that 
\begin_inset Formula 
\begin{align*}
E(w+h) & =\frac{1}{2}\left(X\left(w+h\right)-t\right)^{T}\left(X\left(w+h\right)-t\right)\\
 & =\frac{1}{2}\left(\left(Xw-t\right)^{T}+\left(Xh\right)^{T}\right)\left(Xw-t+Xh\right)\\
 & =\frac{1}{2}\left[\left(Xw-t\right)^{T}\left(Xw-t\right)+\left(Xw-t\right)^{T}Xh+\left(Xh\right)^{T}\left(Xw-t\right)+\left(Xh\right)^{T}\left(Xh\right)\right]\\
 & =E\left(w\right)+\left\langle \left(Xw-t\right)^{T},Xh\right\rangle +\frac{1}{2}\left\langle Xh,Xh\right\rangle \\
 & =E\left(w\right)+\left\langle X^{T}\left(Xw-t\right),h\right\rangle +\frac{1}{2}\left\langle Xh,Xh\right\rangle .
\end{align*}

\end_inset

Note that 
\begin_inset Formula $\left\langle X^{T}\left(Xw-t\right),h\right\rangle \in\text{Hom}(\mathbb{R}^{M+1},\mathbb{R})$
\end_inset

 and 
\begin_inset Formula 
\[
\frac{1}{2}\left\langle Xh,Xh\right\rangle \leq\frac{1}{2}\left\Vert Xh\right\Vert \left\Vert Xh\right\Vert \leq\frac{C}{2}\left\Vert X\right\Vert _{\infty}^{2}\left\Vert h\right\Vert \xrightarrow{\left\Vert h\right\Vert \rightarrow0}0,
\]

\end_inset

it follows that 
\begin_inset Formula $\nabla E(w)=X^{T}(Xw-t).$
\end_inset

 Set it to zero and we get 
\begin_inset Formula 
\[
X^{T}(Xw-t)=0\iff X^{T}Xw=X^{\top}t.
\]

\end_inset

So 
\begin_inset Formula $X^{T}X$
\end_inset

 is the 
\begin_inset Formula $A$
\end_inset

 proposed in the problem.
 
\begin_inset Formula 
\[
\left[X^{T}X\right]_{ij}=\sum_{n=1}^{N}\left(x_{n}^{i}x_{n}^{j}\right)=\sum_{n=1}^{N}x_{n}^{i+j},\text{ and }\left[X^{T}t\right]_{i}=\sum_{n=1}^{N}x_{n}^{i}t_{n},
\]

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.2 - Closed form solution to regularized polynomial regression
\end_layout

\begin_layout Standard
We use the same notation as in the previous problem and still rewrite the
 loss function in matrix form as follows: 
\begin_inset Formula 
\[
\widetilde{E}(w)=\frac{1}{2}\left\langle Xw-t,Xw-t\right\rangle +\frac{\lambda}{2}\left\langle w,w\right\rangle .
\]

\end_inset

Still we differentiate the expression.
 Note that if we let 
\begin_inset Formula $\varphi(w)=\frac{\lambda}{2}\left\langle w,w\right\rangle ,$
\end_inset

we have that 
\begin_inset Formula 
\begin{align*}
\varphi(w+h) & =\frac{\lambda}{2}\left(w+h\right)^{T}\left(w+h\right)\\
 & =\frac{\lambda}{2}\left(w^{T}w+w^{T}h+h^{T}x+\left\Vert h\right\Vert \right)\\
 & =\varphi\left(w\right)+\left\langle \lambda w,h\right\rangle +\underbrace{\frac{\lambda}{2}\left\Vert h\right\Vert ^{2}}_{=o(\left\Vert h\right\Vert )}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula $\nabla\varphi(w)=\lambda w,$
\end_inset

 and as a result 
\begin_inset Formula 
\[
\nabla\widetilde{E}(w)=\nabla E(w)+\nabla\varphi(w)=X^{T}(Xw-t)+\lambda w.
\]

\end_inset

Setting it to zero: 
\begin_inset Formula 
\[
X^{T}(Xw-t)+\lambda w=0\iff(X^{T}X+\lambda I)w=X^{T}t.
\]

\end_inset

Hence, 
\begin_inset Formula $(X^{T}X+\lambda I)$
\end_inset

 and 
\begin_inset Formula $X^{T}t$
\end_inset

 are the corresponding matrices.
\end_layout

\begin_layout Section
Problem 1.3 - Bayes formula warm up
\end_layout

\begin_layout Standard
According to the Bayes formula, we get that 
\begin_inset Formula 
\begin{align*}
P\left(\text{apple}\right) & =P\left(\text{apple}\vert\text{r}\right)P\left(\text{r}\right)+P\left(\text{apple}\vert\text{g}\right)P\left(\text{g}\right)+P\left(\text{apple}\vert\text{b}\right)P\left(\text{b}\right)\\
 & =\frac{3}{10}\cdot\frac{2}{10}+\frac{1}{2}\frac{2}{10}+\frac{3}{10}\frac{6}{10}=\frac{17}{50}.
\end{align*}

\end_inset

And again, we can use formula to get 
\begin_inset Formula 
\begin{align*}
P\left(\text{g}\vert\text{orange}\right) & =\frac{P\left(\text{orange}\vert\text{g}\right)P\left(\text{g}\right)}{P\left(\text{orange}\vert\text{g}\right)P\left(\text{g}\right)+P\left(\text{orange}\vert\text{b}\right)P\left(\text{b}\right)+P\left(\text{orange}\vert\text{r}\right)P\left(\text{r}\right)}\\
 & =\frac{\frac{3}{10}\frac{6}{10}}{\frac{3}{10}\frac{6}{10}+\frac{2}{10}\frac{1}{2}+\frac{2}{10}\frac{4}{10}}\\
 & =\frac{1}{2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.4 - Nonlinear transform of likelihood function doesn't preserve
 its extrema
\end_layout

\begin_layout Standard
We first observe that if 
\begin_inset Formula $x_{*}$
\end_inset

 maximizes the likelihood function 
\begin_inset Formula $p_{x}(x)$
\end_inset

, then 
\begin_inset Formula $p_{x}'(x_{*})=0.$
\end_inset

 By chain rule, we have that 
\begin_inset Formula 
\begin{align*}
\frac{dp_{x}(g(y))\left|g'\left(y\right)\right|}{dy} & =\frac{dp_{x}(g(y))}{dy}\left|g'(y)\right|+p_{x}(g(y))\frac{d\left|g'(y)\right|}{dy}\\
 & =\frac{dp_{x}(g(y))}{dg(y)}\frac{dg(y)}{dy}\left|g'(y)\right|+p_{x}(g(y))\frac{d\left|g'(y)\right|}{dy}\tag{1}.
\end{align*}

\end_inset

Hence, if 
\begin_inset Formula $x_{*}=g(y_{*})$
\end_inset

, the 
\begin_inset Formula 
\[
\frac{dp_{x}(g(y_{*}))}{dg(y_{*})}=\frac{dp_{x}(x_{*})}{dx_{*}}=0.
\]

\end_inset

However, there is no guarantee that the second term of the RHS of Eq.
 1 is zero.
 For example, if 
\begin_inset Formula $p_{x}(x)=2x$
\end_inset

 for 
\begin_inset Formula $0\leq x\leq1$
\end_inset

 and 
\begin_inset Formula $x=\sin(y),$
\end_inset

 where 
\begin_inset Formula $0\leq y\leq\pi/2.$
\end_inset

 Then according to the transformation formula, we have that 
\begin_inset Formula 
\[
p_{y}(y)=p_{x}(g(y))g'(y)=2\sin(y)\cos(y)=\sin(2y)\text{ for }0\leq y\leq\frac{\pi}{2}.
\]

\end_inset

Clearly, 
\begin_inset Formula $p_{y}(y)$
\end_inset

 reaches its peak at 
\begin_inset Formula $y=\pi/4$
\end_inset

 but 
\begin_inset Formula $\sin(\pi/4)\neq x_{*}=1.$
\end_inset

 Thus, we have found a counterexample.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

On the other hand, if 
\begin_inset Formula $g(y)$
\end_inset

 is an affine map, then 
\begin_inset Formula $g'(y)$
\end_inset

 is a constant map and as a result 
\begin_inset Formula 
\[
\frac{d\left|g'(y)\right|}{dy}=0
\]

\end_inset

 
\end_layout

\begin_layout Section
Problem 1.5 - Characterization of variance
\end_layout

\begin_layout Standard
It suffices to show that 
\begin_inset Formula $\mathrm{Var}[X]=\E[X^{2}]-(\E[X])^{2}$
\end_inset

 since any a measurable function of a random variable is again a random
 variable and in this case 
\begin_inset Formula $f$
\end_inset

 although is not mentioned, it is safe to assume in this context that 
\begin_inset Formula $f$
\end_inset

 is measurable.
 So note 
\begin_inset Formula 
\begin{align*}
\mathrm{Var}[X] & =\E[X-\E[X]]^{2}\\
 & =\E[X^{2}]-2\E[X]\E[X]+(\E[X])^{2}\\
 & =\E[X^{2}]-\E[X]^{2}
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.6 - Covariance of two independent r.v.
 is zero
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $X\perp Y,$
\end_inset

 then it follows that 
\begin_inset Formula $\E[XY]=\E[X]\E[Y]$
\end_inset

.
 Then we have 
\begin_inset Formula 
\begin{align*}
\mathrm{Cov}(X,Y) & =\E[(X-\E[X])(Y-\E[Y])]\\
 & =\E[XY-X\E[Y]-\E[X]Y+\E[X]\E[Y]]\\
 & =\E[XY]-\E[X]\E[Y]-\E[X]\E[Y]+\E[X]\E[Y]\\
 & =\E[X]\E[Y]-\E[X]\E[Y]-\E[X]\E[Y]+\E[X]\E[Y]\\
 & =0.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.7 - Gaussian integral via polar coordinate
\end_layout

\begin_layout Standard
First, we write 
\begin_inset Formula 
\begin{align*}
I^{2} & =\left(\int_{\R}\exp\bigg\{-\frac{1}{2\sigma^{2}}x^{2}\bigg\} dx\right)\left(\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}y^{2}\right\} dy\right)\\
 & =\int\int_{\R\times\R}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(x^{2}+y^{2}\right)\right\} dxdx.
\end{align*}

\end_inset

Now using polar coordinate - let 
\begin_inset Formula $x=r\cos\theta$
\end_inset

 and 
\begin_inset Formula $y=r\sin\theta$
\end_inset

.
 Then we get the Jacobian matrix as 
\begin_inset Formula 
\[
\frac{\partial(x,y)}{\partial(r,\theta)}=\begin{bmatrix}\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{bmatrix}\implies\left|\frac{\partial(x,y)}{\partial(r,\theta)}\right|=r(\cos\theta^{2}+\sin\theta^{2})=r.
\]

\end_inset

Hence, as a result 
\begin_inset Formula 
\begin{align*}
I^{2} & =\int_{0}^{2\pi}\int_{0}^{\infty}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} rdrd\theta\\
 & =_{1}\int_{0}^{2\pi}\int_{0}^{\infty}\exp(-u)\sigma^{2}dud\theta\\
 & =\int_{0}^{2\pi}\sigma^{2}d\theta\int_{0}^{\infty}\exp(-u)du\\
 & =2\pi\sigma^{2}\left[-\exp(-u)\right]_{0}^{\infty}=2\pi\sigma^{2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.8 - Second moment of gaussian integral via Feymann's trick
\end_layout

\begin_layout Standard
The differentiation under the integral needs a bit more theoretical justificatio
n.
 We won't reproduce the related theorems here.
 But they could be found in e.g.
 Theorem 3.2, Theorem 3.3 in Chapter XIII of 
\begin_inset CommandInset citation
LatexCommand cite
key "sergeundergrad"
literal "false"

\end_inset

 or in 
\begin_inset CommandInset citation
LatexCommand cite
key "diffsign"
literal "false"

\end_inset

.
 With this in mind, we get 
\begin_inset Formula 
\begin{align*}
\frac{d}{d\sigma^{2}}\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} dx & =\int_{\R}\frac{d}{d\sigma^{2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} dx\\
 & =\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}\left(-\frac{1}{2}\right)(\sigma^{-2})^{2}dx
\end{align*}

\end_inset

On the the other hand, we have 
\begin_inset Formula 
\[
\frac{d}{d\sigma^{2}}(2\pi\sigma^{2})^{1/2}=-\frac{1}{2}(2\pi)(\sigma^{2})^{-1/2}.
\]

\end_inset

So combined together, we get 
\begin_inset Formula 
\[
\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}\left(-\frac{1}{2}\right)(\sigma^{-2})^{2}dx=\left(-\frac{1}{2}\right)(2\pi)^{1/2}(\sigma^{2})^{-1/2}.
\]

\end_inset

One step of reduction, we get 
\begin_inset Formula 
\begin{align*}
\E[(x-\E[x])^{2}] & =\mathrm{Var}[x]\\
 & =\frac{1}{(2\pi\sigma^{2})^{1/2}}\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}dx\\
 & =\sigma^{2}.
\end{align*}

\end_inset

And as a result, 
\begin_inset Formula 
\[
\E[x^{2}]=\mathrm{Var}[x]+(\E[x])^{2}=\sigma^{2}+\mu^{2}.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 1.9 - Gaussian density peaks at mean
\end_layout

\begin_layout Standard
It suffices to show the result holds in the multidimensional case since
 1-dim is just a special case.
 Recall that the density of the Gaussian distribution in 
\begin_inset Formula $D$
\end_inset

 dimension is 
\begin_inset Formula 
\[
N(x\vert u,\Sigma)=\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} .
\]

\end_inset

Differentiate w.r.t.
 
\begin_inset Formula $x$
\end_inset

 and we get: 
\begin_inset Formula 
\[
\nabla_{x}N(x\vert u,\Sigma)=\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} \nabla_{x}\left(\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right).
\]

\end_inset

Now note that 
\begin_inset Formula $\varphi(x)=(x-\mu)^{T}\Sigma^{-1}(x-\mu)$
\end_inset

 for 
\begin_inset Formula $x\in\R^{d}$
\end_inset

 , then note for any 
\begin_inset Formula $h\in\mathbb{R}^{D}$
\end_inset


\begin_inset Formula 
\begin{align*}
\varphi(x+h) & =(x-u+h)^{T}\Sigma^{-1}(x-\mu+h)\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu+h)+h^{T}\Sigma^{-1}(x-\mu+h)\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu)+(x-\mu)^{T}\Sigma^{-1}h+h^{T}\Sigma^{-1}(x-\mu)+h^{T}\Sigma^{-1}h\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu)+\left\langle 2\Sigma^{-1}(x-\mu),h\right\rangle +h^{T}\Sigma^{-1}h
\end{align*}

\end_inset

Note that and 
\begin_inset Formula 
\[
h^{T}\Sigma^{-1}h=\left\langle h\Sigma^{-1/2},h\Sigma^{-1/2}\right\rangle \leq\left\Vert h\Sigma^{-1/2}\right\Vert ^{2}\leq C\left\Vert h\right\Vert ^{2}\left\Vert \Sigma\right\Vert _{\infty}^{2}=o(\left\Vert h\right\Vert ),
\]

\end_inset

and that 
\begin_inset Formula $\left\langle 2\Sigma^{-1}(x-\mu),h\right\rangle \in\text{Hom}(\mathbb{R}^{d},\mathbb{R}).$
\end_inset

 It follows that 
\begin_inset Formula 
\[
\nabla_{x}\varphi(x)=2\Sigma^{-1}(x-\mu),
\]

\end_inset

whence 
\begin_inset Formula 
\[
\nabla_{x}\varphi(x)=0\iff2\Sigma^{-1}(x-\mu)=0\iff x=\mu.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 1.10 - Linearity of expectation and variance
\end_layout

\begin_layout Enumerate
Note 
\begin_inset Formula 
\begin{align*}
\E\left[x+y\right] & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}(x+y)f_{(x,y)}(x,y)dxdy\\
 & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}(x+y)f_{x}(x)f_{y}(y)dxdy\\
 & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}xf_{x}(x)f_{y}(y)dxdy+\int_{\text{supp}(x)}\int_{\text{supp}(y)}yf_{x}(x)f_{y}(y)dxdy\\
 & =\int_{\text{supp}(x)}xf_{x}(x)dx\int_{\text{supp}(y)}f_{y}(y)+\int_{\text{supp}(x)}f_{x}(x)dx\int_{\text{supp}(y)}yf_{y}(y)\\
 & =\E[x]+\E[y].
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Note 
\begin_inset Formula 
\begin{align*}
\mathrm{Var}[x+y] & =\E[x+y]^{2}-(\E[x+y])^{2}\\
 & =\E[x^{2}]+\E[y^{2}]+\underbrace{2\E[xy]}_{\E[x]\E[y]}-(\E[x])^{2}-(\E[y])^{2}-2\E[x]\E[y]\\
 & =\E[x^{2}]-(\E[x])^{2}+\E[y^{2}]-(\E[y])^{2}\\
 & =\mathrm{Var}[x]+\mathrm{Var}[y].
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.11 - MLE of gaussian
\end_layout

\begin_layout Standard
Recall that the log-likelihood function for Gaussian distribution is 
\begin_inset Formula 
\[
\ln p(x\vert\mu,\sigma^{2})=-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{n}-\mu)^{2}-\frac{N}{2}\ln\sigma^{2}-\frac{N}{2}\ln(2\pi).
\]

\end_inset

Now we differentiate it w.r.t.
 
\begin_inset Formula $\mu$
\end_inset

 and setting it to zero: 
\begin_inset Formula 
\[
\frac{\partial\ln p(x\vert\mu,\sigma^{2})}{\partial\mu}=-\frac{1}{2\sigma^{2}}\cdot2\cdot\sum_{i=1}^{N}(x_{n}-\mu)=0\iff\sum_{i=1}^{N}(x_{n}-\mu)=0\iff\mu_{ML}=\frac{1}{n}\sum_{i=1}^{N}x_{n}.
\]

\end_inset

Now we differentiate it w.r.t.
 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and setting it to zero: 
\begin_inset Formula 
\begin{align*}
\frac{\partial\ln(p\vert\mu,\sigma^{2})}{\partial\sigma^{2}} & =\underbrace{\sum_{n=1}^{N}(x_{n}-\mu)^{2}\left(-\frac{1}{2}\right)(-1)(\sigma^{2})^{-2}-\frac{N}{2\sigma^{2}}=0}_{(\star)}.
\end{align*}

\end_inset

To rearrange, we get 
\begin_inset Formula 
\begin{align*}
(\star) & \iff\sum_{n=1}^{N}(x_{n}-\mu)^{2}\sigma^{-4}=\frac{N}{\sigma^{2}}\\
 & \iff\sum_{n=1}^{N}(x_{n}-\mu)^{2}=\sigma^{2}N\\
 & \iff\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu)^{2}.
\end{align*}

\end_inset

Plug in 
\begin_inset Formula $\mu=\mu_{ML}$
\end_inset

 we get 
\begin_inset Formula $\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu)^{2}$
\end_inset

 as desired.
 
\end_layout

\begin_layout Section
Problem 1.12 - Inconsistency gaussian MLE
\end_layout

\begin_layout Section
Problem 1.14 - Independent terms of 2-nd order term in polynomial
\end_layout

\begin_layout Standard
We rewrite the sum in matrix form: 
\begin_inset Formula $\sum_{i=1}^{D}\sum_{j=1}^{D}w_{ij}x_{i}x_{j}=x^{T}Wx,$
\end_inset

 where 
\begin_inset Formula $[W]_{ij}=w_{ij}.$
\end_inset

 Define 
\begin_inset Formula 
\[
W_{S}=\frac{1}{2}(W+W^{T})\text{ and }W_{A}=\frac{1}{2}(W-W^{T}).
\]

\end_inset

Clearly, 
\begin_inset Formula $W_{S}$
\end_inset

 is symmetric and 
\begin_inset Formula $W_{A}^{T}=\frac{1}{2}(W^{T}-W)=-W_{A}$
\end_inset

 is anti-symmetric and 
\begin_inset Formula $W_{S}+W_{A}=W.$
\end_inset

 Therefore, 
\begin_inset Formula 
\[
x^{T}Wx=x^{T}(W_{S}+W_{A})x=x^{T}W_{S}x+x^{T}W_{A}x.
\]

\end_inset

Notice that 
\begin_inset Formula 
\[
x^{T}W_{A}x=\frac{1}{2}(x^{T}W_{S}x-x^{T}W^{T}x)=\frac{1}{2}(x^{T}W_{S}s-x^{T}Wx)=0,
\]

\end_inset

where the last inequality follows from the fact that 
\begin_inset Formula $x^{T}W^{T}x$
\end_inset

 is a scalar and is equal to 
\begin_inset Formula $x^{T}Wx.$
\end_inset

 Since we have shown the sum, 
\begin_inset Formula $\sum_{i,j}w_{ij}x_{i}x_{j}$
\end_inset

, only depends on a symmetric matrix, 
\begin_inset Formula $W_{S}$
\end_inset

, whose independent items is of the cardinality of 
\begin_inset Formula $\sum_{i=1}^{D}i=D(D+1)/2$
\end_inset

 if we assume its of dimension 
\begin_inset Formula $D\times D$
\end_inset

, we have established our claim.
 
\end_layout

\begin_layout Section
Problem 1.15 - Independent terms of 
\begin_inset Formula $M$
\end_inset

-th order term in polynomial
\end_layout

\begin_layout Enumerate
Since by writing the 
\begin_inset Formula $M$
\end_inset

-th order in the form of 
\begin_inset Formula 
\[
\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{D}\cdots\sum_{i_{M}=1}^{D}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}
\]

\end_inset

introduces duplicate terms, e.g.
 if 
\begin_inset Formula $w_{1,3,2}x_{1}x_{3}x_{2}$
\end_inset

 and 
\begin_inset Formula $w_{2,3,1}x_{2}x_{3}x_{1}$
\end_inset

 are the same and can be combined into 
\begin_inset Formula $(w_{1,3,2}+w_{2,3,1})x_{1}x_{2}x_{3},$
\end_inset

 we can introduce an ordering that prevents such duplication from happening.
 Rewrite the sum in the newly introduced ordering yields 
\begin_inset Formula 
\[
\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}.
\]

\end_inset

Thus, we have ll
\begin_inset Formula 
\begin{align*}
n(D,M) & =\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}\\
 & =\sum_{i_{1}=1}^{D}\left(\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}\right)\\
 & =\sum_{i_{1}=1}^{D}n(i_{1},M-1).
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To show the equality holds using induction, we note for the base case of
 
\begin_inset Formula $D=1,$
\end_inset

 
\begin_inset Formula 
\[
\text{LHS}=\frac{(1+M-2)!}{0!(M-1)!}=\frac{(M-1)!}{(M-1)!}=1.
\]

\end_inset

And 
\begin_inset Formula 
\[
\text{RHS}=\frac{(1+M-1)!}{(D-1)!M!}=\frac{M!}{M!}=1.
\]

\end_inset

Now suppose 
\begin_inset Formula $D=k$
\end_inset

 and the equality holds.
 Then 
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{k+1}\frac{(i+M-2)!}{(i-1)!(M-1)!} & =\sum_{i=1}^{k}\frac{(i+M-2)!}{(i-1)!(M-1)!}+\frac{(k+1+M-2)!}{k!(M-1)!}\\
 & =\frac{(k+M-1)!}{(k-1)!M!}+\frac{(k+M-1)!}{k!(M-1)!}\tag{1}\\
 & =\frac{(k+M-1)!(k+M)}{k!(M-1)!}\\
 & =\frac{(k+M)!}{k!M!}\\
 & =\frac{((k+1)+M-1)!}{(k+1-1)!M!},
\end{align*}

\end_inset

where Eq.
 (1) follows from induction hypothesis.
 
\end_layout

\begin_layout Enumerate
We establish the identity by inducting on 
\begin_inset Formula $M$
\end_inset

.
 By Problem 1.14, it follows that 
\begin_inset Formula 
\[
n(D,2)=\frac{1}{2}D(D+1)=\frac{(D+2-1)!}{(D-1)!2!}=\frac{(D+1)!}{(D-1)!2!},
\]

\end_inset

which proves the base case.
 Now suppose the statement holds for 
\begin_inset Formula $M=k$
\end_inset

.
 Then for 
\begin_inset Formula $M=k+1$
\end_inset

, we have 
\begin_inset Formula 
\[
n(D,k+1)=\sum_{i=1}^{D}n(i,k)=\sum_{i=1}^{D}\frac{(i+M-2)!}{(i-1)!(M-1)!}=\frac{(D+M-1)!}{(D-1)!M!}
\]

\end_inset

using part-2.
 
\end_layout

\begin_layout Section
Problem 1.16 - Independent terms of high order polynomial
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

The first equality just follows from that summing up all the independent
 terms: 
\begin_inset Formula 
\[
N(D,M)=\sum_{i=0}^{M}n(D,i).
\]

\end_inset


\end_layout

\begin_layout Enumerate
We prove this inequality by inducting on 
\begin_inset Formula $M.$
\end_inset

 Now for the base case, 
\begin_inset Formula $M=0,$
\end_inset

 we note that 
\begin_inset Formula 
\[
\text{LHS}=n(D,0)=\frac{(D+0-1)!}{(D-1)!0!}=1=\frac{(D+0)!}{D!0!}=\text{RHS}.
\]

\end_inset

Now assume that the claim holds for 
\begin_inset Formula $M=k$
\end_inset

.
 Then for 
\begin_inset Formula $M=k+1,$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
N(D,k+1) & =\sum_{i=0}^{k}n(D,i)+n(D,k+1)\\
 & =\frac{(D+k)!}{D!k!}+\frac{(D+k+1-1)!}{(D-1)!(k+1)!}\\
 & =\frac{(D+k)!(D+k+1)}{D!(k+1)!}\\
 & =\frac{(D+k+1)!}{D!(k+1)!},
\end{align*}

\end_inset

proving the inducting step.
 
\end_layout

\begin_layout Enumerate
Now we show that 
\begin_inset Formula $N(D,M)$
\end_inset

 grows in polynomial fashion like 
\begin_inset Formula $D^{M}.$
\end_inset

 Assume 
\begin_inset Formula $D\ll M.$
\end_inset

 First, we write 
\begin_inset Formula 
\begin{align*}
N(D,M) & =\frac{(D+M)!}{D!M!}\\
 & \simeq\frac{(D+M)^{D+M}e^{-(D+M)}}{D!M^{M}e^{-M}}\tag{by Stirling's approximation}\\
 & =\frac{1}{D!M^{M}}\left(1+\frac{D}{M}\right)^{D+M}M^{D+M}\frac{e^{-(D+M)}}{e^{-M}}\\
 & =\frac{e^{-D}}{D!}\left(1+\frac{D}{M}\right)^{D+M}M^{D}.\tag{1}
\end{align*}

\end_inset

Now we take a more delicate look at the term 
\begin_inset Formula $(1+\frac{D}{M})^{D+M}.$
\end_inset

 Note that 
\begin_inset Formula 
\begin{align*}
\left(1+\frac{D}{M}\right)^{D+M} & =\left(1+\frac{D}{M}\right)^{M}\left(1+\frac{D}{M}\right)^{D}\\
 & =\biggl(\left(1+\frac{1}{M/D}\right)^{M/D}\biggl)^{D}\left(1+\frac{D}{M}\right)^{D}\\
 & \leq e^{D}2^{D},
\end{align*}

\end_inset

where the inequality comes from the fact that 
\begin_inset Formula $(1+1/x)^{x}$
\end_inset

 is an increasing function and 
\begin_inset Formula $D<M\Rightarrow D/M\leq1.$
\end_inset

 Substitution back into Eq (1), we get 
\begin_inset Formula 
\[
N(D,M)\leq\frac{e^{-D}}{D!}e^{D}2^{D}M^{D}=\frac{2^{D}}{D!}M^{D}.
\]

\end_inset

The case for 
\begin_inset Formula $M\ll D$
\end_inset

 follows by symmetry.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.17 - Gamma density warmup
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

Note 
\begin_inset Formula 
\begin{align*}
\Gamma(x+1) & =\int_{0}^{\infty}u^{x}e^{-u}du\\
 & =\left[-u^{x}e^{-u}\right]_{u=0}^{\infty}+\int_{0}^{\infty}xu^{x-1}e^{-u}du\\
 & =x\Gamma(x).
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
We note that 
\begin_inset Formula 
\[
\Gamma(1)=\int_{0}^{\infty}e^{-u}du=\left[e^{-u}\right]_{0}^{\infty}=1.
\]

\end_inset

And as a result, by recursion
\begin_inset Formula 
\[
\Gamma(x+1)=x\Gamma(x)=\cdots=x!\text{ for }x\in\mathbb{N}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.18 - Volume of unit sphere in n-space
\end_layout

\begin_layout Standard
To state the problem statement in a clearer manner, we solve this problem
 in several steps.
 In this problem, we let 
\begin_inset Formula $d\mu$
\end_inset

 denote the Lebesgue measure.
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

First we derive Eq (1.142) in the book.
 We first rewrite the LHS in the following way.
 Let 
\begin_inset Formula $x\in\mathbb{R}^{D}$
\end_inset

 be arbitrary, then 
\begin_inset Formula 
\begin{align*}
\int_{\mathbb{R}^{d}}e^{-\left\Vert x\right\Vert ^{2}}dx & =\int_{\mathbb{R}}\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}e^{-(x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2})}dx_{1}dx_{2}\cdots dx_{n}\\
 & =\prod_{i=1}^{D}\int_{\mathbb{R}}e^{-x_{i}^{2}}dx_{i}.
\end{align*}

\end_inset

Next, we evaluate this integral.
 In order to make the computation easier, we choose to let the integrand
 be 
\begin_inset Formula $e^{-\pi\left|x\right|^{2}}$
\end_inset

instead (it doesn't effect the final result, and one could always get the
 original integral by scaling).
 Note that using the same argument as above, we have 
\begin_inset Formula 
\[
\int_{\mathbb{R}^{D}}e^{-\pi\left\Vert x\right\Vert ^{2}}dx=\left(\int_{\mathbb{R}}e^{-\pi x^{2}}dx\right)^{D}.
\]

\end_inset

Next, we have 
\begin_inset Formula 
\begin{align*}
\left(\int_{\mathbb{R}}e^{-\pi x^{2}}dx\right)^{2} & =\left(\int_{\mathbb{R}}e^{-\pi x_{1}^{2}}dx_{1}\right)\left(\int_{\mathbb{R}}e^{-\pi x_{2}^{2}}dx_{2}\right)\\
 & =\int_{\mathbb{R}\times\mathbb{R}}e^{-\pi(x_{1}^{2}+x_{2}^{2})}d(x_{1}\times x_{2})\tag{by Fubini's theorem}\\
 & =\int_{\mathbb{R}}\int_{\mathbb{R}}e^{-\pi(x_{1}^{2}+x_{2}^{2})}dx_{1}dx_{2}\tag{by Fubini's theorem}\\
 & =\int_{[0,2\pi]}\int_{\mathbb{R}}e^{-\pi r^{2}}rdrd\theta\tag{switch to polar coordinates}\\
 & =\int_{[0,2\pi]}d\theta\int_{\mathbb{R}}e^{-\pi r^{2}}rdr\\
 & =2\pi\left[-\frac{1}{2\pi}e^{-\pi r^{2}}\right]_{0}^{\infty}\\
 & =1.
\end{align*}

\end_inset

Since 
\begin_inset Formula $\int_{\mathbb{R}}e^{\pi x^{2}}dx>0$
\end_inset

, it follows that 
\begin_inset Formula $\int_{\mathbb{R}^{D}}e^{-\pi\left\Vert x\right\Vert ^{2}}dx=1.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Consider the function 
\begin_inset Formula $f:\mathbb{R}^{D}\rightarrow\mathbb{R};x\mapsto e^{-\pi\left\Vert x\right\Vert ^{2}}.$
\end_inset

 We just showed in part-1 that 
\begin_inset Formula $f\in L^{1}(\mathbb{R}^{D}).$
\end_inset

 Therefore, using generalized spherical coordinate (e.g.
 Theorem 6.3.4 in 
\begin_inset CommandInset citation
LatexCommand cite
key "stein2005real"
literal "false"

\end_inset

), we have that 
\begin_inset Formula 
\begin{align*}
1=\int_{\mathbb{R}^{D}}f(x)dx & =\int_{S^{D-1}}\left(\int_{\mathbb{R^{+}}}f(r\gamma)r^{D-1}dr\right)d\sigma(\gamma)\\
 & =\int_{S^{D-1}}\left(\int_{\mathbb{R}^{+}}e^{-\pi\left\Vert r\gamma\right\Vert ^{2}}r^{D-1}dr\right)d\sigma(\gamma)\\
 & =\int_{S^{D-1}}\left(\int_{\mathbb{R}^{+}}e^{-\pi r^{2}}r^{D-1}dr\right)d\sigma(\gamma)\\
 & =\int_{S^{D-1}}d\sigma(r)\int_{\mathbb{R}^{+}}e^{-\pi r^{2}}r^{D-1}dr\\
 & =\sigma(S^{D-1})\int_{\mathbb{R}^{+}}e^{-\pi r^{2}}r^{D-1}dr.
\end{align*}

\end_inset

Now we evaluate the integral on the RHS: 
\begin_inset Formula 
\begin{align*}
\int_{\mathbb{R}^{+}}e^{-\pi r^{r}}r^{D-1}dr & =\int_{0}^{\infty}e^{-u}\left(\frac{u}{\pi}\right)^{\frac{D-1}{2}}\frac{1}{2\pi(u/\pi)^{1/2}}du\\
 & =\frac{1}{2\pi}\int_{0}^{\infty}e^{-u}\left(\frac{u}{\pi}\right)^{\frac{D}{2}-1}du\\
 & =\frac{1}{2\pi}\pi^{1-\frac{D}{2}}\int_{0}^{\infty}e^{-u}u{}^{\frac{D}{2}-1}du\\
 & =\frac{1}{2}\pi^{-\frac{D}{2}}\Gamma\left(\frac{D}{2}\right).
\end{align*}

\end_inset

Therefore, substituting back we get 
\begin_inset Formula 
\[
\sigma(S^{D-1})=\frac{1}{\int_{\mathbb{R}^{+}}e^{-\pi r^{2}}r^{D-1}dr}=\frac{2\pi^{D/2}}{\Gamma\left(D/2\right)}.
\]

\end_inset

This 
\begin_inset Formula $\sigma(S^{D-1})$
\end_inset

 is the 
\begin_inset Formula $S_{D}$
\end_inset

 in the problem.
 
\end_layout

\begin_layout Enumerate
Now we calculate the volume of the ball.
 Let 
\begin_inset Formula $B_{1}$
\end_inset

 denote the unit ball in 
\begin_inset Formula $\mathbb{R}^{D}$
\end_inset

.
 Note that again by generalized spherical coordinate, 
\begin_inset Formula 
\begin{align*}
V_{D} & =\int_{\mathbb{R}^{D}}\1_{B_{1}}(x)d\mu\\
 & =\int_{S^{D-1}}\int_{\mathbb{R}^{+}}\1_{B_{1}}(r\gamma)r^{D-1}d\sigma(\gamma)\\
 & =\int_{S^{D-1}}\left(\int_{[0,1]}r^{D-1}dr\right)d\sigma(\gamma)\\
 & =\left(\int_{S^{D-1}}d\sigma(\gamma)\right)\left(\int_{[0,1]}r^{D-1}dr\right)\\
 & =\sigma(S^{D-1})\left[\frac{1}{D}r^{D}\right]_{0}^{1}\\
 & =\frac{\pi^{D/2}}{\Gamma(D/2)(D/2)}\\
 & =\frac{\pi^{D/2}}{\Gamma(D/2+1)}.
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Enumerate
When 
\begin_inset Formula $D=2,$
\end_inset

 we get 
\begin_inset Formula 
\[
S_{D}=\frac{2\pi^{2/2}}{\Gamma(1)}=2\pi\text{ and }V_{D}=\frac{S_{D}}{D}=\pi.
\]

\end_inset

When 
\begin_inset Formula $D=2,$
\end_inset

 we get 
\begin_inset Formula 
\[
S_{D}=\frac{2\pi^{3/2}}{\Gamma(3/2)}=\frac{2\pi^{3/2}}{\pi^{1/2}/2}=4\pi\text{ and }V_{D}=\frac{4}{3}\pi.
\]

\end_inset


\end_layout

\begin_layout Remark
This problem could have been solved heuristically.
 But it loses rigor.
 What was showed was a rigorous mathematical way to treat this problem.
 
\end_layout

\begin_layout Section
Problem 1.19 - High dimensional cubes concentrate on corners
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

Using the result of the previous problem, and the fact that 
\begin_inset Formula $m_{d}(rB)=r^{d}m(B),$
\end_inset

 where 
\begin_inset Formula $m_{d}$
\end_inset

 is the Lebesgue measure in 
\begin_inset Formula $d$
\end_inset

-dimensional Euclidean space (e.g.
 Exercise 1.6 in 
\begin_inset CommandInset citation
LatexCommand cite
key "stein2005real"
literal "false"

\end_inset

), we have that 
\begin_inset Formula 
\begin{align*}
\frac{V_{\text{sphere}}}{V_{\text{cube}}} & =\frac{\pi^{D/2}a^{D}}{\Gamma(D/2+1)2^{D}a^{D}}=\frac{\pi^{D/2}}{\Gamma(D/2+1)2^{D}}\\
 & \simeq\frac{\pi^{D/2}}{(2\pi)^{1/2}e^{-D/2}(D/2)^{D/2+1/2}2^{D}}\tag{by Stirling formula}\\
 & =C\frac{\pi^{D/2}e^{D/2}}{(D/2)^{D/2}}\frac{1}{D^{1/2}}2^{-D}\tag{\ensuremath{C} is some constant}\\
 & =C\left(\frac{2\pi e}{D}\right)^{D/2}\frac{1}{D^{1/2}2^{D}}\xrightarrow{D\rightarrow\infty}0.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
On the other hand, we have 
\begin_inset Formula 
\begin{align*}
\text{dist}(\text{center to corner}) & =\sqrt{Da^{2}}=a\sqrt{D}\\
\text{dist}(\text{center to top)} & =a.
\end{align*}

\end_inset

And thus the ratio is 
\begin_inset Formula $\sqrt{D}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.20 - High dimensional gaussian concentrate on a thin strip
\end_layout

\begin_layout Standard
First, note that the density given in the problem is that of a Gaussian
 in 
\begin_inset Formula $D$
\end_inset

 dimensional Euclidean space with 
\begin_inset Formula $\Sigma=\text{diag}(\sigma^{2}).$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

To show that the density is of the form exhibited in (1.148), we note that
 again by generalized spherical coordinate we have 
\begin_inset Formula 
\begin{align*}
\int_{\mathbb{R}^{D}}p(x)dx & =\int_{S^{D-1}}\int_{\mathbb{R}^{+}}p(\gamma r)drd\sigma(\gamma)\\
 & =\int_{S^{D-1}}\int_{\mathbb{R}^{+}}\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{\left\Vert \gamma r\right\Vert ^{2}}{2\sigma^{2}}\right\} r^{D-1}drd\sigma(\gamma)\\
 & =\int_{S^{D-1}}\int_{\mathbb{R}^{+}}\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{\left\Vert \gamma\right\Vert ^{2}r^{2}}{2\sigma^{2}}\right\} r^{D-1}drd\sigma(\gamma)\\
 & =\int_{S^{D-1}}d\sigma(\gamma)\int_{\mathbb{R^{+}}}\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} r^{D-1}dr\\
 & =\sigma(S^{D-1})\int_{\mathbb{R}^{+}}\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} r^{D-1}dr.
\end{align*}

\end_inset

This is the formula in (1.148) if we relabel 
\begin_inset Formula $\sigma(S^{D-1})=S_{D}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
First, we note 
\begin_inset Formula 
\begin{align*}
\frac{d}{dr}p(r) & =C\cdot\frac{d}{dr}\left[r^{D-1}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} \right]\\
 & =C\cdot\left[(D-1)r^{D-2}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} +r^{D-1}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} \left(-\frac{1}{2\sigma^{2}}\right)2r\right]\\
 & =C\left[(D-1)r^{D-2}-\frac{r^{D}}{\sigma^{2}}\right]\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} .
\end{align*}

\end_inset

To find the stationary point, we set it to zero: 
\begin_inset Formula 
\begin{align*}
\frac{d}{dr}p(r)=0 & \iff C\left[(D-1)r^{D-2}-\frac{r^{D}}{2\sigma^{2}}\right]\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} =0\\
 & \iff(D-1)r^{D-2}-\frac{r^{D}}{\sigma^{2}}=0\\
 & \iff\hat{r}=\sqrt{(D-1)\sigma^{2}}\simeq\sqrt{D}\sigma,
\end{align*}

\end_inset

where the approximation follows since 
\begin_inset Formula $\sqrt{D+1}=\sqrt{D}$
\end_inset

 for large 
\begin_inset Formula $D$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
To show (1.149), first we note 
\begin_inset Formula 
\begin{align*}
p(\hat{r}+\varepsilon) & =\frac{S_{D}(\hat{r}+\varepsilon)^{D-1}}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{(\hat{r}+\varepsilon)^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{S_{D}}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{(\hat{r}+\varepsilon)^{2}}{2\sigma^{2}}+(D-1)\log(\hat{r}+\varepsilon)\right\} \\
 & =\frac{S_{D}}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{(\hat{r}+\varepsilon)^{2}}{2\sigma^{2}}+(D-1)\left[\log\left(1+\frac{\varepsilon}{\hat{r}}\right)+\log\hat{r}\right]\right\} \\
 & =\frac{S_{D}\hat{r}^{D-1}}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{\hat{r}^{2}}{2\sigma^{2}}-\frac{\hat{r}\varepsilon}{\sigma^{2}}-\frac{\varepsilon^{2}}{2\sigma^{2}}+(D-1)\bigg(\frac{\varepsilon}{\hat{r}}-\frac{\varepsilon^{2}}{2\hat{\gamma}^{2}}+o\bigg(\frac{\varepsilon^{2}}{\hat{\gamma}^{2}}\bigg)\bigg)\right\} \\
 & =\underbrace{\frac{S_{D}\hat{r}^{D-1}}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{\hat{r}^{2}}{2\sigma^{2}}\right\} }_{=p(r)}\underbrace{\exp\left\{ -\frac{\hat{r}\varepsilon}{\sigma^{2}}-\frac{\varepsilon^{2}}{2\sigma^{2}}+(D-1)\bigg(\frac{\varepsilon}{\hat{r}}-\frac{\varepsilon^{2}}{2\hat{\gamma}^{2}}+o\bigg(\frac{\varepsilon^{2}}{\hat{\gamma}^{2}}\bigg)\bigg)\right\} }_{:=\mathcal{E}(\varepsilon,\sigma,\hat{r})}\tag{1}.
\end{align*}

\end_inset

Now, we just need to massage last term in the RHS of (1): since 
\begin_inset Formula $\hat{r}=\sqrt{D-1}\sigma$
\end_inset

, we get 
\begin_inset Formula 
\begin{align*}
\mathcal{E}(\varepsilon,\sigma,\hat{r}) & =\exp\left\{ -\frac{\sqrt{D}-1\varepsilon}{\sigma}-\frac{\varepsilon^{2}}{2\sigma^{2}}+\frac{\sqrt{D-1}\varepsilon}{\sigma}-\frac{\varepsilon^{2}}{2\sigma^{2}}+o\bigg(\frac{\varepsilon^{2}}{\hat{\gamma}^{2}}\bigg)\right\} \\
 & =\exp\left\{ -\frac{\varepsilon^{2}}{\sigma^{2}}\right\} \exp\left\{ o\bigg(\frac{\varepsilon^{2}}{\hat{\gamma}^{2}}\bigg)\right\} .
\end{align*}

\end_inset

Since by assumption 
\begin_inset Formula $\varepsilon\ll\hat{r}$
\end_inset

, it follows that 
\begin_inset Formula $\mathcal{E}(\varepsilon,\sigma,\hat{r})\simeq\exp\{-\varepsilon^{2}/\sigma^{2}\}.$
\end_inset

 Substituting back we get 
\begin_inset Formula 
\[
p(\hat{r}+\varepsilon)=p(r)\exp\left\{ -\frac{\varepsilon^{2}}{\sigma^{2}}\right\} 
\]

\end_inset

as desired.
 
\end_layout

\begin_layout Enumerate
Note that we have 
\begin_inset Formula 
\[
p(x=0)=\frac{1}{(2\pi\sigma^{2})^{D/2}},
\]

\end_inset

 and 
\begin_inset Formula 
\begin{align*}
p(x\in\Gamma\vert\Gamma=\{\gamma\in\mathbb{R}^{d}\vert\left\Vert \gamma\right\Vert =\sqrt{D-1}\sigma\}) & =\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{(D-1)\sigma^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{(2\pi\sigma^{2})^{D/2}}\exp\left\{ -\frac{D-1}{2}\right\} ,
\end{align*}

\end_inset

whence 
\begin_inset Formula 
\begin{align*}
\frac{p(x\in\Gamma\vert\Gamma=\{\gamma\in\mathbb{R}^{d}\vert\left\Vert \gamma\right\Vert =\sqrt{D-1}\sigma\})}{p(x=0)} & =\exp\left\{ -\frac{D-1}{2}\right\} \\
 & \simeq\exp\left\{ -\frac{D}{2}\right\} \text{ when }D\text{ is large}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.21 - Upper bound of bayesian classification error
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

Since 
\begin_inset Formula $x\mapsto\sqrt{x}$
\end_inset

 is monotonically increasing and 
\begin_inset Formula $a\leq b,$
\end_inset

 it follows that 
\begin_inset Formula $0\leq a^{1/2}\leq b^{1/2},$
\end_inset

 which then implies 
\begin_inset Formula $a\leq a^{1/2}b^{1/2}$
\end_inset

 after multiplying both sides with 
\begin_inset Formula $a^{1/2}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
To show the desired inequality, we note (for notation, we let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 be the ambient input space), 
\begin_inset Formula 
\begin{align*}
\P(\text{mistake}) & =\int_{\mathcal{R}_{1}}\P(x,\mathcal{C}_{2})dx+\int_{\mathcal{R}_{2}}\P(x,\mathcal{C}_{1})dx\\
 & \leq\int_{\mathcal{R}_{1}}\P(x,\mathcal{C}_{1})\P(x,\mathcal{C}_{2})dx+\int_{\mathcal{R}_{2}}\P(x,\mathcal{C}_{1})\P(x,\mathcal{C}_{2})dx\tag{by part-1 }\\
 & =\int_{\mathcal{R}_{1}\cup\mathcal{R}_{2}}\P(x,\mathcal{C}_{1})\P(x,\mathcal{C}_{2})dx\\
 & =\int_{\mathcal{X}}\P(x,\mathcal{C}_{1})\P(x,\mathcal{C}_{2})dx,
\end{align*}

\end_inset

where the last inequality follows since we are working in a two-class setting
 and the fact that decision regions partition the input space.
 
\end_layout

\begin_layout Section
Problem 1.22 - Uniform loss maximizes posterior probability 
\end_layout

\begin_layout Standard
For concise notation, we write the loss matrix as 
\begin_inset Formula $L=\mathbbm{1}\mathbbm{1}^{T}-I$
\end_inset

, where here 
\begin_inset Formula $\mathbbm{1}$
\end_inset

 stands for vector of 
\begin_inset Formula $1$
\end_inset

's and 
\begin_inset Formula $\vec{\P}(\mathcal{C}\vert x)$
\end_inset

 as a vector of 
\begin_inset Formula $\P(\mathcal{C}_{k},x)$
\end_inset

's.
 Then we can rewrite Eq.
 (1.81) in the book as 
\begin_inset Formula 
\begin{align*}
\min_{j}\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x) & =\min_{j}\vec{\P}(\mathcal{C}\vert x)^{T}(\mathbbm{1}\mathbbm{1}^{T}-I)e_{j}\\
 & =\min_{j}\vec{\P}(\mathcal{C}\vert x)^{T}\mathbbm{1}-\P(\mathcal{C}_{j}\vert x)\\
 & =\min_{j}1-\P(\mathcal{C}_{j}\vert x)\\
 & =\max_{j}\P(\mathcal{C}_{j}\vert x).
\end{align*}

\end_inset

where the second equality follows from the fact the conditional distribution
 sums to 1.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

We can interpret this loss in the following way: this loss assigns unit
 weight to each misclassified labels and zero weight to correctly classified
 labels and therefore minimizing the expectation represents minimizing the
 misclassification rate.
 
\end_layout

\begin_layout Section
Problem 1.23 - Characterization for minimizing general expected loss
\end_layout

\begin_layout Standard
Note 
\begin_inset Formula 
\[
\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x)=\frac{1}{p(x)}\sum_{k}L_{kj}\P(x\vert\mathcal{C}_{k})\P(\mathcal{C}_{k}).
\]

\end_inset

Suppose 
\begin_inset Formula $m=\min(\sum_{k}L_{kj}\P(x\vert\mathcal{C}_{k})$
\end_inset

, if we increase 
\begin_inset Formula $\P(\mathcal{C}_{k})$
\end_inset

, we would have to decrease 
\begin_inset Formula $L_{kj}$
\end_inset

 to keep the minimum.
 Hence, there is a direct trade-off between 
\begin_inset Formula $\P(\mathcal{C}_{k})$
\end_inset

 and 
\begin_inset Formula $L_{kj}.$
\end_inset


\end_layout

\begin_layout Section
Problem 1.24 - Duality between decision and rejection criterion 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

According to Eq.
 (1.81) in the book, the decision of labels is found by computing 
\begin_inset Formula $\argmin_{j}\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x).$
\end_inset

 Since rejection option is also used, let 
\begin_inset Formula $\hat{j}$
\end_inset

 be the minimum, then the decision criterion can be modeled as a function
 
\begin_inset Formula $\varphi:\mathbb{N}\rightarrow\mathbb{N}\cup\{\emptyset\}$
\end_inset

 by 
\begin_inset Formula 
\[
j\mapsto\begin{cases}
\argmin_{j}\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x) & \text{if }\min_{j}\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x)\\
\emptyset & \text{otherwise}
\end{cases}.
\]

\end_inset

Note the 
\begin_inset Formula $j$
\end_inset

 defined in 
\begin_inset Formula $\varphi$
\end_inset

 by default refers to the minimizer of 
\begin_inset Formula $\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x),$
\end_inset

 and the mapping to empty set means rejection.
 
\end_layout

\begin_layout Enumerate
When 
\begin_inset Formula $L=\mathbbm{1}\mathbbm{1}^{T}-I,$
\end_inset

 then we have by previous part that 
\begin_inset Formula 
\begin{align*}
\varphi(\hat{j})=j & \iff\min_{j}\sum_{k}L_{kj}\P(\mathcal{C}_{k}\vert x)\leq\lambda\\
 & \iff\min_{j}1-\P(\mathcal{C}_{j}\vert x)\leq\lambda\tag{by Problem 1.22}\\
 & \iff\max\P(\mathcal{C}_{k}\vert x)\geq1-\lambda.
\end{align*}

\end_inset

Note that the last stipulation is equivalent to 
\begin_inset Formula $\theta=1-\lambda$
\end_inset

 in the reject option definition.
 Hence, the two criteria coincide when 
\begin_inset Formula $\theta=1-\lambda.$
\end_inset

 
\end_layout

\begin_layout Section
Problem 1.25 - Generalized squared loss function
\end_layout

\begin_layout Standard
We follow the same procedure as in the 1 dimensional case.
 Note 
\begin_inset Formula 
\begin{align*}
\frac{\delta\E[L]}{\delta L} & =\frac{\delta}{\delta L}\left[\int\int\left\Vert y(x)-t\right\Vert ^{2}p(t,x)dxdt\right]\\
 & =\int2(y(x)-t)p(t,x)dt.
\end{align*}

\end_inset

Setting it to zero yields: 
\begin_inset Formula 
\begin{align*}
y(x)\int p(t,x)dt=\int tp(t,x)dt & \iff y(x)=\frac{\int tp(t,x)dt}{\int p(t,x)dt}\\
 & \iff y(x)=\frac{\int tp(t,x)dt}{p(x)}=\int tp(t\vert x)dt
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.26 - Decomposition of expected squared loss
\end_layout

\begin_layout Standard
We use the similar argument as in deriving Eq.
 (1.90) in here.
 We write 
\begin_inset Formula 
\begin{align*}
\left\Vert y(x)-t\right\Vert ^{2} & =\left\Vert y(x)-\E[t\vert x]+\E[t\vert x]-t\right\Vert ^{2}\\
 & =\left\Vert y(x)-\E[t\vert x]\right\Vert ^{2}-2(y(x)-\E[y\vert x])^{T}(\E[t\vert x]-t)+\left\Vert \E[t\vert x]-t\right\Vert ^{2}.
\end{align*}

\end_inset

Also note that we can rewrite 
\begin_inset Formula $\E[t\vert x]-t=\E[t\vert x]-\E[\E[t\vert x]]$
\end_inset

 and that 
\begin_inset Formula $\E[y(x)-\E[y\vert x]]=\E[y]-\E[y]=0$
\end_inset

.
 Hence 
\begin_inset Formula 
\begin{align*}
\E\left[\left\Vert y(x)-t\right\Vert ^{2}\right] & =\int\left\Vert y(x)-\E[t\vert x]\right\Vert ^{2}p(x)dx+\int\left\Vert \E[t\vert x-t\right\Vert ^{2}p(x)dx\\
 & =\int\left\Vert y(x)-\E[t\vert x]\right\Vert ^{2}p(x)dx+\int\text{Var}[t\vert x]p(x)dx.
\end{align*}

\end_inset

Hence, we see that 
\begin_inset Formula $\E[\left\Vert y(x)-t\right\Vert ^{2}]$
\end_inset

 is minimized when 
\begin_inset Formula $y(x)=\E[t\vert x],$
\end_inset

 which is analogues to Eq.
 (1.90).
 
\end_layout

\begin_layout Section
Problem 1.27 - Maximizer of 
\begin_inset Formula $L_{1},L_{0^{+}}$
\end_inset

 expected loss 
\end_layout

\begin_layout Standard
According to Eq.
 (1.91), an application of Fubini's theorem we can rewrite the expected Minkowski
 loss in the following form:
\begin_inset Formula 
\[
\E[L]=\int\underbrace{\int\left|y(x)-t\right|^{q}p(x,t)dt}_{:=G(x,y(x))}dx
\]

\end_inset

Here we need assume 
\begin_inset Formula $G(x,y(x))$
\end_inset

 converges uniformly so that we can differentiate under the improper (possibly)
 integral.
 As usual, we compute the first variation: 
\begin_inset Formula 
\begin{align*}
\frac{\delta\E[L]}{\delta y(x)} & =\frac{\partial G(x,y(x))}{\partial y(x)}=\int q\left|y(x)-t\right|^{q-1}\frac{(y(x)-t)}{\left|y(x)-t\right|}p(x,t)dt\\
 & =p(x)\int q\left|y(x)-t\right|^{q-1}\text{sgn}(y(x)-t)p(t\vert x)dt\\
 & =p(x)\left(\int_{\{t\leq y(x)\}}q\left|y(x)\right|^{q-1}p(t\vert x)dt-\int_{t>y(x)}q\left|y(x)-t\right|^{q-1}p(t\vert x)dt\right)\tag{1}
\end{align*}

\end_inset

To find the stationary point when 
\begin_inset Formula $q=1$
\end_inset

, we set Eq.(1) to zero: 
\begin_inset Formula 
\begin{align*}
 & p(x)\left(\int_{\{t\leq y(x)\}}q\left|y(x)\right|^{q-1}p(t\vert x)dt-\int_{\{t>y(x)\}}q\left|y(x)-t\right|^{q-1}p(t\vert x)dt\right)=0\\
\implies_{1} & \int_{\{t\leq y(x)\}}p(t\vert x)dt=\int_{\{t>y(x)\}}p(t\vert x),\tag{2}
\end{align*}

\end_inset

where 
\begin_inset Formula $\implies_{1}$
\end_inset

 follows since we only need to care about 
\begin_inset Formula $x\in\text{supp}(p(x)).$
\end_inset

 Hence, the 
\begin_inset Formula $y(x)$
\end_inset

 that maximizes the expected loss function with 
\begin_inset Formula $q=1$
\end_inset

 satisfies Eq.(2), which is the definition of the median.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Now we consider the case when 
\begin_inset Formula $p\rightarrow0.$
\end_inset

 Instead of taking the functional derivative, we will use a more delicate
 and analytical approach.
 First, we write 
\begin_inset Formula 
\[
\E[L]=\lim_{q\rightarrow0}\int\left|y(x)-t\right|^{q}d(F_{x}\times F_{t}).
\]

\end_inset

Observe that 
\begin_inset Formula $\lim_{q\rightarrow0}\left|y(x)-t\right|^{q}=\mathbbm{1}_{\{y(x)\neq t\}},$
\end_inset

 which is in 
\begin_inset Formula $L_{2}(\Omega)$
\end_inset

 (we use 
\begin_inset Formula $\Omega$
\end_inset

 to denote the probability space).
 An application of DCT yields 
\begin_inset Formula 
\begin{align*}
\E[L] & =\int\mathbbm{1}_{\{y(x)\neq t\}}d(F_{x}\times F_{t})\\
 & =\int d(F_{x}\times F_{t})-\int\mathbbm{1}_{\{y(x)=t\}}d(F_{x}\times F_{t})\\
 & =1-\underbrace{\int\int\mathbbm{1}_{\{y(x)=t\}}p(x,t)dtdx}_{:=\mathcal{I}_{1}(y(x),x,t)}\tag{by change of variable theorem}.
\end{align*}

\end_inset

In order to minimize 
\begin_inset Formula $\E[L]$
\end_inset

, it suffices to find 
\begin_inset Formula $\argmax_{y(x)}\mathcal{I}_{1}(y(x),x,t).$
\end_inset

 First, we rewrite 
\begin_inset Formula 
\[
\mathcal{I}_{1}(y(x),x,t)=\int p(x)\underbrace{\int\mathbbm{1}_{\{t=y(x)\}}p(t\vert x)dt}_{:=\mathcal{I}_{2}(y(x),x,t)}dx.
\]

\end_inset

Since 
\begin_inset Formula $p(x)\geq0$
\end_inset

 for any 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{2}(y(x),x,t)\geq0,$
\end_inset

 it follows that 
\begin_inset Formula $\argmax_{y(x)}\mathcal{I}_{1}(y(x),x,t)=\argmax_{y(x)}\mathcal{I}_{2}(y(x),x,t).$
\end_inset

 Note that 
\begin_inset Formula $\mathcal{I}_{2}(y(x),x,t)=0$
\end_inset

 since it is an integral w.r.t to a singleton point whose Lebesgue measure
 is zero.
 However, we can circumvent this in the following manner: note that 
\begin_inset Formula 
\begin{align*}
\mathcal{I}_{2}(y(x),x,t) & =\int\lim_{n\rightarrow\infty}\mathbbm{1}_{\{t\in(y(x)-\frac{1}{2n},y(x)+\frac{1}{2n})\}}p(t\vert x)dt\\
 & =\lim_{n\rightarrow\infty}\int\mathbbm{1}_{\{t\in(y(x)-\frac{1}{2n},y(x)+\frac{1}{2n})\}}p(t\vert x)dt\\
 & \leq\lim_{n\rightarrow\infty}\frac{1}{n}\sup_{t\in(y(x)-\frac{1}{2n},y(x)+\frac{1}{2n})}p(t\vert x)\tag{3}
\end{align*}

\end_inset

If we define 
\begin_inset Formula $F_{n}(y(x))=\frac{1}{n}\sup_{t\in(y(x)-1/(2n),y(x)+1/(2n))}p(t\vert x),$
\end_inset

 then it follows from Eq.(3) that 
\begin_inset Formula $F_{n}(y(x))\rightarrow0$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 for any 
\begin_inset Formula $y(x)\in\mathbb{R}$
\end_inset

.
 However, we would like to find a 
\begin_inset Formula $\tilde{y}(x)$
\end_inset

 s.t.
 
\begin_inset Formula $F_{n}(\tilde{y}(x))\leq F_{n}(y(x))$
\end_inset

 for any other choice of 
\begin_inset Formula $y(x).$
\end_inset

 We claim that 
\begin_inset Formula $\tilde{y}(x)=\argmax_{t\in\mathbb{R}}p(t\vert x).$
\end_inset

 Indeed, if so, we have 
\begin_inset Formula 
\begin{align*}
F_{n}(\tilde{y}(x)) & =\frac{1}{n}\sup_{t\in(\argmax_{t}p(t\vert x)-\frac{1}{2n},\argmax_{t}p(t\vert x)+\frac{1}{2n})}p(t\vert x)=\frac{1}{n}\sup_{t\in\mathbb{R}^{n}}p(t\vert x)\\
 & \geq\frac{1}{n}\sup_{t\in(y(x)-1/(2n),y(x)+1/(2n))}p(t\vert x)=F(y(x)).
\end{align*}

\end_inset

So to translate into heuristic terms, 
\begin_inset Formula $y(x)=\argmax_{t}p(t\vert x)$
\end_inset

 minimizes the loss function in "each step" of the process of "approaching
 the limit of 
\begin_inset Formula $q\rightarrow0$
\end_inset

".
 
\end_layout

\begin_layout Section
Problem 1.28 - Derivation of information content
\end_layout

\begin_layout Standard
Assuming the randoms variables to be discrete does simplifies the argument
 but it also losses rigor.
 To achieve maximum amount of rigor possible, we use a measure theoretic
 language.
 For this reason, we will use a slightly different formulation, but the
 idea remains the same.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Instead of using 
\begin_inset Formula $x,y$
\end_inset

 to denote random variable, we use 
\begin_inset Formula $X,Y$
\end_inset

.
 Note that for any 
\begin_inset Formula $A\in\mathcal{B}(X),$
\end_inset

 
\begin_inset Formula $B\in\mathcal{B}(Y)$
\end_inset

, where 
\begin_inset Formula $\mathcal{B}$
\end_inset

 denote the Borel sets, if 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent, 
\begin_inset Formula 
\begin{align*}
h(X\in A,Y\in B) & =h\bigg(\int_{A\times B}d(F_{X}\times F_{Y})\bigg)=h\bigg(\int_{A}dF_{X}\cdot\int_{B}dF_{Y}\bigg).\tag{by independence}\\
 & =h(X\in A)+h(Y\in B)=h\bigg(\int_{A}dF_{X}\bigg)+h\bigg(\int_{B}dF_{Y}\bigg).
\end{align*}

\end_inset

If we let 
\begin_inset Formula $x=\int_{A}dF_{X}$
\end_inset

 and 
\begin_inset Formula $y=\int_{B}dF_{Y},$
\end_inset

 then this problem reduces to the following form: find a representation
 of 
\begin_inset Formula $h$
\end_inset

 such that 
\begin_inset Formula $h(xy)=h(x)+h(y)$
\end_inset

 for any 
\begin_inset Formula $x,y\in[0,1].$
\end_inset

 This is variant of the Cauchy Functional Equation problem.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Recall that the Cauchy functional equation in its standard form is as follows:
 find a function 
\begin_inset Formula $f$
\end_inset

 that satisfies 
\begin_inset Formula $f(x+y)=f(x)+f(y).$
\end_inset

 The obvious solution to 
\begin_inset Formula $f$
\end_inset

 is the linear one: 
\begin_inset Formula $x\mapsto cx$
\end_inset

 for 
\begin_inset Formula $x\in\mathbb{R}^{n}.$
\end_inset

 However, without additional assumptions, one can obtain other complicated
 solutions as well.
 But generally these solutions serves as pedagogical example.
 A classical result is that if we assume 
\begin_inset Formula $f$
\end_inset

 to be either continuous or monotone, then 
\begin_inset Formula $f(x)=cx$
\end_inset

 for arbitrary 
\begin_inset Formula $c\in\mathbb{R}$
\end_inset

 is the only solution.
 (cf.
 
\begin_inset CommandInset citation
LatexCommand cite
key "kuczma2009an"
literal "false"

\end_inset

).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

With this in mind, to solve for 
\begin_inset Formula $h$
\end_inset

, we define 
\begin_inset Formula $g(x)=h(e^{x}).$
\end_inset

 Then we see that 
\begin_inset Formula 
\[
g(x+y)=h(e^{x}e^{y})=h(e^{x})+h(e^{y})=g(x)+g(y).
\]

\end_inset

Then if we require 
\begin_inset Formula $g$
\end_inset

 to be continuous, then 
\begin_inset Formula $g(x)$
\end_inset

 is uniquely represented as 
\begin_inset Formula $cx$
\end_inset

 for any 
\begin_inset Formula $c\in\mathbb{R}.$
\end_inset

 Then note that for any 
\begin_inset Formula $x\in\mathbb{R}^{+},$
\end_inset

 
\begin_inset Formula 
\[
h(x)=h(e^{\ln x})=g(\ln x)=c\ln x,\text{ for any }c\in\mathbb{R}.
\]

\end_inset

Therefore, we have 
\begin_inset Formula $h(x)\propto\ln(x)$
\end_inset

 as desired.
 
\end_layout

\begin_layout Section
Problem 1.29 - Upper bound for entropy of discrete variables
\end_layout

\begin_layout Standard
We directly apply Jensen's inequality: 
\begin_inset Formula 
\[
H(x)=-\sum_{i=1}^{M}p(x_{i})\ln(x_{i})=\sum_{i=1}^{M}p(x_{i})\ln\frac{1}{p(x_{i})}\leq\ln\bigg(\sum_{i=1}^{M}p(x_{i})\frac{1}{p(x_{i})}\bigg)=\ln M,
\]

\end_inset

where the 
\begin_inset Formula $\leq$
\end_inset

 follows since 
\begin_inset Formula $\ln(x)$
\end_inset

 is concave.
 
\end_layout

\begin_layout Section
Problem 1.30 - KL-divergence for Gaussian
\end_layout

\begin_layout Standard
We use the original definition of KL-divergence: 
\begin_inset Formula 
\[
KL(p||q)=\underbrace{-\int p(x)\ln q(x)dx}_{(1)}-\underbrace{\left(-\int p(x)\ln p(x)dx\right)}_{(2)}.
\]

\end_inset

We compute it term by term, first note that 
\begin_inset Formula 
\begin{alignat*}{1}
(1)= & -\int\varphi(x\vert\mu,\sigma^{2})\ln\left[\varphi(x\vert m,s^{2})\right]dx\\
= & -\int\varphi(x\vert\mu,\sigma^{2})\left\{ \ln\frac{1}{(2\pi s^{2})^{1/2}}-\frac{(x-m)^{2}}{2s^{2}}\right\} \\
= & \int\varphi(x\vert\mu,\sigma^{2})\left[\frac{1}{2}\ln(2\pi s^{2})+\frac{(x-m)^{2}}{2s^{2}}\right]dx\\
= & \frac{1}{2}\int\varphi(x\vert\mu,\sigma^{2})\ln(2\pi s^{2})dx+\frac{1}{2s^{2}}\left[\int\varphi(x\vert\mu,\sigma^{2})x^{2}dx-2m\int\varphi(x\vert\mu,\sigma^{2})xdx+\int\varphi(x\vert\mu,\sigma^{2})m^{2}dx\right]\\
= & \frac{1}{2}\ln(2\pi s^{2})+\frac{1}{2s^{2}}\left[\sigma^{2}+\mu^{2}-2m\mu+m^{2}\right].
\end{alignat*}

\end_inset

And similarly 
\begin_inset Formula 
\begin{align*}
(2) & =-\int\varphi(x\vert\mu,\sigma^{2})\ln\left[\varphi(x\vert\mu,\sigma^{2})\right]dx\\
 & =\frac{1}{2}\int\varphi(x\vert\mu,\sigma^{2})\ln(2\pi\sigma^{2})dx+\frac{1}{2\sigma^{2}}\left[\int\varphi(x\vert\mu,\sigma^{2})x^{2}dx-2\mu\int\varphi(x\vert\mu,\sigma^{2})xdx+\mu^{2}\int\varphi(x\vert\mu,\sigma^{2})dx\right]\\
 & =\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{1}{2\sigma^{2}}\left[\sigma^{2}+\mu^{2}-2\mu^{2}+\mu^{2}\right]\\
 & =\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{1}{2}.
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula 
\begin{align*}
KL(p||q) & =\frac{1}{2}\ln(2\pi s^{2})+\frac{1}{2s^{2}}\left[\sigma^{2}+\mu^{2}-2m\mu+m^{2}\right]-\frac{1}{2}\ln(2\pi\sigma^{2})-\frac{1}{2}\\
 & =\frac{1}{2s^{2}}\left[(m-\mu)^{2}+(\sigma^{2}-s^{2})+s^{2}\log\frac{s^{2}}{\sigma^{2}}\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.31 - Differential entropy and independence
\end_layout

\begin_layout Standard
In this problem, we extend the definition to of KL-divergence to a more
 general setting as follows:
\end_layout

\begin_layout Definition
If 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 are probability measures over a set 
\begin_inset Formula $\Omega$
\end_inset

, if 
\begin_inset Formula $P$
\end_inset

 is absolutely continuous w.r.t.
 
\begin_inset Formula $Q$
\end_inset

, then the KL divergence is defined as 
\begin_inset Formula 
\[
KL(P||Q)=\int_{\Omega}\log\frac{dP}{dQ}dP,
\]

\end_inset

where 
\begin_inset Formula $dP/dQ$
\end_inset

 is the Radon-Nikodym derivative, whose existence is guaranteed by the fact
 that 
\begin_inset Formula $P$
\end_inset

 is absolutely continuous w.r.t 
\begin_inset Formula $Q$
\end_inset

.
 
\end_layout

\begin_layout Lemma
\begin_inset Formula $KL(P||Q)\geq0$
\end_inset

 for any pair
\begin_inset CommandInset label
LatexCommand label
name "lem: KL divergence eq"

\end_inset

 of probability measures 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 such that 
\begin_inset Formula $P\ll Q$
\end_inset

, the equality if 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 are equal.
 
\end_layout

\begin_layout Proof
This is a directly application of Jensen's inequality.
 Note that 
\begin_inset Formula 
\[
KL(P||Q)=-\int_{\Omega}\log\frac{dQ}{dP}dP\geq-\log\left(\int_{\Omega}\frac{dQ}{dP}dP\right)=-\log\int_{\Omega}dQ=0.
\]

\end_inset

Recall that Jensen's inequality attains the equality if and only if when
 the function is affine or its argument is constant.
 In this case, 
\begin_inset Formula $\log(t)$
\end_inset

 is not constant, and thus 
\begin_inset Formula $KL(P||Q)=0$
\end_inset

 iff 
\begin_inset Formula $dP/dQ=C$
\end_inset

 for some constant 
\begin_inset Formula $C\in\mathbb{R}$
\end_inset

.
 We claim that 
\begin_inset Formula $C=1,$
\end_inset

 since otherwise we would have 
\begin_inset Formula 
\[
\int_{\Omega}dP=\int_{\Omega}\frac{dP}{dQ}dQ=C\int_{\Omega}dQ=C\neq1,
\]

\end_inset

which is a contradiction since 
\begin_inset Formula $P$
\end_inset

 is a probability measure.
 Then we claim that 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 are equal.
 For any set 
\begin_inset Formula $A$
\end_inset

 in the (predefined) sigma algebra, we have 
\begin_inset Formula 
\[
P(A)=\int_{A}dP=\int_{A}\frac{dP}{dQ}dQ=\int_{A}1dQ=Q(A).
\]

\end_inset

Hence, 
\begin_inset Formula $P=Q.$
\end_inset

 
\end_layout

\begin_layout Standard
Now we come back to the problem.
 We instead use 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 two denote the random variables and 
\begin_inset Formula $f_{X},f_{Y},f_{X,Y}$
\end_inset

, to denote their (marginal) densities.
 Suppose 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
 Then it follows that 
\begin_inset Formula 
\begin{align*}
H(X,Y) & =\int\int f_{X,Y}(x,y)\log f_{X,Y}(x,y)dxdy\\
 & =\int\int f_{X}(x)f_{Y}(y)(\log f_{X}(x)+\log f_{Y}(y))dxdy\\
 & =\int\int f_{X}(x)f_{Y}(y)\log f_{X}(x)dxdy+\int\int f_{X}(x)f_{Y}(y)\log f_{Y}(y)dxdy\\
 & =\int f_{X}(x)\log f_{X}(x)dx+\int f_{Y}(y)\log f_{Y}(y)dy\\
 & =H(X)+H(Y).
\end{align*}

\end_inset

Now on the other hand, suppose 
\begin_inset Formula $H(X,Y)=H(X)+H(Y).$
\end_inset

 Then since 
\begin_inset Formula $H(X,Y)=H(Y\vert X)+H(X)$
\end_inset

 according to Eq.
 (1.112), it follows that 
\begin_inset Formula $H(Y\vert X)=H(Y).$
\end_inset

 Note that 
\begin_inset Formula 
\begin{align*}
H(Y\vert X)-H(Y) & =-\int\int f(x,y)\log f(y\vert x)dxdy+\int\int f(x,y)\log f(y)dxdy\\
 & =\int\int f(x,y)\log\frac{f(y)}{f(y\vert x)}dxdy\\
 & =KL(f_{Y}(y)||f_{Y\vert X}(y\vert x)).
\end{align*}

\end_inset

Then according to 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: KL divergence eq"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $f_{Y}(y)=f_{Y\vert X}(y\vert x)$
\end_inset

 almost surely, and as a result 
\begin_inset Formula $f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)$
\end_inset

 which implies that 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
 
\end_layout

\begin_layout Section
Problem 1.32 - Entropy under linear transformation 
\end_layout

\begin_layout Standard
Since this problem uses transformation theorem, we first recall this classical
 result: 
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
after "Thm. 17.2"
key "billingsley2012probability"
literal "false"

\end_inset


\end_layout

\end_inset

 Let 
\begin_inset Formula $T$
\end_inset

 be a continuously differentiable map of the open set 
\begin_inset Formula $U$
\end_inset

 onto 
\begin_inset Formula $V.$
\end_inset

 Suppose that 
\begin_inset Formula $T$
\end_inset

 is injective an that 
\begin_inset Formula $J(x)\neq0$
\end_inset

 for all 
\begin_inset Formula $x.$
\end_inset

 If 
\begin_inset Formula $f$
\end_inset

 is non-negative, then 
\begin_inset CommandInset label
LatexCommand label
name "thm: transformation thm"

\end_inset


\begin_inset Formula 
\[
\int_{U}f(Tx)\left|J(x)\right|dx=\int_{V=TU}f(y)dy.
\]

\end_inset


\end_layout

\begin_layout Remark
We can use this theorem to the get change of variable formula for random
 variables in 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

 in the following way.
 Suppose 
\begin_inset Formula $X$
\end_inset

 is a random variable in 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

 with density 
\begin_inset Formula $f_{X}$
\end_inset

 and 
\begin_inset Formula $g(\cdot)$
\end_inset

 is a 
\begin_inset Formula $C^{1}$
\end_inset

 diffeomorphism in 
\begin_inset Formula $\mathbb{R}^{d},$
\end_inset

 whose inverse is denoted as 
\begin_inset Formula $T$
\end_inset

 and 
\begin_inset Formula $J_{T}(x)\neq0,$
\end_inset

 then it follows that 
\begin_inset Formula 
\[
\P[g(X)\in A]=\P[X\in g^{-1}(A)]=\P[X\in TA]=\int_{TA}f_{X}(y)dy.
\]

\end_inset

Now apply 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and we get 
\begin_inset Formula 
\[
\int_{TA}f_{X}(y)dy=\int_{A}f_{X}(Tx)|J_{T}(x)|dx=\int_{A}f_{X}(g^{-1}(x))|J_{g^{-1}}(x)|dx.
\]

\end_inset

Hence, from 
\begin_inset Formula 
\begin{align*}
\P(g(X)\in A) & =\int\mathbbm{1}_{A}dF_{g(X)}=\int\mathbbm{1}_{A}\frac{dF_{g(X)}}{dx}dx\tag{\ensuremath{dx} refers to Lebesgue measure}\\
 & =\int_{A}f_{X}(g^{-1}(x))\left|J_{g^{-1}}(x)\right|dx
\end{align*}

\end_inset

and the fact that Radon-Nikodym derivative is unique it follows that 
\begin_inset Formula $g(X)$
\end_inset

 has density of the form 
\begin_inset Formula $f_{X}(g^{-1}(x))\left|J_{g^{-1}}(x)\right|.$
\end_inset


\end_layout

\begin_layout Remark
Now we return to the problem.
 We instead use 
\begin_inset Formula $f_{Y}(y)$
\end_inset

 and 
\begin_inset Formula $f_{X}(x)$
\end_inset

 to denote the density function for 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 First, by previous remark, we see that 
\begin_inset Formula $f_{Y}(y)=f_{X}(A^{-1}y)\left|J_{A^{-1}}\right|=f_{X}(A^{-1}y)\left|\det(A)^{-1}\right|.$
\end_inset

 So,
\begin_inset Formula 
\begin{align*}
H(Y) & =-\int\ln f_{Y}(y)dF_{Y}=\int\ln f_{X}(A^{-1}y)\left|\det(A)^{-1}\right|dF_{Y}\\
 & =-\int\ln\left[f_{X}(A^{-1}y)\left|\det(A)^{-1}\right|\right]f_{X}(A^{-1}y)\left|\det(A)^{-1}\right|dy\\
 & =-\int\ln\left[f_{X}(A^{-1}Ax)\left|\det(A)^{-1}\right|\right]f_{X}(A^{-1}Ax)\left|\det(A)^{-1}\right|\left|\det(A)\right|dx\tag{1}\\
 & =-\int\ln\left[f_{X}(x)\left|\det(A)^{-1}\right|\right]f_{X}(x)dx\\
 & =-\int f_{X}(x)\ln f_{X}(x)dx+\int(\ln\det A)f_{X}(x)dx\\
 & =H(X)+\ln(\det A)
\end{align*}

\end_inset

as desired.
 Note that the justification for Eq.
 (1) is as follows: we abbreviate 
\begin_inset Formula 
\[
\varphi(x)=f_{X}(x)\left|\det(A)^{-1}\right|f_{X}(x)\left|\det(A)^{-1}\right|,
\]

\end_inset

then again by an application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula 
\[
(1)=-\int\varphi\circ Ld\mu=-\det(L^{-1})\int\varphi\circ L\circ L^{-1}d\mu=-\det(L^{-1})\int\varphi d\mu.
\]

\end_inset

where 
\begin_inset Formula $\mu$
\end_inset

 is the Lebesgue measure.
 Here since 
\begin_inset Formula $L$
\end_inset

 is represented by 
\begin_inset Formula $A^{-1},$
\end_inset

 
\begin_inset Formula $L^{-1}$
\end_inset

 is thus represented by 
\begin_inset Formula $A.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.33 - Zero conditional entropy implies singleton concentration
\end_layout

\begin_layout Standard
Instead of 
\begin_inset Formula $x,y,$
\end_inset

 we use 
\begin_inset Formula $X,Y$
\end_inset

 to denote random variables.
 First, we reformulate 
\begin_inset Formula $H(Y\vert X)$
\end_inset

 as follows:
\begin_inset Formula 
\begin{align*}
H(Y\vert X) & =-\sum_{i}\sum_{j}\P(X=x_{i},Y=y_{j})\log\P(Y_{j}=y_{j}\vert X=x_{i})\\
 & =-\sum_{i}\sum_{j}\P(Y=y_{j}\vert X=x_{i})\P(X=x_{i})\log\P(Y_{j}=y_{j}\vert X=x_{i})\\
 & =\sum_{i}\P(X=x_{i})\sum_{j}f(x_{ij}),
\end{align*}

\end_inset

where 
\begin_inset Formula $f:\mathbb{R}^{+}\cup\{0\}\rightarrow\mathbb{R}$
\end_inset

 is defined as 
\begin_inset Formula $x\mapsto-x\log x$
\end_inset

.
 We now observe that 
\begin_inset Formula $f$
\end_inset

 is strictly positive for 
\begin_inset Formula $x\in(0,1)$
\end_inset

 and zero for 
\begin_inset Formula $x=1$
\end_inset

or 
\begin_inset Formula $0$
\end_inset

.
 The latter is straightforward by direct substitution.
 To see the former, note 
\begin_inset Formula 
\[
f(x)=x\log\frac{1}{x}>x\log1=0\text{ for }x\in(0,1).
\]

\end_inset

Without loss of generality, we assume 
\begin_inset Formula $\P(X=x_{i})>0$
\end_inset

 since otherwise we get remove these zeros terms without affect the sum.
 Note that 
\begin_inset Formula 
\begin{align*}
H(Y\vert X)=0 & \implies\sum_{i}x\P(X=x_{i})\sum_{j}f(x_{ij})=0\\
 & \implies\sum_{j}f(x_{ij})=0\text{ \ \ \ \ for any given }i\tag{since \ensuremath{\P(X=x_{i})>0} for any \ensuremath{i} }.
\end{align*}

\end_inset

Since 
\begin_inset Formula $f(x)=0$
\end_inset

 iff 
\begin_inset Formula $x_{ij}=0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

, it follows that for any given 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\P(Y=y_{j}\vert X=x_{i})=0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

 for any 
\begin_inset Formula $j$
\end_inset

.
 Clearly, there must be only 
\begin_inset Formula $j$
\end_inset

 such that 
\begin_inset Formula $\P(Y=y_{j}\vert X=x_{i})=1$
\end_inset

 and 
\begin_inset Formula $\P(Y=y_{j}\vert X=x_{i})=0$
\end_inset

 for all other 
\begin_inset Formula $j$
\end_inset

's since otherwise 
\begin_inset Formula $\sum_{j}\P(Y=y_{j}\vert X=x_{i})\neq0$
\end_inset

, causing a contradiction.
 
\end_layout

\begin_layout Section
Problem 1.34 - Gaussian distribution maximizes entropy under constraints
\end_layout

\begin_layout Standard
To facilitate the notation, we define 
\begin_inset Formula 
\[
F(p(x)=-\int_{\mathbb{R}}p(x)\ln p(x)dx+\lambda_{1}\left(\int_{\R}p(x)dx-1\right)+\lambda_{2}\left(\int_{\mathbb{R}}xp(x)dx-\mu\right)+\lambda_{3}\left(\int_{\mathbb{R}}(x-\mu)^{2}p(x)dx-\sigma^{2}\right).
\]

\end_inset

First, we rearrange to get 
\begin_inset Formula 
\[
F(p(x))=\int_{\mathbb{R}}\underbrace{-p(x)\ln p(x)dx+\lambda_{1}p(x)+\lambda_{2}xp(x)+\lambda_{3}(x-\mu)^{2}p(x)}_{:=G(p(x),x)}dx-(\lambda_{1}+\lambda_{2}\mu+\lambda_{3}\sigma^{2}).
\]

\end_inset

To get the stationary point, we take the functional derivative: 
\begin_inset Formula 
\[
\frac{\delta F(p(x))}{\delta p(x)}=\frac{\partial G(p(x),x)}{\partial p(x)}=-\ln p(x)-1+\lambda_{1}+\lambda_{2}x+\lambda_{3}(x-\mu)^{2}.
\]

\end_inset

Setting it to zero yields, 
\begin_inset Formula 
\[
\ln(p(x))=\lambda_{1}+\lambda_{2}x+\lambda_{3}(x-\mu)^{2}-1\implies p(x)=\exp\{\lambda_{1}+\lambda_{2}x+\lambda_{3}(x-\mu)^{2}-1\}.
\]

\end_inset

Now we need to eliminate the 
\begin_inset Formula $\lambda'$
\end_inset

s by substituting back to the constraints 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\int p(x)dx=1$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\int xp(x)dx=\mu$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\int(x-\mu)^{2}p(x)dx=\sigma^{2}.$
\end_inset


\end_layout

\begin_layout Standard
This is system of integral equations.
 To solve it using first principles would require a lot more work (plus
 I don't know if Gaussian density is the unique solution).
 But since we are only required to show that Gaussian density is indeed
 one solution, we are relieved from the burden of proving uniqueness.
 And we can just directly compare the coefficients.
 Note that 
\begin_inset Formula 
\[
\int_{\mathbb{R}}\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{(x-\mu)^{2}}{2\sigma^{2}}\right\} dx=1.
\]

\end_inset

Hence, if we let 
\begin_inset Formula 
\[
\exp\left\{ \lambda_{2}x+\lambda_{3}(x-\mu)^{2}\right\} =\exp\left\{ -\frac{(x-\mu)^{2}}{2\sigma^{2}}\right\} \implies\lambda_{3}=-\frac{1}{2\sigma^{2}},\lambda_{2}=0\text{ is a solution}
\]

\end_inset

and 
\begin_inset Formula 
\[
\exp\left\{ \lambda_{1}-1\right\} =\frac{1}{(2\pi\sigma^{2})^{1/2}}\implies\lambda_{1}=1-\frac{1}{2}\ln2\pi\sigma^{2}\text{ is a solution}.
\]

\end_inset

Hence, we have shown that we can find admissible 
\begin_inset Formula $\lambda_{1},\lambda_{2},\lambda_{3}$
\end_inset

 such that 
\begin_inset Formula $p(x)$
\end_inset

 satisfies the constraint, and the resulting distribution with this set
 of 
\begin_inset Formula $\lambda$
\end_inset

's is Gaussian.
 Therefore, Gaussian distribution is a minimizer.
 
\end_layout

\begin_layout Remark
One can potentially ask is Gaussian a unique minimizer for this optimization
 problem? I don't know on the top of my head.
 This is equivalent to showing that the solution to the integral constraints
 with 
\begin_inset Formula $p(x)=\exp\{\lambda_{1}+\lambda_{2}x+\lambda_{3}(x-\mu)^{2}-1\}$
\end_inset

, has unique solution.
 I would guess some deep theorems are needed to prove this result, assuming
 it is true.
\end_layout

\begin_layout Section
Problem 1.35 - Entropy of Gaussian
\end_layout

\begin_layout Standard
We let 
\begin_inset Formula $\varphi(x\vert\mu,\sigma^{2})$
\end_inset

 denote the density of Gaussian distribution.
 Let 
\begin_inset Formula $X$
\end_inset

 be a Gaussian random variable, then 
\begin_inset Formula 
\begin{align*}
H(X) & =-\int\varphi(x\vert\mu,\sigma^{2})\ln\left[\varphi(x\vert\mu,\sigma^{2})\right]dx\\
 & =\frac{1}{2}\int\varphi(x\vert\mu,\sigma^{2})\ln(2\pi\sigma^{2})dx+\frac{1}{2\sigma^{2}}\left[\int\varphi(x\vert\mu,\sigma^{2})x^{2}dx-2\mu\int\varphi(x\vert\mu,\sigma^{2})xdx+\mu^{2}\int\varphi(x\vert\mu,\sigma^{2})dx\right]\\
 & =\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{1}{2\sigma^{2}}\left[\sigma^{2}+\mu^{2}-2\mu^{2}+\mu^{2}\right]\\
 & =\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{1}{2}\\
 & =\frac{1}{2}(1+\ln(2\pi\sigma^{2}))
\end{align*}

\end_inset

as desired.
\end_layout

\begin_layout Section
Problem 1.36 - Second order characterization of convexity
\end_layout

\begin_layout Standard
We prove a slightly more generalized version.
 First, we recall the definition of the convexity.
 
\end_layout

\begin_layout Definition
A function 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 is convex if its domain 
\begin_inset Formula $\mathcal{D}_{f}$
\end_inset

 is a convex set and for any 
\begin_inset Formula $x,y\in\mathcal{D}_{f}$
\end_inset

 and 
\begin_inset Formula $\lambda\in[0,1],$
\end_inset

 
\begin_inset Formula 
\[
f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y).\tag{1}
\]

\end_inset

The result of this problem is an direct consequence of the following proposition.
\end_layout

\begin_layout Proposition
Let 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 be a function.
 Then the following statements are equivalent.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $f$
\end_inset

 is convex.
 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $f(y)\geq f(x)+\nabla f(x)^{T}(y-x)$
\end_inset

 assuming 
\begin_inset Formula $f$
\end_inset

 is differentiable.
 
\end_layout

\begin_layout Enumerate
The Hessian matrix 
\begin_inset Formula $H_{f}(x)$
\end_inset

 is positive semidefinite, assuming 
\begin_inset Formula $f$
\end_inset

 is twice differentiable and 
\begin_inset Formula $\mathcal{D}_{f}$
\end_inset

 is open.
 
\end_layout

\end_deeper
\begin_layout Proof
\begin_inset Formula $(1)\Rightarrow(2).$
\end_inset

 Suppose 
\begin_inset Formula $f$
\end_inset

 is convex.
 Then by definition for any 
\begin_inset Formula $y,x\in\mathcal{D}_{f}$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
f(\lambda y+(1-\lambda)x) & =f(x+\lambda(y-x))\\
 & \leq\lambda f(y)+(1-\lambda)f(x)\\
 & =f(x)+\lambda(f(y)-f(x)).
\end{align*}

\end_inset

Rearranging the expression yields 
\begin_inset Formula 
\[
\frac{f(x+\lambda(y-x))-f(x)}{\lambda}\leq f(y)-f(x).
\]

\end_inset

Now we take the limit: 
\begin_inset Formula 
\[
\lim_{\lambda\rightarrow0}\frac{f(x+\lambda(y-x))-f(x)}{\lambda}=\nabla f(x)^{T}(y-x).
\]

\end_inset

This equality can be derived from the following argument: note the Taylor
 expansion of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $x+h$
\end_inset

 is 
\begin_inset Formula 
\[
f(x+th)=f(x)+t\left\langle \nabla f(x),h\right\rangle +o(\left\Vert th\right\Vert ).
\]

\end_inset

Then by rearranging we get 
\begin_inset Formula 
\[
\frac{f(x+th)-f(x)}{t}=\left\langle \nabla f(x),h\right\rangle +\frac{o(\left\Vert th\right\Vert )}{t\left\Vert h\right\Vert }\left\Vert h\right\Vert \xrightarrow{t\rightarrow0}\left\langle \nabla f(x),h\right\rangle =\nabla f(x)^{T}h.
\]

\end_inset

Hence, we have 
\begin_inset Formula 
\[
\nabla f(x)^{T}(y-x)\leq f(y)-f(x)\iff f(y)\geq f(x)+f(x)^{T}(y-x)
\]

\end_inset

as desired.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $(2)\Rightarrow(1)$
\end_inset

.
 Now assume 
\begin_inset Formula $f(y)\geq f(x)+\nabla f(x)^{T}(y-x)$
\end_inset

 for any 
\begin_inset Formula $x,y\in\mathcal{D}_{f}.$
\end_inset

 Fix 
\begin_inset Formula $x,y\in\mathcal{D}_{f}$
\end_inset

.
 Then note that since 
\begin_inset Formula $\mathcal{D}_{f}$
\end_inset

 is convex, 
\begin_inset Formula $\lambda x+(1-\lambda)y\in\mathcal{D}_{f}$
\end_inset

.
 We first apply it to the pair 
\begin_inset Formula $(\lambda x+(1-\lambda)y,y)$
\end_inset

: 
\begin_inset Formula 
\begin{align*}
f(y) & \geq f(\lambda x+(1-\lambda)y)+\nabla f(\lambda x+(1-\lambda)y)^{T}(y-\lambda x-(1-\lambda)y)\\
 & =f(\lambda x+(1-\lambda)y)+\nabla f(\lambda x+(1-\lambda)y)^{T}\lambda(y-x).\tag{2}
\end{align*}

\end_inset

Similarly, we apply it to the pair 
\begin_inset Formula $(\lambda x+(1-\lambda)y,x):$
\end_inset


\begin_inset Formula 
\[
f(x)\geq f(\lambda x+(1-\lambda)y)+\nabla f(\lambda x+(1-\lambda)y)(1-\lambda)(x-y).\tag{3}
\]

\end_inset

Now, we note that for 
\begin_inset Formula $\lambda\in(0,1),$
\end_inset

 
\begin_inset Formula 
\begin{align*}
(1-\lambda)\times\text{Eq.}(2)+\lambda\times\text{Eq.}(3)= & (1-\lambda f(y))+\lambda f(x)\\
\geq & (1-\lambda+\lambda)f(\lambda x+(1-\lambda)y)\\
= & f(\lambda x+(1-\lambda)y),
\end{align*}

\end_inset

which is the definition of convexity in defined in Eq.
 (1).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $(2)\Rightarrow(3).$
\end_inset

 Pick arbitrary 
\begin_inset Formula $x,h\in\mathcal{D}_{f}$
\end_inset

.
 Since 
\begin_inset Formula $\mathcal{D}_{f}$
\end_inset

 is open, we can find a sufficiently small 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $x+\lambda h\in\mathcal{D}_{f}$
\end_inset

.
 We first write out the second order Taylor expansion of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $x+\lambda h,$
\end_inset

 
\begin_inset Formula 
\[
f(x+\lambda h)=f(x)+\lambda\left\langle \nabla f(x),h\right\rangle +\frac{\lambda^{2}}{2}H_{f}(x)(h,h)+o(\left\Vert \lambda h\right\Vert ^{2}).\tag{4}
\]

\end_inset

Since 
\begin_inset Formula $f$
\end_inset

 is convex, it follows that 
\begin_inset Formula $f(x+\lambda h)\geq f(x)+\lambda\left\langle \nabla f(x),h\right\rangle $
\end_inset

.
 Substituting back to Eq.(4) yields 
\begin_inset Formula 
\begin{align*}
\lambda^{2}H_{f}(x)(h,h)+o(\left\Vert \lambda h\right\Vert ^{2})\geq0 & \implies H_{f}(x)(h,h)+\frac{o(\left\Vert \lambda h\right\Vert ^{2})}{\left\Vert \lambda h\right\Vert ^{2}}\left\Vert h\right\Vert ^{2}\geq0\tag{any \ensuremath{\lambda\in(0,1)}}\\
 & \implies\lim_{\lambda\rightarrow0^{+}}\left[H_{f}(x)(h,h)+\frac{o(\left\Vert \lambda h\right\Vert ^{2})}{\left\Vert \lambda h\right\Vert ^{2}}\left\Vert h\right\Vert ^{2}\right]\geq0\\
 & \implies H_{f}(x)(h,h)\geq0.
\end{align*}

\end_inset

Since 
\begin_inset Formula $h$
\end_inset

 is arbitrary, it follows that 
\begin_inset Formula $H_{f}(x)$
\end_inset

 is positive semidefinite.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $(3)\Rightarrow(2).$
\end_inset

 Suppose 
\begin_inset Formula $H_{f}$
\end_inset

 is positive semidefinite.
 Then for any 
\begin_inset Formula $x,y\in\mathcal{D}_{f},$
\end_inset

 since 
\begin_inset Formula $\mathcal{D}_{f}$
\end_inset

 is convex, 
\begin_inset Formula $\lambda x+(1-\lambda)y\in\mathcal{D}_{f}$
\end_inset

 for any 
\begin_inset Formula $\lambda\in(0,1).$
\end_inset

 Then by a second order Taylor formula, we can write 
\begin_inset Formula 
\[
f(y)=f(x)+\left\langle \nabla f(x),y-x\right\rangle +\frac{1}{2}H_{f}(z)(y-x,y-x)
\]

\end_inset

for some 
\begin_inset Formula $z$
\end_inset

 in the segment 
\begin_inset Formula $\left[x,y\right]:=\left\{ \text{all points of form }\lambda x+(1-\lambda)y\text{ for }\lambda\in(0,1)\right\} $
\end_inset

.
 Since 
\begin_inset Formula $H_{f}$
\end_inset

 is positive semidefinite, it follows that 
\begin_inset Formula $f(y)\geq f(x)+\left\langle \nabla f(x),y-x\right\rangle .$
\end_inset

 
\end_layout

\begin_layout Section
Problem 1.37 - Decomposition of joint entropy
\end_layout

\begin_layout Standard
We instead use 
\begin_inset Formula $X,Y$
\end_inset

 to denote the random variable and 
\begin_inset Formula $f_{X},f_{Y}$
\end_inset

 denote marginal distribution and 
\begin_inset Formula $f_{X,Y}$
\end_inset

 joint distribution.
 Note that 
\begin_inset Formula 
\begin{align*}
H(X,Y) & =\int\int f_{X,Y}(x,y)\log f_{X,Y}(x,y)dxdy\\
 & =\int\int f_{X,Y}(x,y)\log f_{Y|X}(y\vert x)f_{X}(x)dxdy\\
 & =\int\int f_{X,Y}(x,y)\log f_{Y\vert X}(y|x)dxdy+\int\int f_{X,Y}(x,y)\log f_{X}(x)dxdy\\
 & =H(Y\vert X)+\int\log f_{X}(x)\left(\int f_{X,Y}(x,y)dy\right)dx\\
 & =H(Y\vert X)+\int f_{X}(x)\log f_{X}(x)dx\\
 & =H(Y\vert X)+H(X),
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.38 - Proof of discrete Jensen's inequality
\end_layout

\begin_layout Standard
We would like to show 
\begin_inset Formula $f(\sum_{i=1}^{M}\lambda_{i}x_{i})\leq\sum_{i=1}^{M}\lambda_{i}f(x_{i})$
\end_inset

 for any set of point 
\begin_inset Formula $\{x_{i}\}_{i=1}^{M}$
\end_inset

 under the assumption that 
\begin_inset Formula $\lambda_{i}\geq0$
\end_inset

 and 
\begin_inset Formula $\sum\lambda_{i}=1$
\end_inset

 and 
\begin_inset Formula $f$
\end_inset

 is convex.
 We show this by inducting on 
\begin_inset Formula $M$
\end_inset

.
 For the base case, note that 
\begin_inset Formula $M=2$
\end_inset

 holds trivially, since by definition of convexity, 
\begin_inset Formula 
\[
f(\lambda_{1}x_{1}+\lambda_{2}x_{2})=f(\lambda_{1}x_{1}+(1-\lambda_{1})x_{2})\leq\lambda_{1}f(x_{1})+(1-\lambda_{2})f(x_{2})=\lambda_{1}f(x_{1})+\lambda_{2}f(x_{2}).
\]

\end_inset

Now suppose the claim holds for 
\begin_inset Formula $M=k.$
\end_inset

 Then for 
\begin_inset Formula $M=k+1,$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
f\left(\sum_{i=1}^{k+1}\lambda_{i}x_{i}\right) & =f\left(\sum_{i=1}^{k}\lambda_{i}x_{i}+\lambda_{k+1}x_{k+1}\right)\\
 & =f\left((1-\lambda_{k+1})\left(\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}x_{i}\right)+\lambda_{k+1}x_{k+1}\right)\\
 & \leq(1-\lambda_{k+1})f\left(\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}x_{i}\right)+\lambda_{k+1}f(x_{k+1})\tag{1}
\end{align*}

\end_inset

where the last inequality follows by treating 
\begin_inset Formula $\sum_{i=1}^{k}\lambda_{i}x_{i}/(1-\lambda_{k+1})$
\end_inset

 as a singleton point and applying the base case.
 Now note 
\begin_inset Formula 
\[
\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}=\frac{1-\lambda_{k+1}}{1-\lambda_{k+1}}=1.
\]

\end_inset

It follows from induction hypothesis that 
\begin_inset Formula 
\[
f\left(\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}x_{i}\right)\leq\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}f(x_{i}).
\]

\end_inset

Now substituting it back to Eq.(1) and we get 
\begin_inset Formula 
\begin{align*}
\text{Eq.(1)} & \leq(1-\lambda_{k+1})\sum_{i=1}^{k}\frac{\lambda_{i}}{1-\lambda_{k+1}}f(x_{i})+\lambda_{k+1}f(x_{k+1})=\sum_{i=1}^{k+1}\lambda_{i}f(x_{i})
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.39 - Calculation of entropy and mutual information 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

To find 
\begin_inset Formula $H(X),$
\end_inset

 note 
\begin_inset Formula 
\begin{align*}
H(X) & =-\sum_{x\in\{0,1\}}f_{X}(x)\log f_{X}(x)=-\frac{2}{3}\log\frac{2}{3}-\frac{1}{3}\log\frac{1}{3}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find 
\begin_inset Formula $H(Y),$
\end_inset

 note 
\begin_inset Formula 
\[
H(Y)=-\sum_{y\in\{0,1\}}f_{Y}(y)\log f_{Y}(y)=-\frac{2}{3}\log\frac{2}{3}-\frac{1}{3}\log\frac{1}{3}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find 
\begin_inset Formula $H(X\vert Y),$
\end_inset

 we need to find 
\begin_inset Formula $f_{X\vert Y}(x\vert y).$
\end_inset

 Note that 
\begin_inset Formula 
\[
f_{X\vert Y}(x\vert y)=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\begin{cases}
1 & \text{if }x=0,y=0\\
0 & \text{if }x=1,y=0\\
\frac{1}{2} & \text{if }x=1,y=1\ \text{or }x=0,y=1.
\end{cases}
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
H(X\vert Y)-=\sum_{(x,y)\in\{0,1)\times\{0,1\}}f_{X,Y}(x,y)\log f_{X\vert Y}(x\vert y)=-\frac{2}{3}\log\frac{1}{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Similarly, to find 
\begin_inset Formula $H(Y|X)$
\end_inset

, note that since 
\begin_inset Formula 
\[
f_{Y\vert X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\begin{cases}
0 & \text{if }x=1,y=0\\
1 & \text{if }x=1,y=1\\
\frac{1}{2} & \text{if }x=0,y=0\text{ or }x=0,y=1
\end{cases},
\]

\end_inset

it follows that 
\begin_inset Formula 
\[
H(Y\vert X)=-\sum_{(x,y)\in\{0,1\}\times\{0,1\}}f_{X,Y}(x,y)\log f_{Y\vert X}(y\vert x)=-\frac{2}{3}\log\frac{1}{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find 
\begin_inset Formula $H(X,Y),$
\end_inset

 note 
\begin_inset Formula 
\[
H(X,Y)=-\sum_{(x,y)\in\{0,1\}\times\{0,1\}}f_{X,Y}(x,y)\log f_{X,Y}(x,y)=\log3.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Finally, to find 
\begin_inset Formula $I(X,Y),$
\end_inset

 we note that 
\begin_inset Formula 
\[
I(X,Y)=H(X)-H(X\vert Y)=\frac{2}{3}\log\frac{3}{4}+\frac{1}{3}\log3.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.40 - Proof of AM-GM using Jensen's inequality
\end_layout

\begin_layout Standard
Since the function 
\begin_inset Formula $x\mapsto\log x$
\end_inset

 is concave, it follows that for any set of points 
\begin_inset Formula $\{x_{i}\}_{i=1}^{N},N\in\mathbb{N}$
\end_inset

 we have 
\begin_inset Formula 
\[
\ln\left(\sum_{i=1}^{N}\frac{1}{N}x_{i}\right)\geq\sum_{i=1}^{N}\frac{1}{N}\log(x_{i})=\log\left(\prod_{i=1}^{N}\sqrt[N]{x_{i}}\right).
\]

\end_inset

Next, since 
\begin_inset Formula $x\mapsto\exp(x)$
\end_inset

 preserves monotonicity, it follows that 
\begin_inset Formula $\sum_{i=1}^{N}\frac{1}{N}x_{i}\geq\prod_{i=1}^{N}\sqrt[^{N}]{x_{i}}$
\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.41 - Characterization of mutual information
\end_layout

\begin_layout Standard
To show that desired equality, note that 
\begin_inset Formula 
\begin{align*}
I(X,Y) & =-\int\int f_{X,Y}(x,y)\log\frac{f_{X}(x)f_{Y}(y)}{f_{X,Y}(x,y)}dxdy\\
 & =-\int\int f_{X,Y}(x,y)\log f_{X}(x)dxdy-\left(-\int\int f_{X,Y}(x,y)\log\frac{f_{X,Y}(x,y)}{f_{Y}(y)}dxdy\right)\\
 & =-\int\log f_{X}(x)\left(\int f_{X,Y}(x,y)dy\right)dx-\left(-\int\int f_{X,Y}(x,y)\log f_{X\vert Y}(x\vert y)dxdy\right)\\
 & =\left(-\int f_{X}(x)\log f_{X}(x)dx\right)-\left(-\int\int f_{X,Y}(x,y)\log f_{X\vert Y}(x\vert y)dxdy\right)\\
 & =H(X)-H(X\vert Y).
\end{align*}

\end_inset

That 
\begin_inset Formula $I(X,Y)=H(Y)-H(Y\vert X)$
\end_inset

 follows by the same argument but swapping 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 
\end_layout

\end_body
\end_document

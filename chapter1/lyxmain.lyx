#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{"/config/config.tex"}
\end_preamble
\use_default_options true
\master ../lyxmain.lyx
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.12
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 2
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3.2cm
\topmargin 3cm
\rightmargin 3.2cm
\bottommargin 3cm
\secnumdepth -1
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Solutions for exercises to chapter 1
\end_layout

\begin_layout Section
Problem 1.1 [closed form solution to polynomial regression]
\end_layout

\begin_layout Standard
We use a slightly better notation to write this problem.
 Let 
\begin_inset Formula $X$
\end_inset

 be the matrix of the form 
\begin_inset Formula 
\[
X=\begin{bmatrix}x_{1}^{0} & x_{1}^{1} & \cdots & x_{1}^{M}\\
x_{2}^{0} & x_{2}^{1} & \cdots & x_{2}^{M}\\
\vdots & \vdots & \ddots & \vdots\\
x_{N}^{0} & x_{N}^{1} & \cdots & x_{N}^{M}
\end{bmatrix},\ \ t=\begin{bmatrix}t_{1}\\
t_{2}\\
\vdots\\
t_{n}
\end{bmatrix}
\]

\end_inset

The the problem can be rewritten in the following form: 
\begin_inset Formula 
\begin{align*}
E(w) & =\frac{1}{2}\left(\left(Xw-t\right)^{T}\left(Xw-t\right)\right).
\end{align*}

\end_inset

Now we differentiate w.r.t 
\begin_inset Formula $w$
\end_inset

, note that 
\begin_inset Formula 
\begin{align*}
E(w+h) & =\frac{1}{2}\left(X\left(w+h\right)-t\right)^{T}\left(X\left(w+h\right)-t\right)\\
 & =\frac{1}{2}\left(\left(Xw-t\right)^{T}+\left(Xh\right)^{T}\right)\left(Xw-t+Xh\right)\\
 & =\frac{1}{2}\left[\left(Xw-t\right)^{T}\left(Xw-t\right)+\left(Xw-t\right)^{T}Xh+\left(Xh\right)^{T}\left(Xw-t\right)+\left(Xh\right)^{T}\left(Xh\right)\right]\\
 & =E\left(w\right)+\left\langle \left(Xw-t\right)^{T},Xh\right\rangle +\frac{1}{2}\left\langle Xh,Xh\right\rangle \\
 & =E\left(w\right)+\left\langle X^{T}\left(Xw-t\right),h\right\rangle +\frac{1}{2}\left\langle Xh,Xh\right\rangle .
\end{align*}

\end_inset

Note that 
\begin_inset Formula $\left\langle X^{T}\left(Xw-t\right),h\right\rangle \in\text{Hom}(\mathbb{R}^{M+1},\mathbb{R})$
\end_inset

 and 
\begin_inset Formula 
\[
\frac{1}{2}\left\langle Xh,Xh\right\rangle \leq\frac{1}{2}\left\Vert Xh\right\Vert \left\Vert Xh\right\Vert \leq\frac{C}{2}\left\Vert X\right\Vert _{\infty}^{2}\left\Vert h\right\Vert \xrightarrow{\left\Vert h\right\Vert \rightarrow0}0,
\]

\end_inset

it follows that 
\begin_inset Formula $\nabla E(w)=X^{T}(Xw-t).$
\end_inset

 Set it to zero and we get 
\begin_inset Formula 
\[
X^{T}(Xw-t)=0\iff X^{T}Xw=X^{\top}t.
\]

\end_inset

So 
\begin_inset Formula $X^{T}X$
\end_inset

 is the 
\begin_inset Formula $A$
\end_inset

 proposed in the problem.
 
\begin_inset Formula 
\[
\left[X^{T}X\right]_{ij}=\sum_{n=1}^{N}\left(x_{n}^{i}x_{n}^{j}\right)=\sum_{n=1}^{N}x_{n}^{i+j},\text{ and }\left[X^{T}t\right]_{i}=\sum_{n=1}^{N}x_{n}^{i}t_{n},
\]

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.2 [closed form solution to regularized polynomial regression]
\end_layout

\begin_layout Standard
We use the same notation as in the previous problem and still rewrite the
 loss function in matrix form as follows: 
\begin_inset Formula 
\[
\widetilde{E}(w)=\frac{1}{2}\left\langle Xw-t,Xw-t\right\rangle +\frac{\lambda}{2}\left\langle w,w\right\rangle .
\]

\end_inset

Still we differentiate the expression.
 Note that if we let 
\begin_inset Formula $\varphi(w)=\frac{\lambda}{2}\left\langle w,w\right\rangle ,$
\end_inset

we have that 
\begin_inset Formula 
\begin{align*}
\varphi(w+h) & =\frac{\lambda}{2}\left(w+h\right)^{T}\left(w+h\right)\\
 & =\frac{\lambda}{2}\left(w^{T}w+w^{T}h+h^{T}x+\left\Vert h\right\Vert \right)\\
 & =\varphi\left(w\right)+\left\langle \lambda w,h\right\rangle +\underbrace{\frac{\lambda}{2}\left\Vert h\right\Vert }_{=o(\left\Vert h\right\Vert )}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula $\nabla\varphi(w)=\lambda w,$
\end_inset

 and as a result 
\begin_inset Formula 
\[
\nabla\widetilde{E}(w)=\nabla E(w)+\nabla\varphi(w)=X^{T}(Xw-t)+\lambda w.
\]

\end_inset

Setting it to zero: 
\begin_inset Formula 
\[
X^{T}(Xw-t)+\lambda w=0\iff(X^{T}X+\lambda I)w=X^{T}t.
\]

\end_inset

Hence, 
\begin_inset Formula $(X^{T}X+\lambda I)$
\end_inset

 and 
\begin_inset Formula $X^{T}t$
\end_inset

 are the corresponding matrices.
\end_layout

\begin_layout Section
Problem 1.3 [bayes formula warm up]
\end_layout

\begin_layout Standard
According to the Bayes formula, we get that 
\begin_inset Formula 
\begin{align*}
P\left(\text{apple}\right) & =P\left(\text{apple}\vert\text{r}\right)P\left(\text{r}\right)+P\left(\text{apple}\vert\text{g}\right)P\left(\text{g}\right)+P\left(\text{apple}\vert\text{b}\right)P\left(\text{b}\right)\\
 & =\frac{3}{10}\cdot\frac{2}{10}+\frac{1}{2}\frac{2}{10}+\frac{3}{10}\frac{6}{10}=\frac{17}{50}.
\end{align*}

\end_inset

And again, we can use formula to get 
\begin_inset Formula 
\begin{align*}
P\left(\text{g}\vert\text{orange}\right) & =\frac{P\left(\text{orange}\vert\text{g}\right)P\left(\text{g}\right)}{P\left(\text{orange}\vert\text{g}\right)P\left(\text{g}\right)+P\left(\text{orange}\vert\text{b}\right)P\left(\text{b}\right)+P\left(\text{orange}\vert\text{r}\right)P\left(\text{r}\right)}\\
 & =\frac{\frac{3}{10}\frac{6}{10}}{\frac{3}{10}\frac{6}{10}+\frac{2}{10}\frac{1}{2}+\frac{2}{10}\frac{4}{10}}\\
 & =\frac{1}{2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.4 [nonlinear transform of likelihood function doesn't preserve
 its extrema]
\end_layout

\begin_layout Standard
We first observe that if 
\begin_inset Formula $x_{*}$
\end_inset

 maximizes the likelihood function 
\begin_inset Formula $p_{x}(x)$
\end_inset

, then 
\begin_inset Formula $p_{x}'(x_{*})=0.$
\end_inset

 By chain rule, we have that 
\begin_inset Formula 
\begin{align*}
\frac{dp_{x}(g(y))\left|g'\left(y\right)\right|}{dy} & =\frac{dp_{x}(g(y))}{dy}\left|g'(y)\right|+p_{x}(g(y))\frac{d\left|g'(y)\right|}{dy}\\
 & =\frac{dp_{x}(g(y))}{dg(y)}\frac{dg(y)}{dy}\left|g'(y)\right|+p_{x}(g(y))\frac{d\left|g'(y)\right|}{dy}\tag{1}.
\end{align*}

\end_inset

Hence, if 
\begin_inset Formula $x_{*}=g(y_{*})$
\end_inset

, the 
\begin_inset Formula 
\[
\frac{dp_{x}(g(y_{*}))}{dg(y_{*})}=\frac{dp_{x}(x_{*})}{dx_{*}}=0.
\]

\end_inset

However, there is no guarantee that the second term of the RHS of Eq.
 1 is zero.
 For example, if 
\begin_inset Formula $p_{x}(x)=2x$
\end_inset

 for 
\begin_inset Formula $0\leq x\leq1$
\end_inset

 and 
\begin_inset Formula $x=\sin(y),$
\end_inset

 where 
\begin_inset Formula $0\leq y\leq\pi/2.$
\end_inset

 Then according to the transformation formula, we have that 
\begin_inset Formula 
\[
p_{y}(y)=p_{x}(g(y))g'(y)=2\sin(y)\cos(y)=\sin(2y)\text{ for }0\leq y\leq\frac{\pi}{2}.
\]

\end_inset

Clearly, 
\begin_inset Formula $p_{y}(y)$
\end_inset

 reaches its peak at 
\begin_inset Formula $y=\pi/4$
\end_inset

 but 
\begin_inset Formula $\sin(\pi/4)\neq x_{*}=1.$
\end_inset

 Thus, we have found a counterexample.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

On the other hand, if 
\begin_inset Formula $g(y)$
\end_inset

 is an affine map, then 
\begin_inset Formula $g'(y)$
\end_inset

 is a constant map and as a result 
\begin_inset Formula 
\[
\frac{d\left|g'(y)\right|}{dy}=0
\]

\end_inset

 
\end_layout

\begin_layout Section
Problem 1.5 [characterization of variance]
\end_layout

\begin_layout Standard
It suffices to show that 
\begin_inset Formula $\mathrm{Var}[X]=\E[X^{2}]-(\E[X])^{2}$
\end_inset

 since any a measurable function of a random variable is again a random
 variable and in this case 
\begin_inset Formula $f$
\end_inset

 although is not mentioned, it is safe to assume in this context that 
\begin_inset Formula $f$
\end_inset

 is measurable.
 So note 
\begin_inset Formula 
\begin{align*}
\mathrm{Var}[X] & =\E[X-\E[X]]^{2}\\
 & =\E[X^{2}]-2\E[X]\E[X]+(\E[X])^{2}\\
 & =\E[X^{2}]-\E[X]^{2}
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 1.6 [covariance of two independent r.v.
 is zero]
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $X\perp Y,$
\end_inset

 then it follows that 
\begin_inset Formula $\E[XY]=\E[X]\E[Y]$
\end_inset

.
 Then we have 
\begin_inset Formula 
\begin{align*}
\mathrm{Cov}(X,Y) & =\E[(X-\E[X])(Y-\E[Y])]\\
 & =\E[XY-X\E[Y]-\E[X]Y+\E[X]\E[Y]]\\
 & =\E[XY]-\E[X]\E[Y]-\E[X]\E[Y]+\E[X]\E[Y]\\
 & =\E[X]\E[Y]-\E[X]\E[Y]-\E[X]\E[Y]+\E[X]\E[Y]\\
 & =0.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.7 [gaussian integral via polar coordinate]
\end_layout

\begin_layout Standard
First, we write 
\begin_inset Formula 
\begin{align*}
I^{2} & =\left(\int_{\R}\exp\bigg\{-\frac{1}{2\sigma^{2}}x^{2}\bigg\} dx\right)\left(\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}y^{2}\right\} dy\right)\\
 & =\int\int_{\R\times\R}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(x^{2}+y^{2}\right)\right\} dxdx.
\end{align*}

\end_inset

Now using polar coordinate - let 
\begin_inset Formula $x=r\cos\theta$
\end_inset

 and 
\begin_inset Formula $y=r\sin\theta$
\end_inset

.
 Then we get the Jacobian matrix as 
\begin_inset Formula 
\[
\frac{\partial(x,y)}{\partial(r,\theta)}=\begin{bmatrix}\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{bmatrix}\implies\left|\frac{\partial(x,y)}{\partial(r,\theta)}\right|=r(\cos\theta^{2}+\sin\theta^{2})=r.
\]

\end_inset

Hence, as a result 
\begin_inset Formula 
\begin{align*}
I^{2} & =\int_{0}^{2\pi}\int_{0}^{\infty}\exp\left\{ -\frac{r^{2}}{2\sigma^{2}}\right\} rdrd\theta\\
 & =_{1}\int_{0}^{2\pi}\int_{0}^{\infty}\exp(-u)\sigma^{2}dud\theta\\
 & =\int_{0}^{2\pi}\sigma^{2}d\theta\int_{0}^{\infty}\exp(-u)du\\
 & =2\pi\sigma^{2}\left[-\exp(-u)\right]_{0}^{\infty}=2\pi\sigma^{2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 1.8 [second moment of gaussian integral via Feymann's trick]
\end_layout

\begin_layout Standard
The differentiation under the integral needs a bit more theoretical justificatio
n.
 We won't reproduce the related theorems here.
 But they could be found in e.g.
 Theorem 3.2, Theorem 3.3 in Chapter XIII of 
\begin_inset CommandInset citation
LatexCommand cite
key "sergeundergrad"
literal "false"

\end_inset

 or in 
\begin_inset CommandInset citation
LatexCommand cite
key "diffsign"
literal "false"

\end_inset

.
 With this in mind, we get 
\begin_inset Formula 
\begin{align*}
\frac{d}{d\sigma^{2}}\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} dx & =\int_{\R}\frac{d}{d\sigma^{2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} dx\\
 & =\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}\left(-\frac{1}{2}\right)(\sigma^{-2})^{2}dx
\end{align*}

\end_inset

On the the other hand, we have 
\begin_inset Formula 
\[
\frac{d}{d\sigma^{2}}(2\pi\sigma^{2})^{1/2}=-\frac{1}{2}(2\pi)(\sigma^{2})^{-1/2}.
\]

\end_inset

So combined together, we get 
\begin_inset Formula 
\[
\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}\left(-\frac{1}{2}\right)(\sigma^{-2})^{2}dx=\left(-\frac{1}{2}\right)(2\pi)^{1/2}(\sigma^{2})^{-1/2}.
\]

\end_inset

One step of reduction, we get 
\begin_inset Formula 
\begin{align*}
\E[(x-\E[x])^{2}] & =\mathrm{Var}[x]\\
 & =\frac{1}{(2\pi\sigma^{2})^{1/2}}\int_{\R}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} (x-\mu)^{2}dx\\
 & =\sigma^{2}.
\end{align*}

\end_inset

And as a result, 
\begin_inset Formula 
\[
\E[x^{2}]=\mathrm{Var}[x]+(\E[x])^{2}=\sigma^{2}+\mu^{2}.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 1.9 [gaussian density peaks at mean]
\end_layout

\begin_layout Standard
It suffices to show the result holds in the multidimensional case since
 1-dim is just a special case.
 Recall that the density of the Gaussian distribution in 
\begin_inset Formula $D$
\end_inset

 dimension is 
\begin_inset Formula 
\[
N(x\vert u,\Sigma)=\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} .
\]

\end_inset

Differentiate w.r.t.
 
\begin_inset Formula $x$
\end_inset

 and we get: 
\begin_inset Formula 
\[
\nabla_{x}N(x\vert u,\Sigma)=\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} \nabla_{x}\left(\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right).
\]

\end_inset

Now note that 
\begin_inset Formula $\varphi(x)=(x-\mu)^{T}\Sigma^{-1}(x-\mu)$
\end_inset

 for 
\begin_inset Formula $x\in\R^{d}$
\end_inset

 , then note for any 
\begin_inset Formula $h\in\mathbb{R}^{D}$
\end_inset


\begin_inset Formula 
\begin{align*}
\varphi(x+h) & =(x-u+h)^{T}\Sigma^{-1}(x-\mu+h)\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu+h)+h^{T}\Sigma^{-1}(x-\mu+h)\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu)+(x-\mu)^{T}\Sigma^{-1}h+h^{T}\Sigma^{-1}(x-\mu)+h^{T}\Sigma^{-1}h\\
 & =(x-\mu)^{T}\Sigma^{-1}(x-\mu)+\left\langle 2\Sigma^{-1}(x-\mu),h\right\rangle +h^{T}\Sigma^{-1}h
\end{align*}

\end_inset

Note that and 
\begin_inset Formula 
\[
h^{T}\Sigma^{-1}h=\left\langle h\Sigma^{-1/2},h\Sigma^{-1/2}\right\rangle \leq\left\Vert h\Sigma^{-1/2}\right\Vert ^{2}\leq C\left\Vert h\right\Vert ^{2}\left\Vert \Sigma\right\Vert _{\infty}^{2}=o(\left\Vert h\right\Vert ),
\]

\end_inset

and that 
\begin_inset Formula $\left\langle 2\Sigma^{-1}(x-\mu),h\right\rangle \in\text{Hom}(\mathbb{R}^{d},\mathbb{R}).$
\end_inset

 It follows that 
\begin_inset Formula 
\[
\nabla_{x}\varphi(x)=2\Sigma^{-1}(x-\mu),
\]

\end_inset

whence 
\begin_inset Formula 
\[
\nabla_{x}\varphi(x)=0\iff2\Sigma^{-1}(x-\mu)=0\iff x=\mu.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 1.10 [linearity of expectation and variance]
\end_layout

\begin_layout Enumerate
Note 
\begin_inset Formula 
\begin{align*}
\E\left[x+y\right] & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}(x+y)f_{(x,y)}(x,y)dxdy\\
 & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}(x+y)f_{x}(x)f_{y}(y)dxdy\\
 & =\int_{\text{supp}(x)}\int_{\text{supp}(y)}xf_{x}(x)f_{y}(y)dxdy+\int_{\text{supp}(x)}\int_{\text{supp}(y)}yf_{x}(x)f_{y}(y)dxdy\\
 & =\int_{\text{supp}(x)}xf_{x}(x)dx\int_{\text{supp}(y)}f_{y}(y)+\int_{\text{supp}(x)}f_{x}(x)dx\int_{\text{supp}(y)}yf_{y}(y)\\
 & =\E[x]+\E[y].
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Note 
\begin_inset Formula 
\begin{align*}
\mathrm{Var}[x+y] & =\E[x+y]^{2}-(\E[x+y])^{2}\\
 & =\E[x^{2}]+\E[y^{2}]+\underbrace{2\E[xy]}_{\E[x]\E[y]}-(\E[x])^{2}-(\E[y])^{2}-2\E[x]\E[y]\\
 & =\E[x^{2}]-(\E[x])^{2}+\E[y^{2}]-(\E[y])^{2}\\
 & =\mathrm{Var}[x]+\mathrm{Var}[y].
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.11 [MLE of gaussian]
\end_layout

\begin_layout Standard
Recall that the log-likelihood function for Gaussian distribution is 
\begin_inset Formula 
\[
\ln p(x\vert\mu,\sigma^{2})=-\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(x_{n}-\mu)^{2}-\frac{N}{2}\ln\sigma^{2}-\frac{N}{2}\ln(2\pi).
\]

\end_inset

Now we differentiate it w.r.t.
 
\begin_inset Formula $\mu$
\end_inset

 and setting it to zero: 
\begin_inset Formula 
\[
\frac{\partial\ln p(x\vert\mu,\sigma^{2})}{\partial\mu}=-\frac{1}{2\sigma^{2}}\cdot2\cdot\sum_{i=1}^{N}(x_{n}-\mu)=0\iff\sum_{i=1}^{N}(x_{n}-\mu)=0\iff\mu_{ML}=\frac{1}{n}\sum_{i=1}^{N}x_{n}.
\]

\end_inset

Now we differentiate it w.r.t.
 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and setting it to zero: 
\begin_inset Formula 
\begin{align*}
\frac{\partial\ln(p\vert\mu,\sigma^{2})}{\partial\sigma^{2}} & =\underbrace{\sum_{n=1}^{N}(x_{n}-\mu)^{2}\left(-\frac{1}{2}\right)(-1)(\sigma^{2})^{-2}-\frac{N}{2\sigma^{2}}=0}_{(\star)}.
\end{align*}

\end_inset

To rearrange, we get 
\begin_inset Formula 
\begin{align*}
(\star) & \iff\sum_{n=1}^{N}(x_{n}-\mu)^{2}\sigma^{-4}=\frac{N}{\sigma^{2}}\\
 & \iff\sum_{n=1}^{N}(x_{n}-\mu)^{2}=\sigma^{2}N\\
 & \iff\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu)^{2}.
\end{align*}

\end_inset

Plug in 
\begin_inset Formula $\mu=\mu_{ML}$
\end_inset

 we get 
\begin_inset Formula $\sigma_{ML}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu)^{2}$
\end_inset

 as desired.
 
\end_layout

\begin_layout Section
Problem 1.12 [inconsistency gaussian MLE]
\end_layout

\begin_layout Section
Problem 1.14 [independent terms of 2-nd order term in polynomial] 
\end_layout

\begin_layout Standard
We rewrite the sum in matrix form: 
\begin_inset Formula $\sum_{i=1}^{D}\sum_{j=1}^{D}w_{ij}x_{i}x_{j}=x^{T}Wx,$
\end_inset

 where 
\begin_inset Formula $[W]_{ij}=w_{ij}.$
\end_inset

 Define 
\begin_inset Formula 
\[
W_{S}=\frac{1}{2}(W+W^{T})\text{ and }W_{A}=\frac{1}{2}(W-W^{T}).
\]

\end_inset

Clearly, 
\begin_inset Formula $W_{S}$
\end_inset

 is symmetric and 
\begin_inset Formula $W_{A}^{T}=\frac{1}{2}(W^{T}-W)=-W_{A}$
\end_inset

 is anti-symmetric and 
\begin_inset Formula $W_{S}+W_{A}=W.$
\end_inset

 Therefore, 
\begin_inset Formula 
\[
x^{T}Wx=x^{T}(W_{S}+W_{A})x=x^{T}W_{S}x+x^{T}W_{A}x.
\]

\end_inset

Notice that 
\begin_inset Formula 
\[
x^{T}W_{A}x=\frac{1}{2}(x^{T}W_{S}x-x^{T}W^{T}x)=\frac{1}{2}(x^{T}W_{S}s-x^{T}Wx)=0,
\]

\end_inset

where the last inequality follows from the fact that 
\begin_inset Formula $x^{T}W^{T}x$
\end_inset

 is a scalar and is equal to 
\begin_inset Formula $x^{T}Wx.$
\end_inset

 Since we have shown the sum, 
\begin_inset Formula $\sum_{i,j}w_{ij}x_{i}x_{j}$
\end_inset

, only depends on a symmetric matrix, 
\begin_inset Formula $W_{S}$
\end_inset

, whose independent items is of the cardinality of 
\begin_inset Formula $\sum_{i=1}^{D}i=D(D+1)/2$
\end_inset

 if we assume its of dimension 
\begin_inset Formula $D\times D$
\end_inset

, we have established our claim.
 
\end_layout

\begin_layout Section*
Problem 1.15 [independent terms of 
\begin_inset Formula $M$
\end_inset

-th order term in polynomial]
\end_layout

\begin_layout Enumerate
Since by writing the 
\begin_inset Formula $M$
\end_inset

-th order in the form of 
\begin_inset Formula 
\[
\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{D}\cdots\sum_{i_{M}=1}^{D}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}
\]

\end_inset

introduces duplicate terms, e.g.
 if 
\begin_inset Formula $w_{1,3,2}x_{1}x_{3}x_{2}$
\end_inset

 and 
\begin_inset Formula $w_{2,3,1}x_{2}x_{3}x_{1}$
\end_inset

 are the same and can be combined into 
\begin_inset Formula $(w_{1,3,2}+w_{2,3,1})x_{1}x_{2}x_{3},$
\end_inset

 we can introduce an ordering that prevents such duplication from happening.
 Rewrite the sum in the newly introduced ordering yields 
\begin_inset Formula 
\[
\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}.
\]

\end_inset

Thus, we have 
\begin_inset Formula 
\begin{align*}
n(D,M) & =\sum_{i_{1}=1}^{D}\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}\\
 & =\sum_{i_{1}=1}^{D}\left(\sum_{i_{2}=1}^{i_{1}}\cdots\sum_{i_{M}=1}^{i_{M-1}}w_{i_{1},i_{2},\cdots,i_{M}}x_{i_{1}}x_{i_{2}}\cdots x_{i_{M}}\right)\\
 & =\sum_{i_{1}=1}^{D}n(i_{1},M-1).
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To show the equality holds using induction, we note for the base case of
 
\begin_inset Formula $D=1,$
\end_inset

 
\begin_inset Formula 
\[
\text{LHS}=\frac{(1+M-2)!}{0!(M-1)!}=\frac{(M-1)!}{(M-1)!}=1.
\]

\end_inset

And 
\begin_inset Formula 
\[
\text{RHS}=\frac{(1+M-1)!}{(D-1)!M!}=\frac{M!}{M!}=1.
\]

\end_inset

Now suppose 
\begin_inset Formula $D=k$
\end_inset

 and the equality holds.
 Then 
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{k+1}\frac{(i+M-2)!}{(i-1)!(M-1)!} & =\sum_{i=1}^{k}\frac{(i+M-2)!}{(i-1)!(M-1)!}+\frac{(k+1+M-2)!}{k!(M-1)!}\\
 & =\frac{(k+M-1)!}{(k-1)!M!}+\frac{(k+M-1)!}{k!(M-1)!}\tag{1}\\
 & =\frac{(k+M-1)!(k+M)}{k!(M-1)!}\\
 & =\frac{(k+M)!}{k!M!}\\
 & =\frac{((k+1)+M-1)!}{(k+1-1)!M!},
\end{align*}

\end_inset

where Eq.
 (1) follows from induction hypothesis.
 
\end_layout

\begin_layout Enumerate
We establish the identity by inducting on 
\begin_inset Formula $M$
\end_inset

.
 By Problem 1.14, it follows that 
\begin_inset Formula 
\[
n(D,2)=\frac{1}{2}D(D+1)=\frac{(D+2-1)!}{(D-1)!2!}=\frac{(D+1)!}{(D-1)!2!},
\]

\end_inset

which proves the base case.
 Now suppose the statement holds for 
\begin_inset Formula $M=k$
\end_inset

.
 Then for 
\begin_inset Formula $M=k+1$
\end_inset

, we have 
\begin_inset Formula 
\[
n(D,k+1)=\sum_{i=1}^{D}n(i,k)=\sum_{i=1}^{D}\frac{(i+M-2)!}{(i-1)!(M-1)!}=\frac{(D+M-1)!}{(D-1)!M!}
\]

\end_inset

using part-2.
 
\end_layout

\begin_layout Section
Problem 1.16 [independent terms of high order polynomial]
\end_layout

\begin_layout Enumerate
The first equality just follows from that summing up all the independent
 terms: 
\begin_inset Formula 
\[
N(D,M)=\sum_{i=0}^{M}n(D,i).
\]

\end_inset


\end_layout

\begin_layout Enumerate
We prove this inequality by inducting on 
\begin_inset Formula $M.$
\end_inset

 Now for the base case, 
\begin_inset Formula $M=0,$
\end_inset

 we note that 
\begin_inset Formula 
\[
\text{LHS}=n(D,0)=\frac{(D+0-1)!}{(D-1)!0!}=1=\frac{(D+0)!}{D!0!}=\text{RHS}.
\]

\end_inset

Now assume that the claim holds for 
\begin_inset Formula $M=k$
\end_inset

.
 Then for 
\begin_inset Formula $M=k+1,$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
N(D,k+1) & =\sum_{i=0}^{k}n(D,i)+n(D,k+1)\\
 & =\frac{(D+k)!}{D!k!}+\frac{(D+k+1-1)!}{(D-1)!(k+1)!}\\
 & =\frac{(D+k)!(D+k+1)}{D!(k+1)!}\\
 & =\frac{(D+k+1)!}{D!(k+1)!},
\end{align*}

\end_inset

proving the inducting step.
 
\end_layout

\begin_layout Enumerate
Now we show that 
\begin_inset Formula $N(D,M)$
\end_inset

 grows in polynomial fashion like 
\begin_inset Formula $D^{M}.$
\end_inset

 Assume 
\begin_inset Formula $D\ll M.$
\end_inset

 First, we write 
\begin_inset Formula 
\begin{align*}
N(D,M) & =\frac{(D+M)!}{D!M!}\\
 & \simeq\frac{(D+M)^{D+M}e^{-(D+M)}}{D!M^{M}e^{-M}}\tag{by Stirling's approximation}\\
 & =\frac{1}{D!M^{M}}\left(1+\frac{D}{M}\right)^{D+M}M^{D+M}\frac{e^{-(D+M)}}{e^{-M}}\\
 & =\frac{e^{-D}}{D!}\left(1+\frac{D}{M}\right)^{D+M}M^{D}.\tag{1}
\end{align*}

\end_inset

Now we take a more delicate look at the term 
\begin_inset Formula $(1+\frac{D}{M})^{D+M}.$
\end_inset

 Note that 
\begin_inset Formula 
\begin{align*}
\left(1+\frac{D}{M}\right)^{D+M} & =\left(1+\frac{D}{M}\right)^{M}\left(1+\frac{D}{M}\right)^{D}\\
 & =\biggl(\left(1+\frac{1}{M/D}\right)^{M/D}\biggl)^{D}\left(1+\frac{D}{M}\right)^{D}\\
 & \leq e^{D}2^{D},
\end{align*}

\end_inset

where the inequality comes from the fact that 
\begin_inset Formula $(1+1/x)^{x}$
\end_inset

 is an increasing function and 
\begin_inset Formula $D<M\Rightarrow D/M\leq1.$
\end_inset

 Substitution back into Eq (1), we get 
\begin_inset Formula 
\[
N(D,M)\leq\frac{e^{-D}}{D!}e^{D}2^{D}M^{D}=\frac{2^{D}}{D!}M^{D}.
\]

\end_inset

The case for 
\begin_inset Formula $M\ll D$
\end_inset

 follows by symmetry.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.17 [gamma density warmup]
\end_layout

\begin_layout Enumerate
Note 
\begin_inset Formula 
\begin{align*}
\Gamma(x+1) & =\int_{0}^{\infty}u^{x}e^{-u}du\\
 & =\left[-u^{x}e^{-u}\right]_{u=0}^{\infty}+\int_{0}^{\infty}xu^{x-1}e^{-u}du\\
 & =x\Gamma(x).
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
We note that 
\begin_inset Formula 
\[
\Gamma(1)=\int_{0}^{\infty}e^{-u}du=\left[e^{-u}\right]_{0}^{\infty}=1.
\]

\end_inset

And as a result, by recursion
\begin_inset Formula 
\[
\Gamma(x+1)=x\Gamma(x)=\cdots=x!\text{ for }x\in\mathbb{N}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 1.18 [volume of unit sphere in n-space]
\end_layout

\begin_layout Standard
To state the problem statement in a clearer manner, we solve this problem
 in several steps.
\end_layout

\begin_layout Enumerate
First we derive Eq (1.142) in the book.
 We first rewrite the LHS in the following way.
 Let 
\begin_inset Formula $x\in\mathbb{R}^{D}$
\end_inset

 be arbitrary, then 
\begin_inset Formula 
\begin{align*}
\int_{\mathbb{R}^{d}}e^{-\left\Vert x\right\Vert ^{2}}dx & =\int_{\mathbb{R}}\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}e^{-(x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2})}dx_{1}dx_{2}\cdots dx_{n}\\
 & =\prod_{i=1}^{D}\int_{\mathbb{R}}e^{-x_{i}^{2}}dx_{i}.
\end{align*}

\end_inset

Next, we evaluate this integral.
 In order to make the computation easier, we choose to let the integrand
 be 
\begin_inset Formula $e^{-\pi\left|x\right|^{2}}$
\end_inset

instead (it doesn't effect the final result, and one could always get the
 original integral by scaling).
 Note that using the same argument as above, we have 
\begin_inset Formula 
\[
\int_{\mathbb{R}^{D}}e^{-\pi\left\Vert x\right\Vert ^{2}}dx=\left(\int_{\mathbb{R}}e^{-\pi x^{2}}dx\right)^{D}.
\]

\end_inset

Next, we have 
\begin_inset Formula 
\begin{align*}
\left(\int_{\mathbb{R}}e^{-\pi x^{2}}dx\right)^{2} & =\left(\int_{\mathbb{R}}e^{-\pi x_{1}^{2}}dx_{1}\right)\left(\int_{\mathbb{R}}e^{-\pi x_{2}^{2}}dx_{2}\right)\\
 & =\int_{\mathbb{R}\times\mathbb{R}}e^{-\pi(x_{1}^{2}+x_{2}^{2})}d(x_{1}\times x_{2})\tag{by Fubini's theorem}\\
 & =\int_{\mathbb{R}}\int_{\mathbb{R}}e^{-\pi(x_{1}^{2}+x_{2}^{2})}dx_{1}dx_{2}\tag{by Fubini's theorem}\\
 & =\int_{[0,2\pi]}\int_{\mathbb{R}}e^{-\pi r^{2}}rdrd\theta\tag{switch to polar coordinates}\\
 & =\int_{[0,2\pi]}d\theta\int_{\mathbb{R}}e^{-\pi r^{2}}rdr\\
 & =2\pi\left[-\frac{1}{2\pi}e^{-\pi r^{2}}\right]_{0}^{\infty}\\
 & =1.
\end{align*}

\end_inset

Since 
\begin_inset Formula $\int_{\mathbb{R}}e^{\pi x^{2}}dx>0$
\end_inset

, it follows that 
\begin_inset Formula $\int_{\mathbb{R}^{D}}e^{-\pi\left\Vert x\right\Vert ^{2}}dx=1.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Consider the function 
\begin_inset Formula $f:\mathbb{R}^{D}\rightarrow\mathbb{R};x\mapsto e^{-\pi\left\Vert x\right\Vert ^{2}}.$
\end_inset

 We just showed in part-1 that 
\begin_inset Formula $f\in L^{1}(\mathbb{R}^{D}).$
\end_inset

 Therefore, using generalized spherical coordinate (e.g.
 Theorem 6.3.4 in 
\begin_inset CommandInset citation
LatexCommand cite
key "stein2005real"
literal "false"

\end_inset

), we have that 
\begin_inset Formula 
\begin{align*}
1=\int_{\mathbb{R}^{D}}f(x)dx & =\int_{S^{d-1}}\left(\int_{\mathbb{R^{+}}}f(r\gamma)r^{d-1}dr\right)d\sigma(r)\\
 & =\int_{S^{d-1}}\left(\int_{\mathbb{R}^{+}}e^{-\pi\left\Vert r\gamma\right\Vert ^{2}}r^{d-1}dr\right)d\sigma(r)\\
 & =\int_{S^{d-1}}\left(\int_{\mathbb{R}^{+}}e^{-\pi r^{2}}r^{d-1}dr\right)d\sigma(r)\\
 & =
\end{align*}

\end_inset


\end_layout

\begin_layout Remark
This problem could have been solved heuristically.
 But it loses rigor.
 What was showed was a rigorous mathematical way to treat this problem.
 
\end_layout

\end_body
\end_document

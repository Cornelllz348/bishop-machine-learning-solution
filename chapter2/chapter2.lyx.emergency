#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{"/config/config.tex"}
\end_preamble
\use_default_options true
\master ../lyxmain.lyx
\begin_modules
theorems-ams-chap-bytype
jason-extension
enumitem
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.12
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 2
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3cm
\secnumdepth -1
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Solutions for exercises to chapter 2
\end_layout

\begin_layout Section
Problem 2.1 - Bernoulli distribution's expectation, variance, normalization,
 entropy
\end_layout

\begin_layout Standard
In the discussion below, 
\begin_inset Formula $X$
\end_inset

 is a random variable following Bernoulli distribution.
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To check normalization, we note 
\begin_inset Formula 
\[
\sum_{x=0}^{1}f_{X}(x\vert\mu)=\mu+(1-\mu)=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the expectation, note that 
\begin_inset Formula 
\[
\E[X]=\sum_{x=0}^{1}xf_{X}(x\vert\mu)=1\cdot\mu+0\cdot(1-\mu)=\mu.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, we note that 
\begin_inset Formula 
\[
\text{Var}[X]=\E[X^{2}]-(\E X)^{2}=\mu-\mu^{2}=\mu(1-\mu).
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the entropy, we note that 
\begin_inset Formula 
\[
H(X)=-\sum_{x=0}^{1}f_{X}(x\vert\mu)f\log f_{X}(x|\mu)=-\mu\log\mu-(1-\mu)\log1-\mu.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 2.2 - Symmetric Bernoulli distribution's expectation, variance, normaliza
tion, entropy
\end_layout

\begin_layout Standard
In the discussion below, 
\begin_inset Formula $X$
\end_inset

 is a random variable following the distribution stipulated by Eq.(2.261).
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To show it's normalized, we note
\begin_inset Formula 
\[
\sum_{x\in\{-1,1\}}f_{X}(x|\mu)=\left(\frac{1-\mu}{2}\right)^{2/2}\left(\frac{1+\mu}{2}\right)^{0}+\left(\frac{1-\mu}{2}\right)^{0}\left(\frac{1+\mu}{2}\right)^{1}=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its expectation, we note 
\begin_inset Formula 
\[
\E[X]=\sum_{x\in\{-1,1\}}xf_{X}(x|\mu)=\left(\frac{1+\mu}{2}\right)-\left(\frac{1-\mu}{2}\right)=\mu.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its variance, we note 
\begin_inset Formula 
\[
\text{Var}[X]=\E[X^{2}]-(\E[X])^{2}=\left(\frac{1+\mu}{2}\right)+\left(\frac{1-\mu}{2}\right)-\mu^{2}=1-\mu^{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its entropy, we note 
\begin_inset Formula 
\[
H(X)=-\sum_{x\in\{-1,1\}}f_{X}(x|\mu)\log f_{X}(x|\mu)=-\left(\frac{1-\mu}{2}\right)\log\frac{1-\mu}{2}-\left(\frac{1+\mu}{2}\right)\log\frac{1+\mu}{2}.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 2.3 - Binomial distribution is normalized
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 First, we show Eq.(2.262) holds: note that 
\begin_inset Formula 
\begin{align*}
\binom{N}{m}+\binom{N}{m-1} & =\frac{N!}{m!(N-m)!}+\frac{N!}{(m-1)!(N-m+1)!}\\
 & =\frac{N!(N-m+1)}{m!(N-m+1)!}+\frac{mN!}{m!(N-m+1)!}\\
 & =\frac{(N+1)!}{m!((N+1)-m)!}\\
 & =\binom{N+1}{m}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To prove the binomial theorem, we induce on 
\begin_inset Formula $N.$
\end_inset

 For the base case 
\begin_inset Formula $N=1$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

, it is trivially true:
\begin_inset Formula 
\begin{align*}
(1+x)^{1} & =\binom{1}{0}x^{0}+\binom{1}{1}x^{1}=1+x,\\
(1+x)^{0} & =\binom{0}{0}x^{0}=1.
\end{align*}

\end_inset

Now suppose the claim holds for 
\begin_inset Formula $N=k.$
\end_inset

 Then for 
\begin_inset Formula $N=k+1$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
(1+x)^{k+1} & =(1+x)(1+x)^{k}=(1+x)\sum_{m=0}^{N}\binom{N}{m}x^{m}\\
 & =\sum_{m=0}^{M}\binom{N}{m}x^{m}+\sum_{m=0}^{N}\binom{N}{m}x^{m+1}\\
 & =\binom{N}{0}x^{0}+\sum_{m=1}^{M}\binom{N}{m}x^{m}+\sum_{m=1}^{N}\binom{N}{m-1}x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\binom{N+1}{0}x^{0}+\sum_{m=1}^{M}\left(\binom{N}{m}+\binom{N}{m-1}\right)x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\binom{N+1}{0}x^{0}+\sum_{m=1}^{M}\binom{N+1}{m}x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\sum_{m=0}^{N+1}\binom{N}{m}x^{m}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Now to show that the binomial distribution is normalized, we note that 
\begin_inset Formula 
\begin{align*}
\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{N-m} & =(1-\mu)^{N}\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{-m}\\
 & =(1-\mu)^{N}\sum_{m=0}^{N}\binom{N}{m}\left(\frac{\mu}{1-\mu}\right)^{m}\\
 & =(1-\mu)^{N}\left(1+\frac{\mu}{1-\mu}\right)^{N}.\tag{by binomial theorem}\\
 & =\left[(1-\mu)\left(1+\frac{\mu}{1-\mu}\right)\right]^{N}
\end{align*}

\end_inset

Since 
\begin_inset Formula 
\begin{align*}
(1-\mu)\left(1+\frac{\mu}{1-\mu}\right) & =1+\frac{\mu}{1-\mu}-\mu-\frac{\mu^{2}}{1-\mu}\\
 & =1+\frac{\mu-\mu+\mu^{2}-\mu^{2}}{1-\mu}\\
 & =1,
\end{align*}

\end_inset

it follows that 
\begin_inset Formula $\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{N-m}=1,$
\end_inset

 and thus the result follows.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Problem 2.4 - Binomial distribution's expectation and variance 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 Following the hint, we differentiate Eq.(2.264) w.r.t 
\begin_inset Formula $\mu$
\end_inset

 once:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\mu}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} = & n\cdot\sum_{n=1}^{N-1}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=1}^{N-1}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
 & -N(1-\mu)^{N-1}+N\mu^{N-1}\\
= & n\cdot\sum_{n=1}^{N}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=0}^{N-1}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
= & n\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
= & \frac{n}{\mu}\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}-\frac{N-n}{1-\mu}\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\\
= & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right).
\end{align*}

\end_inset

Since 
\begin_inset Formula $\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}=1,$
\end_inset

 it follows that 
\begin_inset Formula 
\begin{align*}
\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right]=0 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right][\mu(1-\mu)]=0\\
 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}[n(1-\mu)-(N-n)\mu]=0\\
 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)=0.\tag{1}
\end{align*}

\end_inset

Now we rearrange Eq.(1):
\begin_inset Formula 
\[
\sum_{n=0}^{N}n\cdot\binom{N}{n}\mu^{n}(1-\mu)^{N-n}=N\mu\left(\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right)=N\mu.
\]

\end_inset

The result follows by observing that 
\begin_inset Formula 
\[
\E[X]=\sum_{n=0}^{N}n\cdot\binom{N}{n}\mu^{n}(1-\mu)^{N-n}
\]

\end_inset


\end_layout

\begin_layout Enumerate
To facilitate notation, we let 
\begin_inset Formula $\varphi(\mu)=\sum_{n=1}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right].$
\end_inset

 Then following the hint, we differentiate twice Eq.(2.264) w.r.t.
 
\begin_inset Formula $\mu$
\end_inset

 and get 
\begin_inset Formula 
\begin{align*}
\frac{\partial^{2}}{\partial\mu^{2}}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\}  & =\frac{\partial\varphi(\mu)}{\partial\mu}\\
 & =\sum_{n=0}^{N}\underbrace{\frac{\partial}{\partial\mu}\left\{ \binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)\right\} }_{:=H(\mu).}.
\end{align*}

\end_inset

Hence, it suffices to evaluate 
\begin_inset Formula $H(\mu)$
\end_inset


\begin_inset Formula 
\begin{align*}
H(\mu) & =\frac{\partial}{\partial\mu}\left\{ \binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} \left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)+\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\frac{\partial}{\partial\mu}\left\{ \frac{n}{\mu}-\frac{N-n}{1-\mu}\right\} \\
 & =\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}+\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]\\
 & =\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right].
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\mu^{2}}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} =\underbrace{\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]=0}_{(2)}.
\]

\end_inset

Now, we arrange Eq.(2) and get 
\begin_inset Formula 
\begin{align*}
 & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right](\mu^{2}(1-\mu)^{2})=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(n(1-\mu)-(N-n)\mu)^{2}-(N-n)\mu^{2}-n(1-\mu)^{2}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(n-N\mu)^{2}-(N-n)\mu^{2}-n(1-\mu)^{2}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(N-n)\mu^{2}+n(1-\mu)^{2}\right]\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(N\mu^{2}+n-2n\mu)\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=N\mu-N\mu^{2}=N\mu(1-\mu).
\end{align*}

\end_inset

The conclusion can be drawn by observing that
\begin_inset Formula 
\[
\text{Var}[X]=\E[(X-\E[X])^{2}]=\sum_{n=0}^{N}\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}.
\]

\end_inset


\end_layout

\begin_layout Section
Problem 2.5 - Beta distribution is normalized 
\end_layout

\begin_layout Standard
First, we note that 
\begin_inset Formula 
\begin{align*}
\Gamma(a)\Gamma(b) & =\int_{0}^{\infty}e^{-x}x^{a-1}dx\int_{0}^{\infty}e^{-y}y^{b-1}dy\\
 & =\int_{0}^{\infty}\int_{0}^{\infty}e^{-(x+y)}x^{a-1}y^{b-1}dydx.\tag{1}
\end{align*}

\end_inset

Now, we make a change of variable 
\begin_inset Formula 
\[
x+y=t\implies\begin{cases}
y=t-x\\
y\geq0\Leftrightarrow t-x\geq0\Leftrightarrow t\geq x\\
x\geq0\\
dt=dy
\end{cases}.
\]

\end_inset

Therefore, it follows that 
\begin_inset Formula 
\begin{align*}
\text{Eq.(1)} & =\int_{0}^{\infty}\int_{x}^{\infty}e^{-t}x^{a-1}(t-x)^{b-1}dtdx\\
 & =\int_{0}^{\infty}\int_{0}^{t}e^{-t}x^{a-1}(t-x)^{b-1}dxdt\tag{by Fubini's theorem}\\
 & =\int_{0}^{\infty}\int_{0}^{1}e^{-t}(t\mu)^{a-1}(t-t\mu)^{b-1}td\mu dt\tag{2}\\
 & =\int_{0}^{\infty}e^{-t}t^{a-1}t^{b-1}tdt\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu\\
 & =\Gamma(a+b)\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu,
\end{align*}

\end_inset

where 
\begin_inset Formula $\text{Eq.(2)}$
\end_inset

 follows from a change of variables 
\begin_inset Formula 
\[
x=t\mu\implies\begin{cases}
0\leq x\leq t\Leftrightarrow0\leq t\mu\leq t\Leftrightarrow0\leq\mu\leq t\\
dx=td\mu
\end{cases}.
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)},
\]

\end_inset

and as a result the Beta density integrates to 1.
 
\end_layout

\begin_layout Section
Problem 2.6 - Beta distribution's expectation, variance, mode 
\end_layout

\begin_layout Standard

\shape italic
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a random variable that follows Beta distribution with parameter 
\begin_inset Formula $a,b\in\mathbb{R}^{+}.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To find the expectation, note that 
\begin_inset Formula 
\begin{align*}
\E[X] & =\int_{0}^{1}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}xdx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}x^{(a+1)-1}(1-x)^{b-1}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}\tag{by Problem 2.5}\\
 & =\frac{\Gamma(a+b)a\Gamma(a)\Gamma(b)}{\Gamma(a)\Gamma(b)\Gamma(a+b)\Gamma(a+b)}\tag{since \ensuremath{\Gamma(x+1)=x\Gamma(x).}}\\
 & =\frac{a}{a+b}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, we first note 
\begin_inset Formula 
\begin{align*}
\E[X^{2}] & =\int\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}x^{2}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}x^{a+2-1}(1-x)^{b-1}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)}\\
 & =\frac{a(a+1)}{(a+b+1)(a+b)}.
\end{align*}

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\text{Var}[X] & =\E[X^{2}]-(\E[X])^{2}=\frac{a(a+1)}{(a+b+1)(a+b)}-\left(\frac{a}{a+b}\right)^{2}\\
 & =\frac{a(a+1)(a+b)-a^{2}(a+b+1)}{(a+b+1)(a+b)^{2}}\\
 & =\frac{ab}{(a+b+1)(a+b)^{2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Since the mode of a continuous probability distribution is defined as its
 density function's critical point, it suffices for us to differentiate
 
\begin_inset Formula $f_{X}(x)$
\end_inset

 and find the critical points.
 Note that 
\begin_inset Formula 
\begin{align*}
\frac{\partial f_{X}(x)}{\partial x} & =\frac{\partial}{\partial x}\left[\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}\right]\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right].
\end{align*}

\end_inset

Setting it to zero yields 
\begin_inset Formula 
\begin{align*}
 & \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right]=0\\
\iff & \left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right]=0\\
\iff & (a-1)(1-x)=(b-1)x\\
\iff & x=\frac{a-1}{a+b-2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 2.7 - Comparison between posterior mean and MLE for Bernoulli model
 
\end_layout

\begin_layout Standard

\shape italic
The book didn't go through the details of deriving some of the calculations.
 Although these calculations are simple, they are worth doing by hand at
 least once.
 Hence, we show them here.
 For notation, we let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 denote the sample data, 
\begin_inset Formula $(x_{1},\ldots x_{N})$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
First, we find the posterior mean for the Bernoulli model.
 By assumption, the parameter of interest, 
\begin_inset Formula $\mu,$
\end_inset

 follows beta distribution, i.e.
 
\begin_inset Formula 
\[
f(\mu|a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}.
\]

\end_inset

And the likelihood function after sampling the data is given by 
\begin_inset Formula 
\[
f(\mathcal{X}\vert\mu)=\mu^{\sum_{i=1}^{N}x_{i}}(1-\mu)^{\sum_{i=1}^{N}(1-x_{i})}=\mu^{n}(1-\mu)^{m}.
\]

\end_inset

Therefore, we have the posterior as 
\begin_inset Formula 
\begin{align*}
f(\mu|\vert\mathcal{X}) & \propto f(\mu\vert a,b)\cdot f(x_{1},\ldots,x_{N}\vert\mu)\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mu^{n}(1-\mu)^{m}\\
 & \propto\mu^{a+n-1}(1-\mu)^{b+m-1}.
\end{align*}

\end_inset

Since 
\begin_inset Formula $f(\mu|\mathcal{X})$
\end_inset

 should integrates to 1 in order to be a valid probability density function,
 in view of Problem 2.5 we see that 
\begin_inset Formula 
\[
f(\mu\vert\mathcal{X})=\frac{\Gamma(a+b+n+m)}{\Gamma(a)\Gamma(b)}\mu^{a+n-1}(1-\mu)^{b+m-1}\sim\text{Beta}(a+n,b+m).
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\E_{\mu|\mathcal{X}}[\mu]=\frac{a+n}{a+b+n+m}
\]

\end_inset

as desired.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Next, we find 
\begin_inset Formula $\mu_{MLE}.$
\end_inset

 First, we write out the likelihood equation, 
\begin_inset Formula 
\[
f(\mathcal{X}|\mu)=\prod_{i=1}^{N}\mu^{x_{i}}(1-\mu)^{1-x_{i}}=\mu^{\sum_{i=1}^{N}x_{i}}(1-\mu)^{\sum_{i=1}^{N}(1-x_{i})},
\]

\end_inset

from which we can get the log-likelihood equation as 
\begin_inset Formula 
\[
\ell(\mu)=\log f(\mathcal{X}\vert\mu)=\left(\sum_{i=1}^{N}x_{i}\right)\log\mu+\left(\sum_{i=1}^{N}(1-x_{i})\right)\log(1-\mu).
\]

\end_inset

Now we differentiate and set to zero 
\begin_inset Formula 
\begin{align*}
 & \frac{\partial\ell(\mu)}{\partial\mu}=\left(\sum_{i=1}^{N}x_{i}\right)\frac{1}{\mu}-\left(\sum_{i=1}^{N}(1-x_{i})\right)\frac{1}{1-\mu}=0\\
\iff & \left(\sum_{i=1}^{N}x_{i}\right)(1-\mu)-\left(\sum_{i=1}^{N}(1-x_{i})\right)\mu=0\\
\iff & \frac{1}{\mu}=\frac{\sum_{i=1}^{N}(1-x_{i})}{\sum_{i=1}^{N}x_{i}}+1=\frac{\sum_{i=1}^{N}1-\sum_{i=1}^{N}x_{i}+\sum_{i=1}^{N}x_{i}}{\sum_{i=1}^{N}x_{i}}\\
\iff & \mu_{MLE}=\frac{n}{n+m}.
\end{align*}

\end_inset

Now it suffices to show that 
\begin_inset Formula 
\[
\frac{a+n}{a+b+n+m}\in\text{Seg}\left(\frac{a}{a+b},\frac{n}{n+m}\right),
\]

\end_inset

where Seg means the line segment whose endpoints are 
\begin_inset Formula $a/(a+b)$
\end_inset

 and 
\begin_inset Formula $n/(n+m)$
\end_inset

.
 To show this, it suffices to show that the solution, denoted as 
\begin_inset Formula $\lambda_{*}$
\end_inset

, to the equation 
\begin_inset Formula 
\[
\lambda\left(\frac{a}{a+b}\right)+(1-\lambda)\frac{n}{n+m}=\frac{a+n}{a+b+n+m}
\]

\end_inset

lies in 
\begin_inset Formula $(0,1).$
\end_inset

 Solving the equation yields 
\begin_inset Formula 
\[
\lambda_{*}=\frac{a+b}{a+b+m+n}.
\]

\end_inset

Then the claim is true since 
\begin_inset Formula $a,b,n,m>0$
\end_inset

 by assumption.
 
\end_layout

\begin_layout Section
Problem 2.9 - Dirichlet distribution is normalized
\end_layout

\begin_layout Standard

\shape italic
In the discussion below, we let 
\begin_inset Formula $f_{D}(\mu)$
\end_inset

 denote the density function for a Dirichlet distribution whose parameter
 
\begin_inset Formula $\mu$
\end_inset

 is in 
\begin_inset Formula $K$
\end_inset

 dimensional Euclidean space.
 We will use a slightly different approach from the one derived from the
 hint from the book.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
We need to show that 
\begin_inset Formula 
\[
\int_{\mathbb{S}_{K}}f_{D}(\mu)d\mu=\int_{\mathbb{S}_{K}}\frac{\Gamma(\sum_{i=1}^{K}\alpha_{i})}{\prod_{i=1}^{K}\Gamma(\alpha_{i})}\prod_{i=1}^{K-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{K-1}\mu_{i}\right)^{\alpha_{K}-1}d\mu=1,
\]

\end_inset

where 
\begin_inset Formula $\mathbb{S}_{k}:=\{x\in\mathbb{R}^{k}:\sum_{i=1}^{k}x_{k}=1,x_{i}\geq0,i=0,...,k\}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-simplex in Euclidean space.
 Following the idea in Problem 2.5, it suffices for us to show that 
\begin_inset Formula 
\[
I_{\mu}(k):=\int_{\mathbb{S}_{k}}\prod_{i=1}^{k-1}\mu_{i}^{\alpha_{i}}\left(1-\sum_{i=1}^{k-1}\mu_{i}\right)^{\alpha_{k}-1}d\mu=\frac{\prod_{i=1}^{k}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{k}\alpha_{i})},
\]

\end_inset

for any 
\begin_inset Formula $\mathbb{N}\ni k\geq2.$
\end_inset

 We prove this using inducting on 
\begin_inset Formula $k$
\end_inset

.
 For the base case 
\begin_inset Formula $k=2$
\end_inset

, note 
\begin_inset Formula 
\begin{align*}
I_{\mu}(2) & =\int_{\{\mu\in\mathbb{R}^{2}:\mu_{1}+\mu_{2}=1,\mu_{1}\geq0,\mu\geq0\}}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu\\
 & =\int_{\{\mu\in\mathbb{R}^{2}:\mu_{1}\times\mu_{2}\in[0.1]\times[0,1]\}}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu\tag{1}\\
 & =\int_{0}^{1}d\mu_{2}\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu_{1}\tag{by Fubini's theorem }\\
 & =\frac{\Gamma(a_{1})\Gamma(\alpha_{2})}{\Gamma(\alpha_{1}+\alpha_{2})}.
\end{align*}

\end_inset

where Eq.(1) follows from the observation that for any 
\begin_inset Formula $\mathbb{N}\ni k\geq2$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\mathbb{S}_{k} & =\left\{ x\in\mathbb{R}^{k}:\sum_{i=1}^{k-1}x_{i}=1-x_{k},x_{k}\in[0,1],x_{i}\geq0,i=1,...,k\right\} \\
 & =\left\{ x\in\mathbb{R}^{k}:\sum_{i=1}^{k-1}x_{i}\leq1,x_{k}\in[0,1],x_{i}\geq0,i=1,...,k-1\right\} ,
\end{align*}

\end_inset

where the equality can be verified by an element trace.
 Now assume the claim is true for 
\begin_inset Formula $k=n.$
\end_inset

 Before going into the inductive step, we carefully formulate the inductive
 hypothesis: note that 
\begin_inset Formula 
\begin{align*}
I_{\mu}(n) & =\int_{\mathbb{S}_{n}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\int_{\left\{ \mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{n}\in[0,1],\mu_{1\leq i\leq n-1}\geq0\right\} }\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\int_{0}^{1}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d\mu_{n}d(\times_{i=1}^{n-1}\mu_{i})\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\int_{0}^{1}d\mu_{n}\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
 & =\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{i}}\mu_{2}^{\alpha_{2}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu_{n-1}d\mu_{n-2}\cdots d\mu_{1}\tag{2}\\
 & =\frac{\prod_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma\left(\sum_{i=1}^{n}\alpha_{i}\right)}
\end{align*}

\end_inset

for any 
\begin_inset Formula $\{\alpha_{1},\ldots,\alpha_{n}\}$
\end_inset

 s.t.
 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}=1.$
\end_inset

 Also note that Eq.(2) follows from repeated application of Fubini's theorem
 in the following way: 
\begin_inset Formula 
\begin{align*}
 & \text{Eq.(2)}\\
= & \int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=2}^{n-1}\mu_{i}\leq1-\mu_{1},\mu_{1}\in[0,1],\mu_{2\leq i\leq n-1}\geq0\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{\{(\mu_{2},\dots,\mu_{n})\in\mathbb{R}^{n-1}:\sum_{i=2}^{n-1}\mu_{i}\leq1-\mu_{1},\mu_{2\leq n-1}\geq0\}}\prod_{i=2}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{\big\{\stackrel{(\mu_{2},\dots,\mu_{n})\in\mathbb{R}^{n-1}:\sum_{i=3}^{n-1}\mu_{i}\leq1-\mu_{1}-\mu_{2}}{\mu_{3\leq i\leq n-1}\geq0,\mu_{2}\in[0,1-\mu_{1}]}\big\}}\prod_{i=2}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=2}^{n-1}\mu_{i})d\mu_{1}\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{1}}\mu_{2}^{\alpha_{2}-1}\int_{\big\{\stackrel{(\mu_{3},\dots,\mu_{n})\in\mathbb{R}^{n-2}:\sum_{i=3}^{n-1}\mu_{i}\leq1-\mu_{1}-\mu_{2},}{\mu_{3\leq i\leq n-1}\geq0}\big\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=3}^{n-1}\mu_{i})d\mu_{2}d\mu_{1}\\
\cdots\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{i}}\mu_{2}^{\alpha_{2}-2}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu_{n-1}d\mu_{n-2}\cdots d\mu_{1}\tag{2}
\end{align*}

\end_inset

We also prove a lemma to facilitate the inductive step.
 
\end_layout

\begin_layout Lemma
For any 
\begin_inset Formula $a\in\mathbb{R}-\{0\}$
\end_inset

 and 
\begin_inset Formula $m,n>0,$
\end_inset

 the following integral identity holds: 
\begin_inset CommandInset label
LatexCommand label
name "lem: problem2.9 lem1"

\end_inset


\begin_inset Formula 
\[
\int_{0}^{1}x^{m-1}(1-x)^{n-1}dx=\frac{1}{a^{m+n-1}}\int_{0}^{a}y^{m-1}(a-y)^{n-1}dy,
\]

\end_inset

and as a result 
\begin_inset Formula 
\[
\int_{0}^{a}y^{m-1}(a-y)^{n-1}=a^{m+n-1}\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}
\]

\end_inset


\end_layout

\begin_layout Proof
By change of variable 
\begin_inset Formula $x=y/a,$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\int_{0}^{1}x^{m-1}(1-x)^{n-1}dx & =\frac{1}{a}\int_{0}^{a}\left(\frac{y}{a}\right)^{m-1}\left(1-\frac{y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}a^{m+n-2}\left(\frac{y}{a}\right)^{m-1}\left(1-\frac{y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}a^{m-1}\left(\frac{y}{a}\right)^{m-1}a^{n-1}\left(\frac{a-y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}y^{m-1}(a-y)^{n-1}dy.
\end{align*}

\end_inset

That 
\begin_inset Formula $\int_{0}^{a}y^{m-1}(a-y)^{n-1}=\frac{1}{a^{m+n-1}}\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$
\end_inset

 then directly follows from Problem 2.5.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Then for 
\begin_inset Formula $k=n+1,$
\end_inset

 again by repeated application of Fubini's theorem we have 
\begin_inset Formula 
\begin{align*}
I_{\mu}(n+1) & =\int_{\mathbb{S}_{n+1}}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu\\
 & =\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{1}}\mu_{2}^{\alpha_{2}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu_{n}d\mu_{n-1}\cdots d\mu_{1}.\tag{3}
\end{align*}

\end_inset

Note that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: problem2.9 lem1"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset Formula 
\begin{align*}
 & \int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu_{n}\\
=\ \  & \int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}-\mu_{n}\right)^{\alpha_{n+1}-1}d\mu_{n}\\
=\ \  & \frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+\alpha_{n+1}-1}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula 
\begin{align*}
\text{Eq.(3)} & =\frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+\alpha_{n+1}-1}d\mu_{n-1}\cdots d\mu_{1}\\
 & =\frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\frac{\Gamma(\alpha_{1})\Gamma(\alpha_{2})\cdots\Gamma(\alpha_{n}+\alpha_{n+1})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n+1})}\tag{by inductive hypothesis}\\
 & =\frac{\prod_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma\left(\sum_{i=1}^{n}\alpha_{i}\right)}
\end{align*}

\end_inset

as desired.
\end_layout

\begin_layout Section
Problem 2.10 - Dirichlet distribution's expectation, variance and covariance
\end_layout

\begin_layout Standard

\shape italic
In the discussion below, we let 
\begin_inset Formula $\mu$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-dimensional random vector s.t 
\begin_inset Formula $\mu\sim\mathrm{Dir}(\alpha)$
\end_inset

 and 
\begin_inset Formula $\mathbb{S}_{n}$
\end_inset

 denote the standard simplex in 
\begin_inset Formula $\mathbb{R}^{n}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To find the expectation, note that 
\begin_inset Formula 
\begin{align*}
\E[\mu_{j}] & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\mu_{j}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+1-1}d\mu & \text{if }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i\geq1,i\neq j}^{n-1}\mu_{j}^{\alpha_{i}-1}\mu_{j}^{\alpha_{j}+1-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }j\in\{1,...,n-1\}
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}\frac{\prod_{i\geq1,i\neq j}^{n-1}\Gamma(\alpha_{i})\Gamma(\alpha_{j}+1)}{\Gamma\left((\sum_{i=1}^{n}\alpha_{i})+1\right)}\\
 & =\frac{\alpha_{j}}{\sum_{i=1}^{n}\alpha_{i}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, note that using the same argument as in part(1), 
\begin_inset Formula 
\begin{align*}
\E[\mu_{j}^{2}] & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+2-1}d\mu & \text{if }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i\geq1,i\neq j}^{n-1}\mu_{j}^{\alpha_{i}-1}\mu_{j}^{\alpha_{j}+2-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }j\in\{1,...,n-1\}
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}\frac{\prod_{i\geq1,i\neq j}^{n-1}\Gamma(\alpha_{i})\Gamma(\alpha_{j}+2)}{\Gamma\left((\sum_{i=1}^{n}\alpha_{i})+2\right)}\\
 & =\frac{\alpha_{j}(\alpha_{j}+1)}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}.
\end{align*}

\end_inset

Hence, 
\begin_inset Formula 
\begin{align*}
\text{Var}[\mu_{j}] & =\E[\mu_{j}^{2}]-(\E[\mu_{j}])^{2}\\
 & =\frac{\alpha_{j}(\alpha_{j}+1)}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}-\frac{\alpha_{j}^{2}}{(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{(\sum_{i=1}^{n}\alpha_{i})\alpha_{j}(\alpha_{j}+1)-(\sum_{i=1}^{n}\alpha_{i}+1)\alpha_{j}^{2}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{\alpha_{j}(\sum_{i=1}^{n}\alpha_{i}-\alpha_{j})}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the covariance, note that 
\begin_inset Formula 
\begin{align*}
\E[\mu_{i}\mu_{j}] & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{k\geq1,k\notin\{i,j\}}^{n-1}\mu_{k}^{\alpha_{i}-1}\mu_{i\in\{i,j\}}^{\alpha_{i\in\{i,j\}}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+1-1}d\mu & \text{if }i=n\text{ or }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{k\geq1,k\neq i,j}^{n-1}\mu_{k}^{\alpha_{i}-1}\mu_{i}^{\alpha_{i}+1-1}\mu_{j}^{\alpha_{j}+1-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }i\neq n\text{ and }j\neq n
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\frac{\prod_{k\geq1,k\neq i,j}^{n}\Gamma(\alpha_{k})\Gamma(\alpha_{i}+1)\Gamma(\alpha_{j}+1)}{\Gamma(\sum_{i=k}^{n}\alpha_{k}+2)}\\
 & =\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}.
\end{align*}

\end_inset

Therefore, it follows that 
\begin_inset Formula 
\begin{align*}
\text{Cov}[\mu_{i},\mu_{j}] & =\E[\mu_{i}\mu_{j}]-\E[\mu_{i}]\E[\mu_{j}]\\
 & =\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}-\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{\alpha_{i}\alpha_{j}(\sum_{i=1}^{n}\alpha_{i})-(\sum_{i=1}^{n}\alpha_{i}+1)\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =-\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 2.11 - Expression for 
\begin_inset Formula $\mathbb{E}[\log\mathrm{Dir}(\alpha)]$
\end_inset


\end_layout

\begin_layout Standard

\shape italic
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-dimensional random vector such that 
\begin_inset Formula $X\sim\mathrm{Dir}(\alpha).$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
Note that for 
\begin_inset Formula $(\mu_{1},\ldots,\mu_{n})$
\end_inset

 in the 
\begin_inset Formula $n$
\end_inset

-dimensional standard simplex, we have 
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\alpha_{j}}\left[\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\right] & =\ln\mu_{j}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}.
\end{align*}

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\E[\ln\mu_{j}] & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\ln\mu_{j}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}d\mu\\
 & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\left[\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\right]d\mu\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\int\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}d\mu\tag{by Leibniz rule}\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\left[\frac{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\right]\tag{(1)}.
\end{align*}

\end_inset

Now we simplify Eq.(1), 
\begin_inset Formula 
\begin{align*}
\text{Eq.(1)} & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\prod_{i=1}^{n}\Gamma(\alpha_{i})}\left[\frac{\prod_{i\geq1,i\neq j}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})^{2}}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial\alpha_{j}}\right]\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\prod_{i=1}^{n}\Gamma(\alpha_{i})}\Biggr[\frac{\prod_{i\geq1,i\neq j}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})^{2}}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial(\sum_{i=1}^{n}\alpha_{i})}\underbrace{\frac{\partial(\sum_{i=1}^{n}\alpha_{i})}{\partial\alpha_{j}}}_{=1}\Biggr]\\
 & =\frac{1}{\Gamma(\alpha_{j})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial(\sum_{i=1}^{n}\alpha_{i})}\\
 & =\frac{\partial}{\partial\alpha_{j}}\ln\Gamma(\alpha_{j})-\frac{\partial}{\partial(\sum_{i=1}^{n}\alpha_{i})}\ln\Gamma\bigg(\sum_{i=1}^{n}\alpha_{i}\bigg).
\end{align*}

\end_inset

Hence, 
\begin_inset Formula $\E[\ln\mu_{j}]=\psi(\alpha_{j})-\psi(\sum_{i=1}^{n}\alpha_{i})$
\end_inset

 as desired.
 
\end_layout

\begin_layout Section
Problem 2.12 - Uniform distribution's normalization, expectation, variance
 
\end_layout

\begin_layout Standard

\shape italic
In the discussion below, the 
\begin_inset Formula $X$
\end_inset

 be a random variable such that 
\begin_inset Formula $X\sim\text{Uniform}(a,b)$
\end_inset

 with 
\begin_inset Formula $f_{X}(x)=\frac{1}{b-a}\mathbbm{1}_{[a,b]}.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To see the normalization, note that 
\begin_inset Formula 
\[
\int\frac{1}{b-a}\mathbbm{1}_{[a,b]}dx=\frac{1}{b-a}(b-a)=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the expectation, note 
\begin_inset Formula 
\[
\E[X]=\int_{a}^{b}x\frac{1}{b-a}dx=\left[\frac{x^{2}}{2}\right]_{a}^{b}(b-a)=\frac{b^{2}-a^{2}}{2}\frac{1}{(b-a)}=\frac{a+b}{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, first we note that 
\begin_inset Formula 
\begin{align*}
\E[X^{2}] & =\int_{a}^{b}x^{2}\frac{1}{b-a}dx=\left[\frac{x^{3}}{3}\right]_{a}^{b}(b-a)=\frac{b^{3}-a^{3}}{3}\frac{1}{(b-a)}\\
 & =\frac{(b-a)(a^{2}+b^{2}+ab)}{3(b-a)}=\frac{a^{2}+b^{2}+ab}{3}.
\end{align*}

\end_inset

 And thus 
\begin_inset Formula 
\begin{align*}
\text{Var}[X] & =\E[X^{2}]-(\E[X])^{2}=\frac{a^{2}+b^{2}+ab}{3}-\left(\frac{a+b}{2}\right)^{2}\\
 & =\frac{4a^{2}+4b^{2}+4ab-3a^{2}-3b^{2}-6ab}{12}\\
 & =\frac{a^{2}+b^{2}-2ab}{12}\\
 & =\frac{(a-b)^{2}}{12}
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Section
Problem 2.14 - Multidimensional gaussian maximizes entropy 
\end_layout

\begin_layout Standard
First, we write out the Lagrangian 
\begin_inset Formula 
\begin{align*}
\mathcal{L}(p(x))= & -\int p(x)\ln p(x)dx+\left\langle \begin{bmatrix}\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}
\end{bmatrix},\begin{bmatrix}\int p(x)dx-1\\
\int p(x)xdx-\mu\\
\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma
\end{bmatrix}\right\rangle _{\text{prod}}\\
= & \int-p(x)\ln p(x)dx\\
 & +\left\langle \lambda_{1},\int p(x)dx-1\right\rangle +\left\langle \lambda_{2},\int p(x)xdx-\mu\right\rangle +\left\langle \lambda_{3},\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right\rangle \\
= & \int-p(x)\ln p(x)dx\\
 & +\lambda_{1}\left(\int p(x)dx-1\right)+\lambda_{2}^{T}\left(\int p(x)xdx-\mu\right)+\text{tr}\left(\lambda_{3}^{T}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)\\
= & \int-p(x)\ln p(x)dx\\
 & +\lambda_{1}\left(\int p(x)dx-1\right)+\lambda_{2}^{T}\left(\int p(x)xdx-\mu\right)+\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)\tag{1}\\
= & \int-p(x)\ln p(x)+\lambda_{1}p(x)+\lambda_{2}^{T}p(x)x+\text{tr}((x-\mu)^{T}\lambda_{3}p(x)(x-\mu))dx-(\lambda_{1}+\lambda_{2}^{T}\mu+\lambda_{3}\Sigma)\\
:= & \int F(p(x)dx+C,\tag{by relabeling}
\end{align*}

\end_inset

where Eq.(1) follow can be justified as follows.
 Note that 
\begin_inset Formula 
\begin{align*}
 & \int p(x)(x-\mu)(x-\mu)^{T}dx\\
=\ \  & \int\begin{bmatrix}p(x)(x_{1}-\mu)(x_{1}-\mu) & p(x)(x_{1}-\mu)(x_{2}-\mu) & \cdots & p(x)(x_{1}-\mu)(x_{n}-\mu)\\
p(x)(x_{2}-\mu)(x_{1}-\mu) & \ddots & \cdots & p(x)(x_{2}-\mu)(x_{n}-\mu)\\
\vdots & \vdots & \cdots & \vdots\\
p(x)(x_{n}-\mu)(x_{1}-\mu) & p(x)(x_{n}-\mu)(x_{2}-\mu) & \cdots & p(x)(x_{n}-\mu)(x_{n}-\mu)
\end{bmatrix}dx\\
=\ \  & \begin{bmatrix}\int p(x)(x_{1}-\mu)(x_{1}-\mu)dx & \int p(x)(x_{1}-\mu)(x_{2}-\mu)dx & \cdots & \int p(x)(x_{1}-\mu)(x_{n}-\mu)dx\\
\int p(x)(x_{2}-\mu)(x_{1}-\mu)dx & \ddots & \cdots & \int p(x)(x_{2}-\mu)(x_{n}-\mu)dx\\
\vdots & \vdots & \cdots & \vdots\\
\int p(x)(x_{n}-\mu)(x_{1}-\mu)dx & \int p(x)(x_{n}-\mu)(x_{2}-\mu)dx & \cdots & \int p(x)(x_{n}-\mu)(x_{n}-\mu)dx
\end{bmatrix},
\end{align*}

\end_inset

which is symmetric, whence by cyclic property of trace it follows that 
\begin_inset Formula 
\begin{align*}
\text{tr}\left(\lambda_{3}^{T}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right) & =\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)^{T}\right)\\
 & =\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)
\end{align*}

\end_inset

To maximize, we take the functional derivative and set it to zero: 
\begin_inset Formula 
\begin{align*}
 & \frac{\delta L(p(x)}{\delta p(x)}=\frac{\partial F(p(x))}{\partial p(x)}=-\ln p(x)-1+\lambda_{1}+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)=0\\
\implies & p(x)=\exp\{\lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\}.
\end{align*}

\end_inset

Now we substitute 
\begin_inset Formula $p(x)$
\end_inset

 into the constraints:
\begin_inset Formula 
\begin{align*}
\int p(x)xdx= & \int\exp\{\lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\}xdx\\
= & \int\exp\left\{ \left(x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)^{T}\lambda_{3}\left(x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} dx\\
= & \int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} \left(y+\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy\tag{2}\\
= & \int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy\tag{3}\\
 & +\int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} \left(\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy\tag{4}\\
= & \mu,
\end{align*}

\end_inset

where Eq.(2) follows from change of variable 
\begin_inset Formula $y=x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}.$
\end_inset

 We take a closer look at Eq.(2).
 A couple of claims are in order.
 
\end_layout

\begin_layout Lemma
The following identity holds:
\begin_inset Formula 
\[
\int_{\mathbb{R}^{n}}\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy=0,
\]

\end_inset

where 
\begin_inset Formula $y\in\mathbb{R}^{n}.$
\end_inset

 
\end_layout

\begin_layout Proof
The key to proving it is that the integrand, denoted as 
\begin_inset Formula $\varphi(y),$
\end_inset

 is a "odd" function in multidimensional space: 
\begin_inset Formula 
\begin{align*}
\varphi(-y) & =\exp\left\{ (-y^{T})\lambda_{3}(-y)-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} (-y)\\
 & =-\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} y\\
 & =-\varphi(y).
\end{align*}

\end_inset

Then note that 
\begin_inset Formula 
\[
\mathbb{R}^{n}=\underbrace{\left(\bigcup_{(\#_{1},\#_{2},\cdots,\#_{n})\in\prod_{i=1}^{n}\{+,-\}}\prod_{i=1}^{n}\mathbb{R}^{\#_{i}}\right)}_{:=P}\bigcup\underbrace{\left(\bigcup_{(\#_{1},\#_{2},\cdots,\#_{n})\in\prod_{i=1}^{n}\{0,1\},\exists\#_{i}=0\ \text{for some}\ i}\prod_{i=1}^{n}\mathbb{R}^{\#_{i}}\right)}_{:=N},
\]

\end_inset

where 
\begin_inset Formula $\mathbb{R}^{+}:=\{x\in\mathbb{R}|x>0\}$
\end_inset

, 
\begin_inset Formula $\mathbb{R}^{-}:=\{x\in\mathbb{R}|x<0\}$
\end_inset

, 
\begin_inset Formula $\mathbb{R}^{0}:=\{0\},$
\end_inset

and 
\begin_inset Formula $\mathbb{R}^{1}:=\mathbb{R}$
\end_inset

.
 Now we can rewrite the integral of interest as 
\begin_inset Formula 
\[
\int_{P\cup N}\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy=\int_{N}\varphi(y)dy+\int_{P}\varphi(y)dy.
\]

\end_inset

Since 
\begin_inset Formula $m(N)=0,$
\end_inset

 (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
after " Lemma 3.5"
key "stein2005real"
literal "false"

\end_inset

), 
\begin_inset Formula $\int_{N}\varphi(y)dy=0.$
\end_inset

 On the other hand, since 
\begin_inset Formula $P$
\end_inset

 can be written as 
\begin_inset Formula $2^{n}$
\end_inset

 disjoint unions by definition (note that for 
\begin_inset Formula $(\#_{1},\ldots,\#_{n})\neq(\tilde{\#}_{1},\ldots,\tilde{\#}_{2}),$
\end_inset

 we have (
\begin_inset Formula $\prod_{i=1}^{n}\mathbb{R}^{\#_{i}})\cap(\prod_{i=1}^{n}\mathbb{R}^{\tilde{\#}_{i}})=\emptyset$
\end_inset

), we have that 
\begin_inset Formula 
\[
\int_{P}\varphi(y)dy=\int_{\sqcup_{i=1}^{2^{n}}P_{i}}\varphi(y)dy=\sum_{i=1}^{2^{n}}\int_{P_{i}}\varphi(y)dy.
\]

\end_inset

Since for any 
\begin_inset Formula $i\in\{1,...,2^{n}\},$
\end_inset

 there exists some 
\begin_inset Formula $j\neq i\in\{1,...,2^{n}\}$
\end_inset

 such that 
\begin_inset Formula $P_{i}=(-1)\cdot P_{j}$
\end_inset

, we can rewrite the last term in the previous term as the sum over pairs
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{2^{n}}\int_{P_{i}}\varphi(y)dy & =\sum_{(i,j)}\left(\int_{P_{i}}\varphi(y)dy+\int_{P_{j}}\varphi(y)dy\right)=\sum_{(i,j)}\left(\int_{P_{i}}-\varphi(-y)dy+\int_{P_{j}}\varphi(y)dy\right)\\
 & =\sum_{(i,j)}\left(\int_{-P_{i}}\varphi(-(-x))dx+\int_{P_{j}}\varphi(y)dy\right)\tag{by \ref{thm: transformation thm}}\\
 & =\sum_{(i,j)}\left(-\int_{P_{j}}\varphi(x)dx+\int_{P_{j}}\varphi(y)dy\right)\\
 & =0.
\end{align*}

\end_inset

Therefore, it follows that the desired integral is zero.
 
\begin_inset CommandInset label
LatexCommand label
name "lem: problem-2.14-lem1"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Therefore, we have 
\begin_inset Formula 
\begin{align*}
\text{Eq.(2)} & =\int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{2}-1\right\} \left(\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy.\tag{5}
\end{align*}

\end_inset

Now we break it down and evaluate Eq.(3) term by term, note that 
\begin_inset Formula 
\[
1=\int\exp\left\{ \lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} dx=\int\exp\left\{ \lambda_{1}-1+\lambda_{2}^{T}\mu+y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}\right\} dy,
\]

\end_inset

by change of variable 
\begin_inset Formula $y=x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}.$
\end_inset

 Hence, substituting these results back, we get 
\begin_inset Formula 
\[
\text{Eq.(2)}=\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}=\mu\iff\lambda_{3}^{-1}\lambda_{2}=0\implies\lambda_{2}=0,
\]

\end_inset

where the last implication can be justified as follows: suppose 
\begin_inset Formula $\lambda_{3}=0,$
\end_inset

 then 
\begin_inset Formula $p(x)=\exp\{\lambda_{1}-1+\lambda_{2}^{T}x\}$
\end_inset

 is a constant.
 And thus 
\begin_inset Formula $\int_{\mathbb{R}^{n}}p(x)dx=\infty$
\end_inset

 unless 
\begin_inset Formula $p(x)=0,$
\end_inset

 in which case the integral evaluates to 0, which does not satisfy Eq.(2.280).
 Hence, it follows that 
\begin_inset Formula 
\[
p(x)=\exp\left\{ \lambda_{1}-1+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} .
\]

\end_inset

Now, we substitute back into the last constraint: 
\begin_inset Formula 
\[
\int\exp\left\{ \lambda_{1}-1+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} (x-\mu)(x-\mu)^{T}dx=\Sigma.
\]

\end_inset

In order to find a solution, we recall that 
\begin_inset Formula 
\[
\int\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} (x-\mu)(x-\mu)^{T}dx=\Sigma.
\]

\end_inset

Hence, by comparison of the coefficients, we see that 
\begin_inset Formula $\lambda_{3}=-\frac{1}{2}\Sigma^{-1}$
\end_inset

 and 
\begin_inset Formula 
\[
\exp\left\{ \lambda_{1}-1\right\} =\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\implies\lambda_{1}=\log\left(\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\right)+1
\]

\end_inset

forms a set of admissible solution.
 With this set of 
\begin_inset Formula $\lambda$
\end_inset

's, we see that the 
\begin_inset Formula $p(x)$
\end_inset

 is the Gaussian density and thus it follows that multivariate Gaussian
 distribution is a minimizer of the calculus of variation program proposed
 in this problem.
 
\end_layout

\begin_layout Section
Problem 2.15 - Entropy of multivariate gaussian
\end_layout

\begin_layout Standard

\shape italic
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a random vector such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma)$
\end_inset

 with density 
\begin_inset Formula $\varphi(x\vert\mu,\Sigma)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

First, we give a lemma to be used later.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{R}}(n,m)$
\end_inset

 and 
\begin_inset Formula $B(x)\in\mathrm{Mat}_{\mathbb{R}}(m,n)$
\end_inset

.
 Then the following identity holds:
\begin_inset CommandInset label
LatexCommand label
name "lem: problem 2.15 lem1"

\end_inset


\begin_inset Formula 
\[
\mathrm{tr}\left(\int AB(x)dx\right)=\int\mathrm{tr}(AB(x))dx.
\]

\end_inset


\end_layout

\begin_layout Proof
Just write out the equation and follow definitions: 
\begin_inset Formula 
\begin{align*}
\text{tr}\left(\int AB(x)dx\right) & =\sum_{i=1}^{n}\left(\int AB(x)dx\right)_{ii}=\sum_{i=1}^{n}\left(\int\sum_{j=1}^{n}A_{ij}B_{ij}(x)dx\right)\\
 & =\int\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}B_{ij}(x)dx=\int\text{tr}(AB(x))dx
\end{align*}

\end_inset

as desired.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 Now, we go back to the proof.
 Note that 
\begin_inset Formula 
\begin{align*}
H(X) & =-\int\varphi(x|\mu,\Sigma)\ln\varphi(x|\mu,\Sigma)\\
 & =\int\varphi(x|\mu,\Sigma)\left(\log\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}+\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)(x-\mu)^{T}\Sigma^{-1}(x-\mu)d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)\text{tr}((x-\mu)^{T}\Sigma^{-1}(x-\mu))d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)\text{tr}(\Sigma^{-1}(x-\mu)(x-\mu)^{T})d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}\left(\Sigma^{-1}\int\varphi(x|\mu,\Sigma)(x-\mu)(x-\mu)^{T}d\mu\right)\tag{by \ref{lem: problem 2.15 lem1}}\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}(\Sigma^{-1}\Sigma)\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}(I_{D})\\
 & =\frac{D}{2}(\log2\pi+1)+\frac{1}{2}\log\left|\Sigma\right|.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 2.16 - Entropy of sum of two gaussians
\end_layout

\begin_layout Standard

\shape italic
This problem can be solved in various ways.
 The method proposed by hint given in the problem is limited in the sense
 that it is hard to generalized to arbitrary transformations and requires
 a lot of computation, which is error prone.
 We shall take a different approach here.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

 First, we give a lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random vector such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Then 
\begin_inset Formula $Y=AX+b\sim\mathrm{MVN}(A\mu+b,A\Sigma A^{*})$
\end_inset

 for 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{R}}(m,n)$
\end_inset

 and 
\begin_inset Formula $b\in\mathbb{R}^{m}.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "lem: distribution of linear transformation of gaussian"

\end_inset


\end_layout

\begin_layout Standard
To prove this lemma, we need to develop more theory and give more background
 knowledge about multivariate Gaussian distribution, which we present below.
\end_layout

\begin_layout Subsubsection*
Supplement knowledge 
\end_layout

\begin_layout Standard
In the book, the notion of a Gaussian distribution was mainly introduced
 as a maximizer of a calculus of variation problem under some constraint.
 This is completely valid and useful.
 But the addition of some more auxiliary definitions and results will help
 us gain a more through understanding of this distribution.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

In the discussion below, assume we have derived the univariate normal distributi
on using the book's view point.
 But now we use another route to push the result to the general setting.
 First, a few lemmas.
 
\end_layout

\begin_layout Lemma
The characteristic function for the 
\begin_inset Formula $\mathrm{N}(\mu,\sigma^{2})$
\end_inset

 is given by 
\begin_inset Formula 
\[
\varphi(t)=e^{i\theta a}e^{-\frac{1}{2}\sigma^{2}\theta^{2}}.
\]

\end_inset


\end_layout

\begin_layout Proof
Following the definition, we have 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\E[\exp(ity)]=\int_{\mathbb{R}}\exp(itx)\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)dx\\
 & =\frac{1}{\sqrt{2\pi}\sigma}\int_{\mathbb{R}}\exp(it(y+\mu))\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy\tag{by letting \ensuremath{y=x-\mu}}\\
 & =\frac{1}{\sqrt{2\pi}\sigma}e^{it\mu}\underbrace{\int_{\mathbb{R}}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy}_{:=\phi(t)}.
\end{align*}

\end_inset

In order to find a more explicit form of 
\begin_inset Formula $\widehat{\mu}(t)$
\end_inset

, we evaluate 
\begin_inset Formula $\phi(t).$
\end_inset

 Now note that 
\begin_inset Formula 
\begin{align*}
\left|\frac{\partial}{\partial t}\left(\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right)\right| & =\left|iy\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right|\leq y\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\in L^{1}(\mathbb{R}).
\end{align*}

\end_inset

Then a corollary of DCT, we have 
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial t}\phi(t) & =\int\frac{\partial}{\partial t}\left\{ \exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right\} dy=\int iy\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy\\
 & =\left[-(i\sigma^{2})^{2}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right]_{-\infty}^{\infty}-\sigma^{2}t\int_{-\infty}^{\infty}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy=-\sigma^{2}t\phi(t).
\end{align*}

\end_inset

Note that this is a first order differential equation.
 Moreover, observe that we also have following initial condition: 
\begin_inset Formula 
\[
\phi(0)=\int_{\mathbb{R}}\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy=\sqrt{2\pi}\sigma\underbrace{\frac{1}{\sqrt{2\pi}\sigma}\int_{\mathbb{R}}\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy}_{=1\text{ since it's gaussian density}}=\sqrt{2\pi}\sigma.
\]

\end_inset

Using integrating factor, we find its general solution if 
\begin_inset Formula $\phi(t)=ce^{-\frac{1}{2}\sigma^{2}t^{2}}.$
\end_inset

 Substituting back into the initial condition, we get that 
\begin_inset Formula $c=\sqrt{2\pi}\sigma$
\end_inset

 and as a result 
\begin_inset Formula $\phi(t)=ce^{-\frac{1}{2}\sigma^{2}t^{2}}.$
\end_inset

 Hence, it follows that 
\begin_inset Formula $\varphi(t)=e^{itx}e^{-\frac{1}{2}\sigma^{2}t^{2}}$
\end_inset

 as desired.
 
\end_layout

\begin_layout Lemma
The characteristic function for 
\begin_inset Formula $\mathrm{MVN}(\mu,\Sigma)$
\end_inset

 in 
\begin_inset Formula $n$
\end_inset

-dimensional Euclidean space is given by 
\begin_inset CommandInset label
LatexCommand label
name "lem: cf of gaussian"

\end_inset


\begin_inset Formula 
\[
\varphi(t)=\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)
\]

\end_inset


\end_layout

\begin_layout Proof
First, we follow the definition of multivariate characteristic function
 to write 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\int_{\mathbb{R}^{n}}\exp(i\left\langle t,x\right\rangle )\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu\right)\\
 & =\int_{\mathbb{R}^{n}}\exp(i\left\langle t,y+\mu\right\rangle )\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}y^{T}\Sigma^{-1}y\right)\left|\det(J_{\varphi}(y))\right|dy\tag{let \ensuremath{x=\varphi(y)=y+\mu}}\\
 & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\underbrace{\int_{\mathbb{R}^{n}}\exp\left(i\left\langle t,y\right\rangle -\frac{1}{2}y^{T}\Sigma^{-1}y\right)}_{:=I_{1}(t)}dy\tag{1}.
\end{align*}

\end_inset

Note that since 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is symmetric (by Problem 2.22), it follows that 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 has an eigen-decomposition in the form of 
\begin_inset Formula $\Sigma^{-1}=V\Lambda V^{*}$
\end_inset

 and 
\begin_inset Formula $\Lambda=V^{*}\Sigma^{-1}V$
\end_inset

, where 
\begin_inset Formula $\Lambda$
\end_inset

 is a diagonal matrix containing eigen values which are real and 
\begin_inset Formula $V\in\mathrm{O}(n)$
\end_inset

 according to Problem 2.18.
 Now we make a change of variable as follows: 
\begin_inset Formula 
\[
\varphi(x):\mathbb{R}^{n}\rightarrow\mathbb{R}^{n},x\mapsto Vx.\implies\left|\det J_{\varphi}(g)\right|=\left|\det V\right|=1.
\]

\end_inset

Then apply this change of variable to the 
\begin_inset Formula $I_{1}(t)$
\end_inset

 and we get 
\begin_inset Formula 
\begin{align*}
I_{1}(t) & =\int_{\mathbb{R}^{n}}\exp\left(i\left\langle t,Vx\right\rangle -\frac{1}{2}x^{T}V^{T}\Sigma^{-1}Vx\right)dx=\int_{\mathbb{R}^{n}}\exp\left(i\left\langle V^{*}t,x\right\rangle -\frac{1}{2}x^{T}\Lambda x\right)dx.
\end{align*}

\end_inset

Now, we bring this result back to Eq.(1) and get 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\int_{\mathbb{R}^{n}}\exp\left(i\left\langle s,x\right\rangle -\frac{1}{2}x^{T}\Lambda x\right)dx\tag{where \ensuremath{s=V^{*}t}}\\
 & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\int_{\mathbb{R}^{n}}\exp\left(i\sum_{i=1}^{n}s_{i}x_{i}-\frac{1}{2}\sum_{i=1}^{n}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx\\
 & =\frac{\exp(i\left\langle t,u\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\prod_{i=1}^{n}\int_{\mathbb{R}^{n}}\exp\left(is_{i}x_{i}-\frac{1}{2}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx_{i}\tag{by Fubini's theroem}\\
 & =\exp(i\left\langle t,u\right\rangle )\prod_{i=1}^{n}\int_{\mathbb{R}^{n}}\exp(its_{i})\frac{1}{\sqrt{2\pi}\lambda_{i}^{1/2}}\exp\left(-\frac{1}{2}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx_{i}\\
 & =\exp(i\left\langle t,u\right\rangle )\prod_{i=1}^{n}\varphi_{X_{i}\sim\mathrm{N}(0,\lambda_{i})}(s_{i})=\exp(i\left\langle t,\mu\right\rangle )\exp\left(\sum_{i=1}^{n}-\frac{\lambda_{i}s_{i}^{2}}{2_{i}}\right)\\
 & =\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}t^{*}\Sigma t\right)=\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right),
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
Now since set of characteristic functions is an isomorphic to the set of
 probability distributions (cf.???), we can alternatively define Gaussian
 distribution using it's characteristic function.
 One advantage of this characterization is the following lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Then 
\begin_inset Formula $X=_{d}\Sigma^{1/2}Z+\mu,$
\end_inset

 where 
\begin_inset Formula $Z\sim\mathrm{MVN}(0,I).$
\end_inset


\end_layout

\begin_layout Proof
This is a standard result.
 For the sake of completeness, we provide a complete proof here.
 Recall a useful lemma: 
\end_layout

\begin_deeper
\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable.
 Then the characteristic function for 
\begin_inset Formula $AX+b$
\end_inset

 where 
\begin_inset Formula $A\in\mathrm{Mat_{\mathbb{K}}}(n,m)$
\end_inset

 and 
\begin_inset Formula $b\in\mathbb{R}^{m}$
\end_inset

 can be characterized as 
\begin_inset Formula $\varphi_{AX+b}=e^{i\left\langle t,b\right\rangle }\varphi_{X}(A^{*}t).$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: characteristic function of AX+b"

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\end_inset

 Following the definition, we have 
\begin_inset Formula 
\begin{align*}
\varphi_{AX+b}(t) & =\int\exp\left(i\left\langle t,AX+b\right\rangle \right)d\Omega=\exp(i\left\langle t,b\right\rangle )\int\exp(i\left\langle t,Ax\right\rangle )d\Omega=\exp(i\left\langle t,b\right\rangle )\int\exp(i\left\langle A^{*}t,x\right\rangle )d\Omega\\
 & =\exp(i\left\langle t,b\right\rangle \varphi_{X}(A^{*}t),
\end{align*}

\end_inset

as desired.
 
\end_layout

\end_deeper
\begin_layout Proof
Now in view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
\varphi_{\Sigma^{1/2}Z+\mu} & =\exp(i\left\langle t,b\right\rangle )\varphi_{Z}(\Sigma^{1/2}t)=\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma^{1/2}t,\Sigma^{1/2}t\right\rangle \right)\\
 & =\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma^{1/2}\Sigma^{1/2}t,t\right\rangle \right)=\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\\
 & =\varphi_{X}(t).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $X=_{d}\Sigma^{1/2}Z+\mu$
\end_inset

 as desired.
 
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\end_inset

 In view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula 
\begin{align*}
\varphi_{AX+b}(t) & =\exp(i\left\langle t,b\right\rangle )\varphi_{X}(A^{*}t)\\
 & =\exp(i\left\langle t,b\right\rangle )\exp(i\left\langle A^{*}t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma A^{*}t,A^{*}t\right\rangle \right)\\
 & =\exp(i\left\langle t,A\mu,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle A\Sigma A^{*}t,t\right\rangle \right)\\
 & =\varphi_{Y\sim\mathrm{MVN}(A\mu+b,A\Sigma A^{*})}(t).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $AX+b=_{d}\mathrm{MVN}(A\mu+b,A\Sigma A^{*})$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Now we go back to the problem itself.
 Instead of 
\begin_inset Formula $x_{1},x_{2}$
\end_inset

, we use 
\begin_inset Formula $X_{1},X_{2}$
\end_inset

 to denote the designated r.v, i.e., 
\begin_inset Formula $X_{1}\sim\mathrm{N}(\mu_{1},\tau_{1}^{-1})$
\end_inset

 and 
\begin_inset Formula $X_{2}\sim\mathrm{N}(\mu_{2},\tau_{2}^{-1})$
\end_inset

 and 
\begin_inset Formula $X_{1}\perp X_{2}.$
\end_inset

 Then it follows that the random vector 
\begin_inset Formula $\widetilde{X}=\begin{bmatrix}X_{1}\\
X_{2}
\end{bmatrix}\sim\mathrm{MVN}\left(\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix},\begin{bmatrix}\tau_{1}^{-1} & 0\\
0 & \tau_{2}^{-1}
\end{bmatrix}\right)$
\end_inset

.
 Note that since 
\begin_inset Formula $X=\mathbf{1}^{T}\widetilde{X}$
\end_inset

 , an application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have that 
\begin_inset Formula 
\[
X\sim\mathrm{N}\left(\mathbf{1}^{T}\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix},\mathbf{1}^{T}\Sigma\mathbf{1}\right)=\mathrm{N}(\mu_{1}+\mu_{2},\tau_{1}^{-1}+\tau_{2}^{-1}).
\]

\end_inset

Then, by Problem 1.35, it follows that 
\begin_inset Formula $H(X_{1}+X_{2})=\frac{1}{2}(1+\ln(2\pi(\tau_{1}^{-1}+\tau_{2}^{-1}))).$
\end_inset


\end_layout

\begin_layout Paragraph
Alternative derivation of gaussian mean, covariance
\end_layout

\begin_layout Standard
In the textbook, the mean and covariance matrix of MVN are derived using
 change of variables techniques when evaluating the integral.
 Since we have mentioned that we can characterize a distribution using its
 characteristic function.
 Naturally it comes the question of deriving moments of random variables
 from their characteristic function.
 Here, we provide a general solution to this problem and apply it to the
 Gaussian case.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable with 
\begin_inset Formula $\E[\left\Vert X\right\Vert ^{N}]<\infty.$
\end_inset

 Then 
\begin_inset Formula 
\[
\mathrm{D}^{\alpha}\varphi_{X}(t)=i^{\left|\alpha\right|}\int X^{\alpha}e^{i\left\langle t,X\right\rangle }d\Omega,
\]

\end_inset

where 
\begin_inset Formula $\alpha=(\alpha_{1},\alpha_{2},...,\alpha_{n})$
\end_inset

 denote the multi-index such that 
\begin_inset Formula $\left|\alpha\right|:=\sum_{i=1}^{n}\alpha_{i}\leq N$
\end_inset

 and 
\begin_inset Formula $X^{\alpha}:=X_{1}^{\alpha_{1}}X_{2}^{\alpha_{2}}\cdots X_{n}^{\alpha_{n}}.$
\end_inset

 As a result, 
\begin_inset Formula 
\[
i^{\left|\alpha\right|}\E[X^{\alpha}]=\mathrm{D}^{\alpha}\varphi_{X}(0).
\]

\end_inset


\end_layout

\begin_layout Proof
To prove this, we induct on 
\begin_inset Formula $N$
\end_inset

.
 Now for the base of 
\begin_inset Formula $N=1,$
\end_inset

 we note that 
\begin_inset Formula $\mathrm{D}^{\alpha}\varphi_{X}(t)$
\end_inset

 then is reduces to 
\begin_inset Formula $\frac{\partial}{\partial t_{i}}\varphi_{X}(t)$
\end_inset

 for some 
\begin_inset Formula $i\in\{1,...,n\}.$
\end_inset

 Note 
\begin_inset Formula $\frac{\partial}{\partial t_{i}}\varphi_{X}(t)=\frac{\partial}{\partial t_{i}}\int e^{i\left\langle t,X\right\rangle }d\Omega$
\end_inset

 and that 
\begin_inset Formula 
\[
\left|\frac{\partial}{\partial t_{i}}\left(\exp(i\left\langle t,X\right\rangle \right)\right|=\left|iX_{i}\exp\left(i\left\langle t,X\right\rangle \right)\right|\leq\left|X_{i}\right|.
\]

\end_inset

We claim that 
\begin_inset Formula $\left|X_{i}\right|\in L^{1}(\Omega).$
\end_inset

 To see this, first we note that by Jensen's inequality 
\begin_inset Formula $(\E[\left\Vert X\right\Vert ])^{N}\leq\E[\left\Vert X\right\Vert ^{N}]<\infty.$
\end_inset

 Take the 
\begin_inset Formula $N$
\end_inset

-th root, we see that 
\begin_inset Formula $\left\Vert X\right\Vert \in L^{1}.$
\end_inset

 Clearly, we have 
\begin_inset Formula $|X_{i}|\leq(\sum_{i=1}^{n}X_{i}^{2})^{1/2}$
\end_inset

.
 Hence, chaining these inequalities shows that 
\begin_inset Formula $\left|X_{i}\right|\in L^{1}(\Omega).$
\end_inset

 Then by a variant of DCT, we can move the differentiation inside and get
 
\begin_inset Formula 
\[
\frac{\partial}{\partial t_{i}}\varphi_{X}(t)=\int\frac{\partial}{\partial t_{i}}\exp(i\left\langle t,X\right\rangle )d\Omega=i\int X_{i}\exp(i\left\langle t,X\right\rangle )d\Omega.
\]

\end_inset

Now assume that the claim holds for 
\begin_inset Formula $N=n-1.$
\end_inset

 Then for 
\begin_inset Formula $N=n,$
\end_inset

 we first note that for some 
\begin_inset Formula $\alpha$
\end_inset

 with 
\begin_inset Formula $\left|\alpha\right|=n,$
\end_inset

 
\begin_inset Formula $\mathrm{D}^{\alpha}\varphi_{X}(t)=\frac{\partial}{\partial t_{i}}(\mathrm{D}^{\beta}\varphi_{X}(t))$
\end_inset

 for some multi-index 
\begin_inset Formula $\beta$
\end_inset

 such that 
\begin_inset Formula $\left|\beta\right|=n-1,$
\end_inset

 and some 
\begin_inset Formula $i\in\{1,...,n\}.$
\end_inset

 Then, we note that first by inductive hypothesis, 
\begin_inset Formula $\mathrm{D}\beta\varphi_{X}(t)=i^{\left|\beta\right|}\int X^{\beta}\exp^{i\left\langle t,X\right\rangle }d\Omega$
\end_inset

 and second, 
\begin_inset Formula 
\[
\left|\frac{\partial}{\partial t_{i}}X^{\beta}\exp(i\left\langle t,X\right\rangle )\right|=\left|iX_{i}X^{\beta}\exp(i\left\langle t,X\right\rangle )\right|\leq\left|X^{\alpha}\right|=\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}.
\]

\end_inset

Now, we claim that 
\begin_inset Formula $\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}\in L^{1}(\Omega)$
\end_inset

.
 To see this we note that for since 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}=N,$
\end_inset

 it follows that 
\begin_inset Formula $\alpha_{i}\leq N.$
\end_inset

 As a result, 
\begin_inset Formula $\left|X_{i}\right|^{\alpha_{i}}\leq\left\Vert X\right\Vert ^{\alpha_{i}}.$
\end_inset

 Therefore, 
\begin_inset Formula $\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}\leq\prod_{i=1}^{n}\left\Vert X\right\Vert ^{\alpha_{i}}=\left\Vert X\right\Vert ^{\sum_{i=1}^{n}\alpha_{i}}=\left\Vert X\right\Vert ^{N}\in L^{1}(\Omega).$
\end_inset

 Again, by the variant of DCT, we have 
\begin_inset Formula 
\begin{align*}
\mathrm{D}^{\alpha}\varphi_{X}(t) & =\frac{\partial}{\partial t_{i}}i^{\left|\beta\right|}\int X^{\beta}\exp(i\left\langle t,X\right\rangle )d\Omega=i^{\left|\beta\right|}\int\frac{\partial}{\partial t_{i}}\left(X^{\beta}\exp(i\left\langle t,X\right\rangle )\right)d\Omega\\
 & =i^{\left|\beta\right|}\int iX^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega=i^{\left|\beta\right|+1}\int X^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega\\
 & =i^{\left|\alpha\right|}\int X^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega,
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
Now we use this result to derive the mean and covariance of a MVN random
 variable, which we denote as 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Recall that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle ).$
\end_inset

 Now we find the Frechet derivative w.r.t 
\begin_inset Formula $t$
\end_inset

: note that by the chain rule:
\begin_inset Formula 
\begin{align*}
\mathrm{D}\varphi_{X}(t) & =\mathrm{D}\left(\exp(t)\right)\circ\left(t\mapsto i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\circ\mathrm{D}\bigg(\underbrace{i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle }_{:=H_{1}(t)}\bigg).
\end{align*}

\end_inset

Note that 
\begin_inset Formula 
\begin{align*}
H_{1}(t+h) & =i\left\langle t+h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma(t+h),t+h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle \Sigma t,h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle t,\Sigma^{*}h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+\left\langle i\mu-\Sigma t,h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle .
\end{align*}

\end_inset

Since 
\begin_inset Formula $\left\langle \Sigma h,h\right\rangle \leq\left\Vert \Sigma\right\Vert _{\infty}\left\Vert h\right\Vert ^{2}\rightarrow0$
\end_inset

 at 
\begin_inset Formula $\left\Vert h\right\Vert \rightarrow0,$
\end_inset

 it follows that 
\begin_inset Formula $\frac{1}{2}\left\langle \Sigma h,h\right\rangle =o(\left\Vert h\right\Vert ).$
\end_inset

 As 
\begin_inset Formula $h\mapsto\left\langle i\mu-\Sigma t,h\right\rangle \in\text{Hom}(\mathbb{R}^{n},\mathbb{R}),$
\end_inset

 it follows that 
\begin_inset Formula $\mathrm{D}H_{1}(t)=i\mu-\Sigma t.$
\end_inset

 Since 
\begin_inset Formula $\mathrm{D}(\exp(t))=\exp(t)$
\end_inset

 by elementary calculus, it follows that 
\begin_inset Formula 
\[
\mathrm{D}\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle )(i\mu-\Sigma t).
\]

\end_inset

Therefore, 
\begin_inset Formula $\E[X]=\mathrm{D}\varphi_{X}(0)=i^{-1}i\mu=\mu.$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Now to find the covariance, we can either partial differentiate term by
 term or use the same notion of Frechet derivative.
 We adopt the second method since it is consistent with our previous method,
 and also yields the total derivative in its matrix form directly, which
 is more elegant than piecing together terms.
 Before we go into the calculation, we prepare ourselves with a tool to
 facilitate the calculation - generalized product rule.
 
\end_layout

\begin_layout Lemma
Suppose the mapping 
\begin_inset Formula $B:X_{1}\times X_{2}\rightarrow Y$
\end_inset

 is bilinear and bounded, i.e., 
\begin_inset Formula 
\[
\left\Vert B(x_{1},x_{2})\right\Vert \leq C\left\Vert x_{1}\right\Vert \left\Vert x_{2}\right\Vert \text{ for all }x_{1}\in X_{1},x_{2}\in X_{2}
\]

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 is fixed and 
\begin_inset Formula $B$
\end_inset

 linear in each argument.
 Suppose further that the maps 
\begin_inset Formula $f_{i}:X\rightarrow X_{i},$
\end_inset

 
\begin_inset Formula $i=1,2$
\end_inset

 are Frechet differentiable at 
\begin_inset Formula $x$
\end_inset

, and there exist and open set 
\begin_inset Formula $U$
\end_inset

 such that 
\begin_inset Formula $x\in U$
\end_inset

 and 
\begin_inset Formula $U\subseteq\mathcal{D}_{f_{i}}.$
\end_inset

 Then the function 
\begin_inset Formula $H(x)=B(f_{1}(x),f_{2}(x))$
\end_inset

 is differentiable at 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset CommandInset label
LatexCommand label
name "lem: prod rule"

\end_inset


\begin_inset Formula 
\[
\mathrm{D}H(x)(h)=B(\mathrm{D}f_{1}(x)(h),f_{2}(x))+B(f_{1}(x),\mathrm{D}f_{2}'(x)(h)).
\]

\end_inset

Note: 
\begin_inset Formula $X_{1},X_{2},X,Y$
\end_inset

 are all assumed to be Banach spaces.
 
\end_layout

\begin_layout Proof
We follow the definition and write out 
\begin_inset Formula $H(x+h)-H(x)$
\end_inset

 for later analysis.
 To facilitate notation, we let 
\begin_inset Formula $f_{i}^{x}:=f_{i}(x)$
\end_inset

 for 
\begin_inset Formula $i=1,2.$
\end_inset

 
\begin_inset Formula 
\begin{align*}
H(x+h)-H(x) & =B(f_{1}^{x+h},f_{2}^{x+h})-B(f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x+h},f_{2}^{x+h})-B(f_{1}^{x+h},f_{2}^{x})+B(f_{1}^{x+h},f_{2}^{x})-B(f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x+h},f_{2}^{x+h}-f_{2}^{x})+B(f_{1}^{x+h}-f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x}+\mathrm{D}f_{1}^{x}(h)+\left\Vert h\right\Vert r_{1}(h),\mathrm{D}f_{2}^{x}(h)+\left\Vert h\right\Vert r_{2}(h))+B(\mathrm{D}f_{1}^{x}(h)+\left\Vert h\right\Vert r_{1}(h),f_{2}^{x})\\
 & =T_{x}(h)+R_{x}(h),
\end{align*}

\end_inset

where 
\begin_inset Formula 
\[
\begin{cases}
\begin{split}T_{x}(h)= & B(f_{1}^{x},\mathrm{D}f_{2}^{x}(h))+B(\mathrm{D}f_{1}^{x}(h),f_{2}^{x})\\
R_{x}(h)= & B(f_{1}^{x},\left\Vert h\right\Vert r_{2}(h))+B(\mathrm{D}f_{1}^{x}(h),\mathrm{D}f_{2}^{x}(h))+B(\mathrm{D}f_{1}^{x}(h),\left\Vert h\right\Vert \\
 & +B(\left\Vert h\right\Vert r_{1}(h),\left\Vert h\right\Vert r_{2}(h))+B(\left\Vert h\right\Vert r_{1}(h),f_{2}^{x}).
\end{split}
r_{2}(h))+B(\left\Vert h\right\Vert _{1}r_{1}(h),\mathrm{D}f_{2}^{x}(h))\end{cases}
\]

\end_inset

In order to show that 
\begin_inset Formula $T_{x}(h)=\mathrm{D}H(x)\circ h.$
\end_inset

 We first need to show that 
\begin_inset Formula $T(h)\in\mathrm{Hom}(X,Y).$
\end_inset

 Indeed, 
\begin_inset Formula 
\begin{align*}
T_{x}(\alpha h+\beta g) & =B(f_{1}^{x},\mathrm{D}f_{2}(\alpha h+\beta g))+B(\mathrm{D}f_{1}^{x}(\alpha h+\beta g),f_{2}^{x})\\
 & =\alpha B(f_{1}^{x},\mathrm{D}f_{2}(h))+\beta B(f_{1}^{x},\mathrm{D}f_{2}(g))+\alpha B(\mathrm{D}f_{1}^{x}(h),f_{2}^{x})+\beta B(\mathrm{D}f_{1}^{x}(g),f_{2}^{x})\\
 & =\alpha T_{x}(h)+\beta T_{x}(g).
\end{align*}

\end_inset

Next, we need to show that 
\begin_inset Formula $R_{x}(h)=o(\left\Vert h\right\Vert ).$
\end_inset

 We analyze 
\begin_inset Formula $R_{x}(h)$
\end_inset

 term by term as follows 
\begin_inset Formula 
\[
\begin{cases}
\begin{alignedat}{1} & B(f_{1}^{x},\left\Vert h\right\Vert r_{2}(h))\leq C\left\Vert f_{1}^{x}\right\Vert \left\Vert h\right\Vert \left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\mathrm{D}f_{1}^{x}(h),\mathrm{D}f_{2}^{x}(h))\leq C\left\Vert \mathrm{D}f_{1}^{x}\right\Vert \left\Vert \mathrm{D}f_{2}^{x}\right\Vert \left\Vert h\right\Vert ^{2}=o(\left\Vert h\right\Vert )\\
 & B(\mathrm{D}f_{1}^{x}(h),\left\Vert h\right\Vert r_{2}(h)\leq C\left\Vert \mathrm{D}f_{1}^{x}\right\Vert \left\Vert h\right\Vert ^{2}\left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert _{1}r_{1}(h),\mathrm{D}f_{2}^{x}(h))\leq C\left\Vert \mathrm{D}f_{2}^{x}\right\Vert \left\Vert h\right\Vert ^{2}\left\Vert r_{1}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert r_{1}(h),\left\Vert h\right\Vert r_{2}(h))\leq C\left\Vert h\right\Vert ^{2}\left\Vert r_{1}(h)\right\Vert \left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert r_{1}(h),f_{2}^{x})\leq C\left\Vert f_{2}^{x}\right\Vert \left\Vert h\right\Vert \left\Vert r_{1}(h)\right\Vert =o(\left\Vert h\right\Vert ).
\end{alignedat}
.\end{cases}
\]

\end_inset

Since 
\begin_inset Formula $R_{x}(h)$
\end_inset

 is the sum of these terms, it follows that 
\begin_inset Formula $R_{x}(h)=o(\left\Vert h\right\Vert )$
\end_inset

.
 Hence, it follows that 
\begin_inset Formula $T_{x}(h)=\mathrm{D}H(x)\circ h.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Now, observe that 
\begin_inset Formula $\mathrm{D}\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle )(i\mu-\Sigma t)=B(\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle ),i\mu-\Sigma t),$
\end_inset

 where 
\begin_inset Formula $B\in\mathrm{Hom}(\mathbb{R},\mathbb{R}^{n};\mathbb{R}^{n})$
\end_inset

 is defined by 
\begin_inset Formula $(x,y)\mapsto xy.$
\end_inset

 Hence, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: prod rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it follows that 
\begin_inset Formula 
\begin{align*}
\mathrm{D}^{2}\varphi_{X}(t)(h) & =B\left(\left\langle \exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)(i\mu-\Sigma t),h\right\rangle ,i\mu-\Sigma t\right)+B\left(\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right),-\Sigma h\right)\\
 & =\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)(i\mu-\Sigma t)(i\mu-\Sigma t)^{T}h-\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\Sigma h\\
 & =\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)((i\mu-\Sigma t)(i\mu-\Sigma t)^{T}h-\Sigma h).
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula 
\[
\mathrm{D}\varphi_{X}(0)h=(i^{2}\mu\mu^{T}-\Sigma)h=(-\mu\mu^{T}-\Sigma)h\implies\E[XX^{T}]=-\mathrm{D}^{2}\varphi_{X}(0)=\mu\mu^{T}+\Sigma.
\]

\end_inset

 As a result, we have according to definition the covariance matrix is 
\begin_inset Formula $\E[XX^{T}]-\mu\mu^{T}=\Sigma.$
\end_inset


\end_layout

\begin_layout Section
Problem 2.17 - Suffices to assume the parameter 
\begin_inset Formula $\Sigma$
\end_inset

 in Gaussian to be symmetric
\end_layout

\begin_layout Standard
This is an direct application of Problem 1.14.
 Recall the MVN in 
\begin_inset Formula $n$
\end_inset

-dimensional space has density in the following form:
\begin_inset Formula 
\[
\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right).
\]

\end_inset

Since 
\begin_inset Formula $(x-\mu)^{T}\Sigma(x-\mu)$
\end_inset

 is a bilinear form.
 In problem 1.14, we showed that it suffices to assume 
\begin_inset Formula $\Sigma^{-1}=\Sigma_{S}^{^{-1}}=\frac{1}{2}(\Sigma^{-1}+(\Sigma^{-1})^{T}),$
\end_inset

 which is symmetric since 
\begin_inset Formula $(x-\mu)^{T}\Sigma^{-1}(x-\mu)=(x-\mu)^{T}\Sigma_{S}^{-1}(x-\mu)$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{R}^{n}.$
\end_inset


\end_layout

\begin_layout Section
Problem 2.18 - Eigen-decomposition for symmetric matrices 
\end_layout

\begin_layout Standard
Before go into the proof, a lemma.
 This lemma is usually known as Gram-Schmidt orthogonalization.
 We prove it in the context of Hilbert space.
 Proof of this result at various level of generality can be found in any
 standard linear algebra textbook.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\mathcal{H}$
\end_inset

 be an Hilbert space.
 Suppose 
\begin_inset Formula $\mathcal{U}=\{u_{1},u_{2},\ldots,u_{n}\}$
\end_inset

 is a set of linearly independent vectors of 
\begin_inset Formula $V.$
\end_inset

 Then there exists a set 
\begin_inset Formula $\mathcal{V=}\{v_{1},v_{2},\ldots,v_{n}\}$
\end_inset

 of elements of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 such that 
\begin_inset CommandInset label
LatexCommand label
name "lem: gram schimt"

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\left\Vert v_{i}\right\Vert =1$
\end_inset

 for 
\begin_inset Formula $1\leq i\leq n.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left\langle v_{i},v_{j}\right\rangle =0$
\end_inset

 for 
\begin_inset Formula $1\leq i,j,\leq n$
\end_inset

 with 
\begin_inset Formula $i\neq j$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathrm{span}(\mathcal{V})=\mathrm{span}(\mathcal{U}).$
\end_inset


\end_layout

\end_deeper
\begin_layout Proof
We prove this constructively.
 The construction goes as follows: first we let 
\begin_inset Formula $v_{1}=\frac{u_{1}}{\left\Vert u_{1}\right\Vert }$
\end_inset

 and 
\begin_inset Formula $v_{2}=\frac{w_{2}}{\left\Vert w_{2}\right\Vert },$
\end_inset

 where 
\begin_inset Formula $w_{2}=u_{2}-\left\langle u_{2},v_{1}\right\rangle v_{1}$
\end_inset

 , and inductively 
\begin_inset Formula $v_{i}=\frac{w_{i}}{\left\Vert w_{i}\right\Vert },$
\end_inset

 where 
\begin_inset Formula $w_{i}=u_{i}-\sum_{j=1}^{i-1}\left\langle u_{i},v_{j}\right\rangle v_{j},$
\end_inset

 assuming 
\begin_inset Formula $v_{1},...,v_{i-1}$
\end_inset

 have already been defined.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

To prove the correctness of our construction, we induct on 
\begin_inset Formula $n.$
\end_inset

 For the base case where 
\begin_inset Formula $k=1.$
\end_inset

 We see that (2) and (3) is trivially satisfied for 
\begin_inset Formula $v_{1}=u_{1}/\left\Vert u_{1}\right\Vert .$
\end_inset

 Also, we have 
\begin_inset Formula $\left\Vert v_{1}\right\Vert =\left\Vert v_{1}\right\Vert /\left\Vert v_{1}\right\Vert =1.$
\end_inset

 Now suppose, the construction yields the desired set of vectors 
\begin_inset Formula $\mathcal{V}_{k=n-1}$
\end_inset

 that satisfies condition (1)-(3) for a given set of vectors 
\begin_inset Formula $\mathcal{U}_{n-1}$
\end_inset

.
 Then for 
\begin_inset Formula $k=n,$
\end_inset

we are given a set of elements in 
\begin_inset Formula $\mathcal{H}$
\end_inset

, 
\begin_inset Formula $\mathcal{U}_{n}=\{u_{1},\ldots,u_{n}\}$
\end_inset

.
 By induction hypothesis, we can construct a set of vectors 
\begin_inset Formula $\mathcal{V}_{n-1}=\{v_{1},\ldots,v_{n-1}\}$
\end_inset

 from the set 
\begin_inset Formula $\mathcal{U}_{n}-\{u_{n}\}$
\end_inset

 that satisfies the conditions (1)-(3) stipulated above with 
\begin_inset Formula $\mathcal{V}=\mathcal{V}_{n-1}$
\end_inset

 and 
\begin_inset Formula $\mathcal{U}=\mathcal{U}_{n-1}$
\end_inset

.
 Now let 
\begin_inset Formula $v_{n}=w_{n}/\left\Vert w_{n}\right\Vert ,$
\end_inset

 where 
\begin_inset Formula $w_{n}=u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}.$
\end_inset

 First, we show that 
\begin_inset Formula $v_{n}$
\end_inset

 is well-defined.
 To prove this, we show that 
\begin_inset Formula $w_{n}\notin\mathrm{span}(\mathcal{V}_{n-1}).$
\end_inset

 Suppose otherwise, then 
\begin_inset Formula $w_{n}=\sum_{i=1}^{n-1}\alpha_{i}v_{n}$
\end_inset

 for some scalars 
\begin_inset Formula $\{\alpha_{i}\}_{i=1}^{n-1}.$
\end_inset

 Then we have 
\begin_inset Formula 
\[
w_{n}=\sum_{i=1}^{n-1}\alpha_{i}v_{i}=u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\implies u_{n}=\sum_{i=1}^{n-1}(\alpha_{i}+\left\langle u_{n},v_{i}\right\rangle )v_{i}\in\mathrm{span}(\mathcal{V}_{n-1}).
\]

\end_inset

Since 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1})=\mathrm{span}(\mathcal{U}_{n-1})$
\end_inset

, it follows that 
\begin_inset Formula $u_{n}\in\mathrm{span}(\mathcal{U}_{n-1}).$
\end_inset

 This is a contradiction since then 
\begin_inset Formula $u_{n}$
\end_inset

 are independent of 
\begin_inset Formula $u_{1},...,u_{n-1}$
\end_inset

 by assumption.
 We claim that 
\begin_inset Formula $\mathcal{V}_{n-1}\cup\{v_{n}\}$
\end_inset

 satisfies conditions (1)-(3) with 
\begin_inset Formula $\mathcal{U}=\mathcal{U}_{n}$
\end_inset

 and 
\begin_inset Formula $\mathcal{V}=\mathcal{V}_{n-1}\cup\{v_{n}\}$
\end_inset

.
 Note that by construction 
\begin_inset Formula $\left\Vert v_{n}\right\Vert =1.$
\end_inset

 And since for 
\begin_inset Formula $j\in\{1,...,n-1\}$
\end_inset


\begin_inset Formula 
\begin{align*}
\left\langle w_{n},v_{j}\right\rangle  & =\left\langle u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i},v_{j}\right\rangle \\
 & =\left\langle u_{n},v_{j}\right\rangle -\sum_{i=1}^{n-1}(\left\langle u_{n},v_{i}\right\rangle \left\langle v_{i},v_{j}\right\rangle )\\
 & =\left\langle u_{n}-v_{j}\right\rangle -\left\langle u_{n}-v_{j}\right\rangle =0,
\end{align*}

\end_inset

it follows that 
\begin_inset Formula $\left\langle w_{n}/\left\Vert w_{n}\right\Vert ,v_{j}\right\rangle =\left\langle v_{n},v_{j}\right\rangle =0$
\end_inset

 for all 
\begin_inset Formula $j=1,...,n$
\end_inset

.
 What's left to prove is that 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\}\}=\mathrm{span}(\mathcal{U}_{n}).$
\end_inset

 Pick 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\})\ni x=\sum_{i=1}^{n}\alpha_{i}v_{i}.$
\end_inset

 If 
\begin_inset Formula $\alpha_{n}=0,$
\end_inset

 then 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{V}_{n-1})=\mathrm{span}(\mathcal{U}_{n-1})\subset\mathrm{span}(\mathcal{U}_{n}).$
\end_inset

 If 
\begin_inset Formula $\alpha_{n}\neq0,$
\end_inset

 then 
\begin_inset Formula 
\begin{align*}
x & =\sum_{i=1}^{n-1}\alpha_{i}v_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }\left(u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right)\\
 & =\sum_{i=1}^{n-1}(\alpha_{i}-\left\langle u_{i},v_{i}\right\rangle )v_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }u_{n}\tag{combining coefficients}\\
 & =\sum_{i=1}^{n-1}\beta_{i}u_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }u_{n}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n})$
\end_inset

.
 On the other hand, suppose 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n}),$
\end_inset

 i.e.
 
\begin_inset Formula $x=\sum_{i=1}^{n}\alpha_{i}u_{i}$
\end_inset

.
 If 
\begin_inset Formula $\alpha_{n}=0,$
\end_inset

 then 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n-1})=\mathrm{span}(\mathcal{V}_{n-1})$
\end_inset

 by induction hypothesis.
 If 
\begin_inset Formula $\alpha_{n}\neq0,$
\end_inset

 then we have 
\begin_inset Formula 
\begin{align*}
x & =\sum_{i=1}^{n-1}\alpha_{i}u_{i}+u_{n}=\sum_{i=1}^{n-1}\beta_{i}v_{i}+u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}+\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\\
 & =\sum_{i=1}^{n-1}(\beta_{i}+\left\langle u_{n},v_{i}\right\rangle )v_{i}+v_{n},
\end{align*}

\end_inset

whence 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\}).$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 We first show that the eigenvalues are real.
 First, we note that by definition, the pair 
\begin_inset Formula $(\lambda_{i},\mu_{i})$
\end_inset

 is an eigenvector, eigenvalue pair iff 
\begin_inset Formula $\Sigma\mu_{i}=\lambda_{i}\mu.$
\end_inset

 Now we fix one such pair 
\begin_inset Formula $(\mu_{i},\lambda_{i})$
\end_inset

.
 Multiplying 
\begin_inset Formula $\mu_{i}^{*}$
\end_inset

 on the left on both side of the equation yields 
\begin_inset Formula $\mu^{*}\Sigma\mu_{i}=\lambda_{i}\mu_{i}^{*}\mu_{i}.$
\end_inset

 Now since 
\begin_inset Formula $\Sigma$
\end_inset

 is symmetric, it follows that 
\begin_inset Formula $(\mu^{*}\Sigma\mu)^{*}=\mu^{*}\Sigma\mu=\lambda_{i}^{*}\mu_{i}^{*}\mu_{i}.$
\end_inset

 Hence, it follows that 
\begin_inset Formula 
\[
\lambda_{i}^{*}\mu_{i}^{*}\mu_{i}=\lambda_{i}\mu_{i}^{*}\mu_{i}\iff(\lambda_{i}^{*}-\lambda_{i})\mu_{i}^{*}\mu_{i}=0\iff\lambda_{i}^{*}=\lambda_{i},
\]

\end_inset

since we assume 
\begin_inset Formula $\mu_{i}\neq0.$
\end_inset

 Therefore, 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is real.
 Since 
\begin_inset Formula $(\mu_{i},\lambda_{i})$
\end_inset

 is chosen to be arbitrary, it follows that all eigen values in this case
 are real.
 
\end_layout

\begin_layout Enumerate
Next, we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with 
\begin_inset Formula $\lambda_{i}\neq\lambda_{j},$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},\lambda_{j}\neq0$
\end_inset

, we have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0.$
\end_inset

 First, note the following identity: 
\begin_inset Formula 
\[
\lambda_{j}\mu_{j}^{*}\mu_{i}=(\Sigma\mu_{j})^{*}\mu_{i}=(\Sigma^{*}\mu_{j})^{*}\mu_{i}=\mu_{j}^{*}\Sigma\mu_{i}=\mu_{j}^{*}\lambda_{i}\mu_{i}=\lambda_{i}\mu_{j}^{*}\mu_{i},
\]

\end_inset

which implies 
\begin_inset Formula $\lambda_{i}\mu_{j}^{*}\mu_{i}=\lambda_{j}\mu_{j}^{*}\mu_{i}\Leftrightarrow\mu_{j}^{*}\mu_{i}(\lambda_{i}-\lambda_{j})=0.$
\end_inset

 Since 
\begin_inset Formula $\lambda_{i}\neq\lambda_{j}$
\end_inset

 by assumption, it follows that 
\begin_inset Formula $\left\langle \mu_{j},\mu_{i}\right\rangle =\mu_{j}^{*}\mu_{i}=0$
\end_inset

, as desired.
 
\end_layout

\begin_layout Enumerate
Now we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with 
\begin_inset Formula $\lambda_{i}=\lambda_{j}$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},\lambda_{j}\neq0,$
\end_inset

 we can still have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0$
\end_inset

.
 For better notation, suppose 
\begin_inset Formula $\lambda_{1}=\lambda_{2}:=\lambda\in\mathbb{R}.$
\end_inset

 First, we show that any linear combination of 
\begin_inset Formula $\mu_{i}$
\end_inset

 and 
\begin_inset Formula $\mu_{j}$
\end_inset

 is also an eigen vector with the same eigen value, i.e., 
\begin_inset Formula $(\lambda,\alpha\mu_{i}+\beta\mu_{2})$
\end_inset

 is a valid eigen pair as well for any 
\begin_inset Formula $\alpha,\beta\in\mathbb{R}-\{0\}.$
\end_inset

 Indeed, we have 
\begin_inset Formula 
\[
\Sigma(\alpha\mu_{i}+\beta\mu_{j})=\alpha\Sigma\mu_{i}+\beta\Sigma\mu_{j}=\lambda\alpha\mu_{i}+\lambda\beta\mu_{j}=\lambda(\alpha\mu_{i}+\beta\mu_{j}).
\]

\end_inset

Therefore, in view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gram schimt"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can orthonormalize 
\begin_inset Formula $\mu_{i}$
\end_inset

 and 
\begin_inset Formula $\mu_{j}$
\end_inset

 to 
\begin_inset Formula $\widetilde{\mu}_{i}$
\end_inset

 and 
\begin_inset Formula $\widetilde{\mu}_{j}$
\end_inset

 such that 
\begin_inset Formula $(\lambda,\tilde{\mu}_{i})$
\end_inset

 and 
\begin_inset Formula $(\lambda,\widetilde{\mu}_{j})$
\end_inset

 are eigen-pairs as well (
\begin_inset Formula $\widetilde{\mu}_{1},\widetilde{\mu}_{2}$
\end_inset

 are linear combination of 
\begin_inset Formula $\mu_{1}$
\end_inset

 and 
\begin_inset Formula $\mu_{2}$
\end_inset

).
 
\end_layout

\begin_layout Enumerate
Now we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with at least one of 
\begin_inset Formula $\lambda_{i}$
\end_inset

 and 
\begin_inset Formula $\lambda_{j}$
\end_inset

 being equal to 0, we can still have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0$
\end_inset

.
 Without loss of generality, suppose 
\begin_inset Formula $\lambda_{i}=0.$
\end_inset

 Then, this means that 
\begin_inset Formula $\Sigma\mu_{i}=0.$
\end_inset

 Now suppose 
\begin_inset Formula $\lambda_{j}\neq0,$
\end_inset

 then we have that 
\begin_inset Formula $\Sigma\mu_{j}=\lambda_{j}\mu_{j}\implies\mu_{j}=\Sigma\mu_{j}/\lambda_{j}.$
\end_inset

 Then it follows that 
\begin_inset Formula 
\[
\left\langle \mu_{i},\mu_{j}\right\rangle =\frac{1}{\lambda_{j}}\mu_{i}^{T}\Sigma\mu_{j}=\frac{1}{\lambda_{j}}(\Sigma\mu_{i})^{T}\mu_{j}=0.
\]

\end_inset

Suppose otherwise that 
\begin_inset Formula $\lambda_{j}=0.$
\end_inset

 Then 
\begin_inset Formula $\mu_{i},\mu_{j}\in\ker(\Sigma),$
\end_inset

 which is a subspace.
 Therefore, we can orthonormalize 
\begin_inset Formula $\mu_{i},\mu_{j}$
\end_inset

 by applying 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gram schimt"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Section
Problem 2.19 - Characterization of 
\begin_inset Formula $\Sigma,\Sigma^{-1}$
\end_inset

 in Gaussian distribution
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\text{Eq.(2.45)}=\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{*}=U\Lambda U^{*},$
\end_inset

 where 
\begin_inset Formula $U=[u_{i}]$
\end_inset

 are vertical stack of eigen vectors of 
\begin_inset Formula $\Sigma.$
\end_inset

 On the other hand, note that 
\begin_inset Formula $\Sigma U=U\Lambda,$
\end_inset

 which implies 
\begin_inset Formula $\Lambda=U^{*}\Sigma U.$
\end_inset

 Therefore, substitute back we get 
\begin_inset Formula $U\Lambda U^{*}=UU^{*}\Sigma UU^{*}=\Sigma$
\end_inset

 as desired.
 
\end_layout

\end_body
\end_document
